{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7d61d0",
   "metadata": {},
   "source": [
    "# üéØ EN3160 Fundamentals of Image Processing and Computer Vision\n",
    "## Complete Implementation Guide with Real Examples\n",
    "\n",
    "Welcome to this comprehensive Jupyter notebook covering the fundamentals of **Image Processing** and **Computer Vision**! This notebook provides both basic and advanced implementations of key concepts.\n",
    "\n",
    "### üìö Course Overview\n",
    "- **Course**: EN3160 Fundamentals of Image Processing and Computer Vision\n",
    "- **Author**: Based on Prof. Ranga Rodrigo's Course Material\n",
    "- **Date**: July 2025\n",
    "\n",
    "### üé® Comprehensive Learning Journey\n",
    "\n",
    "#### **1Ô∏è‚É£ Image Processing & Early Vision**\n",
    "- **Sampling & Interpolation**: Converting continuous images to discrete grids\n",
    "- **Point Operations**: Brightness, contrast, gamma correction\n",
    "- **Filtering & Convolutions**: Smoothing, sharpening, noise reduction\n",
    "- **Edge Detection & Feature Extraction**: Sobel, Canny, Harris corners\n",
    "- **Optical Flow**: Motion estimation between frames\n",
    "\n",
    "#### **2Ô∏è‚É£ Fitting & Alignment**\n",
    "- **Least Squares**: Error minimization techniques\n",
    "- **Voting Methods**: Hough Transform for shape detection\n",
    "- **Geometric Transforms**: Affine, perspective, homography\n",
    "- **Image Stitching**: Panorama creation and mosaicking\n",
    "\n",
    "#### **3Ô∏è‚É£ Image Formation**\n",
    "- **Perspective Projection**: 3D to 2D mapping principles\n",
    "- **Camera Models**: Pinhole cameras, lens systems, distortion\n",
    "- **Light & Shading**: Illumination models, reflectance\n",
    "- **Color Theory**: Color spaces, perception, transformations\n",
    "\n",
    "#### **4Ô∏è‚É£ 3D Vision**\n",
    "- **Camera Calibration**: Intrinsic and extrinsic parameters\n",
    "- **Two-view Geometry**: Epipolar geometry, fundamental matrix\n",
    "- **Structure from Motion**: 3D reconstruction from multiple views\n",
    "- **Stereo Vision**: Depth estimation from image pairs\n",
    "- **Dense 3D Reconstruction**:\n",
    "  - Neural Radiance Fields (NeRFs)\n",
    "  - Gaussian Splatting techniques\n",
    "\n",
    "#### **5Ô∏è‚É£ Segmentation**\n",
    "- **Thresholding**: Binary, adaptive, Otsu's method\n",
    "- **Region Growing**: Seed-based segmentation\n",
    "- **Active Contours**: Snakes and level sets\n",
    "- **Graph-based Methods**: GrabCut, normalized cuts\n",
    "- **Deep Learning Segmentation**: U-Net, Mask R-CNN\n",
    "\n",
    "#### **6Ô∏è‚É£ Deep Learning for Vision**\n",
    "- **Linear Classifiers**: Perceptrons, SVM fundamentals\n",
    "- **Image Classification**: CNNs, transfer learning\n",
    "- **Object Detection**: YOLO, R-CNN family\n",
    "- **Generative Methods**:\n",
    "  - Autoregressive Models\n",
    "  - Diffusion Models (DDPM, Stable Diffusion)\n",
    "  - GANs and VAEs\n",
    "\n",
    "#### **7Ô∏è‚É£ Recent Topics & Advanced Methods**\n",
    "- **Foundation Models**: Vision Transformers (ViTs)\n",
    "- **Vision-Language Models**: CLIP, DALL-E\n",
    "- **Real-time Vision Systems**: Efficient architectures\n",
    "- **Multi-modal Learning**: Text-image understanding\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Setup Requirements\n",
    "Make sure you have the following libraries installed:\n",
    "- OpenCV (`cv2`) - Computer vision operations\n",
    "- NumPy (`numpy`) - Numerical computations\n",
    "- Matplotlib (`matplotlib`) - Visualization\n",
    "- PyTorch (`torch`) - Deep learning framework\n",
    "- Scikit-learn (`sklearn`) - Machine learning utilities\n",
    "- PIL (`Pillow`) - Image processing\n",
    "- SciPy (`scipy`) - Scientific computing\n",
    "- Skimage (`scikit-image`) - Advanced image processing\n",
    "\n",
    "### üìñ Learning Path\n",
    "This notebook is designed for **progressive learning**. Each section builds upon previous concepts, so we recommend following the order presented. Each topic includes:\n",
    "- üìö **Theoretical foundation**\n",
    "- üíª **Hands-on implementation**\n",
    "- üéØ **Real-world applications**\n",
    "- üî¨ **Advanced techniques**\n",
    "\n",
    "Let's embark on this comprehensive computer vision journey! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6598861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for better visualization\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"üéØ EN3160 Computer Vision Examples - Complete Implementation\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üîß Using PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñºÔ∏è Using OpenCV version: {cv2.__version__}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6338d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries for comprehensive computer vision course\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle, Polygon\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from scipy.interpolate import griddata, interp2d\n",
    "from scipy.spatial.distance import cdist\n",
    "import skimage\n",
    "from skimage import feature, measure, morphology, segmentation\n",
    "from skimage.filters import sobel, gaussian, median\n",
    "from skimage.transform import hough_line, hough_circle, probabilistic_hough_line\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for better visualization\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "print(\"üéØ EN3160 Computer Vision - Complete Implementation\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üîß Using PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñºÔ∏è Using OpenCV version: {cv2.__version__}\")\n",
    "print(f\"üî¨ Using SciPy version: {scipy.__version__}\")\n",
    "print(f\"üé® Using Scikit-image version: {skimage.__version__}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9084e",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Example Images & Setup\n",
    "\n",
    "Let's create and load example images that we'll use throughout this course. We'll generate synthetic images and also show how to load real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d561bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleImages:\n",
    "    \"\"\"\n",
    "    Create and manage example images for computer vision demonstrations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.images = {}\n",
    "        self.create_all_examples()\n",
    "    \n",
    "    def create_all_examples(self):\n",
    "        \"\"\"Create all example images used in the course\"\"\"\n",
    "        print(\"üé® Creating example images for computer vision course...\")\n",
    "        \n",
    "        # Create various test images\n",
    "        self.create_geometric_shapes()\n",
    "        self.create_noisy_images()\n",
    "        self.create_texture_patterns()\n",
    "        self.create_gradient_images()\n",
    "        self.create_checkerboard()\n",
    "        self.create_natural_scene_simulation()\n",
    "        \n",
    "        print(\"‚úÖ All example images created successfully!\")\n",
    "    \n",
    "    def create_geometric_shapes(self):\n",
    "        \"\"\"Create images with basic geometric shapes\"\"\"\n",
    "        # Simple shapes image\n",
    "        img = np.zeros((400, 400, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Draw colorful shapes\n",
    "        cv2.rectangle(img, (50, 50), (150, 150), (255, 0, 0), -1)  # Blue rectangle\n",
    "        cv2.circle(img, (300, 100), 50, (0, 255, 0), -1)  # Green circle\n",
    "        cv2.ellipse(img, (200, 300), (80, 40), 45, 0, 360, (0, 0, 255), -1)  # Red ellipse\n",
    "        \n",
    "        # Add some lines\n",
    "        cv2.line(img, (0, 200), (400, 200), (255, 255, 255), 3)\n",
    "        cv2.line(img, (200, 0), (200, 400), (255, 255, 255), 3)\n",
    "        \n",
    "        # Add text\n",
    "        cv2.putText(img, 'Computer Vision', (10, 380), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        self.images['shapes'] = img\n",
    "        \n",
    "        # Grayscale version\n",
    "        self.images['shapes_gray'] = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    def create_noisy_images(self):\n",
    "        \"\"\"Create images with different types of noise\"\"\"\n",
    "        base_img = np.ones((300, 300), dtype=np.uint8) * 128\n",
    "        \n",
    "        # Gaussian noise\n",
    "        noise = np.random.normal(0, 30, base_img.shape).astype(np.int16)\n",
    "        noisy_gaussian = np.clip(base_img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        self.images['noisy_gaussian'] = noisy_gaussian\n",
    "        \n",
    "        # Salt and pepper noise\n",
    "        noisy_sp = base_img.copy()\n",
    "        salt_pepper = np.random.random(base_img.shape)\n",
    "        noisy_sp[salt_pepper < 0.05] = 0  # Pepper\n",
    "        noisy_sp[salt_pepper > 0.95] = 255  # Salt\n",
    "        self.images['noisy_salt_pepper'] = noisy_sp\n",
    "        \n",
    "        # Uniform noise\n",
    "        noise_uniform = np.random.uniform(-50, 50, base_img.shape)\n",
    "        noisy_uniform = np.clip(base_img + noise_uniform, 0, 255).astype(np.uint8)\n",
    "        self.images['noisy_uniform'] = noisy_uniform\n",
    "    \n",
    "    def create_texture_patterns(self):\n",
    "        \"\"\"Create various texture patterns\"\"\"\n",
    "        size = 256\n",
    "        \n",
    "        # Checkerboard pattern\n",
    "        x, y = np.meshgrid(range(size), range(size))\n",
    "        checkerboard = ((x // 32) + (y // 32)) % 2 * 255\n",
    "        self.images['checkerboard'] = checkerboard.astype(np.uint8)\n",
    "        \n",
    "        # Sinusoidal pattern\n",
    "        x_wave = np.sin(2 * np.pi * x / 50) * 127 + 128\n",
    "        y_wave = np.sin(2 * np.pi * y / 50) * 127 + 128\n",
    "        wave_pattern = ((x_wave + y_wave) / 2).astype(np.uint8)\n",
    "        self.images['wave_pattern'] = wave_pattern\n",
    "        \n",
    "        # Random texture\n",
    "        random_texture = np.random.randint(0, 256, (size, size), dtype=np.uint8)\n",
    "        self.images['random_texture'] = random_texture\n",
    "    \n",
    "    def create_gradient_images(self):\n",
    "        \"\"\"Create gradient images for testing\"\"\"\n",
    "        size = 300\n",
    "        \n",
    "        # Horizontal gradient\n",
    "        h_gradient = np.tile(np.linspace(0, 255, size), (size, 1)).astype(np.uint8)\n",
    "        self.images['horizontal_gradient'] = h_gradient\n",
    "        \n",
    "        # Vertical gradient\n",
    "        v_gradient = np.tile(np.linspace(0, 255, size).reshape(-1, 1), (1, size)).astype(np.uint8)\n",
    "        self.images['vertical_gradient'] = v_gradient\n",
    "        \n",
    "        # Radial gradient\n",
    "        center = size // 2\n",
    "        y, x = np.ogrid[:size, :size]\n",
    "        distance = np.sqrt((x - center)**2 + (y - center)**2)\n",
    "        radial_gradient = (255 * (1 - distance / np.max(distance))).astype(np.uint8)\n",
    "        self.images['radial_gradient'] = radial_gradient\n",
    "    \n",
    "    def create_checkerboard(self):\n",
    "        \"\"\"Create checkerboard pattern for calibration\"\"\"\n",
    "        board_size = (9, 6)  # Internal corners\n",
    "        square_size = 50\n",
    "        \n",
    "        board = np.zeros((board_size[1] * square_size, board_size[0] * square_size), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(board_size[1]):\n",
    "            for j in range(board_size[0]):\n",
    "                if (i + j) % 2 == 0:\n",
    "                    y1, y2 = i * square_size, (i + 1) * square_size\n",
    "                    x1, x2 = j * square_size, (j + 1) * square_size\n",
    "                    board[y1:y2, x1:x2] = 255\n",
    "        \n",
    "        self.images['calibration_board'] = board\n",
    "    \n",
    "    def create_natural_scene_simulation(self):\n",
    "        \"\"\"Create a simulated natural scene\"\"\"\n",
    "        img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Sky gradient (blue to light blue)\n",
    "        for y in range(150):\n",
    "            color_intensity = int(255 * (150 - y) / 150)\n",
    "            img[y, :] = [color_intensity, 200, 255]\n",
    "        \n",
    "        # Ground (green)\n",
    "        img[150:, :] = [34, 139, 34]\n",
    "        \n",
    "        # Add some hills\n",
    "        hill_x = np.arange(600)\n",
    "        hill_y = 150 + 30 * np.sin(hill_x * 0.02) + 20 * np.sin(hill_x * 0.05)\n",
    "        \n",
    "        for x in range(600):\n",
    "            y_start = int(hill_y[x])\n",
    "            img[y_start:, x] = [34, 100, 34]  # Darker green for hills\n",
    "        \n",
    "        # Add a simple house\n",
    "        # House base\n",
    "        cv2.rectangle(img, (200, 200), (300, 300), (139, 69, 19), -1)\n",
    "        # Roof\n",
    "        roof_points = np.array([[180, 200], [250, 150], [320, 200]], np.int32)\n",
    "        cv2.fillPoly(img, [roof_points], (160, 82, 45))\n",
    "        # Door\n",
    "        cv2.rectangle(img, (230, 250), (270, 300), (101, 67, 33), -1)\n",
    "        # Window\n",
    "        cv2.rectangle(img, (210, 220), (240, 240), (173, 216, 230), -1)\n",
    "        \n",
    "        # Add a tree\n",
    "        cv2.circle(img, (450, 200), 40, (34, 139, 34), -1)  # Tree crown\n",
    "        cv2.rectangle(img, (445, 240), (455, 300), (139, 69, 19), -1)  # Trunk\n",
    "        \n",
    "        # Add some clouds\n",
    "        cv2.ellipse(img, (100, 50), (40, 20), 0, 0, 360, (255, 255, 255), -1)\n",
    "        cv2.ellipse(img, (500, 70), (60, 25), 0, 0, 360, (255, 255, 255), -1)\n",
    "        \n",
    "        self.images['natural_scene'] = img\n",
    "        self.images['natural_scene_gray'] = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    def load_sample_image_from_url(self, url, name):\n",
    "        \"\"\"Load an image from URL (for demonstration)\"\"\"\n",
    "        try:\n",
    "            import urllib.request\n",
    "            from PIL import Image\n",
    "            import io\n",
    "            \n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                image_data = response.read()\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_data))\n",
    "            \n",
    "            # Convert to OpenCV format\n",
    "            opencv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            self.images[name] = opencv_image\n",
    "            print(f\"‚úÖ Loaded image '{name}' from URL\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load image from URL: {e}\")\n",
    "            print(\"üí° Using synthetic image instead\")\n",
    "            # Fallback to synthetic image\n",
    "            if name not in self.images:\n",
    "                self.images[name] = self.images['natural_scene']\n",
    "    \n",
    "    def get_image(self, name):\n",
    "        \"\"\"Get a specific image by name\"\"\"\n",
    "        if name in self.images:\n",
    "            return self.images[name].copy()\n",
    "        else:\n",
    "            print(f\"‚ùå Image '{name}' not found. Available images: {list(self.images.keys())}\")\n",
    "            return self.images['shapes']  # Return default image\n",
    "    \n",
    "    def show_all_images(self):\n",
    "        \"\"\"Display all created example images\"\"\"\n",
    "        print(\"üñºÔ∏è Displaying all example images...\")\n",
    "        \n",
    "        # Calculate grid size\n",
    "        num_images = len(self.images)\n",
    "        cols = 4\n",
    "        rows = (num_images + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
    "        if rows == 1:\n",
    "            axes = [axes]\n",
    "        if cols == 1:\n",
    "            axes = [[ax] for ax in axes]\n",
    "        \n",
    "        image_names = list(self.images.keys())\n",
    "        \n",
    "        for i, (ax_row) in enumerate(axes):\n",
    "            for j, ax in enumerate(ax_row):\n",
    "                idx = i * cols + j\n",
    "                if idx < len(image_names):\n",
    "                    name = image_names[idx]\n",
    "                    img = self.images[name]\n",
    "                    \n",
    "                    if len(img.shape) == 3:  # Color image\n",
    "                        img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    else:  # Grayscale\n",
    "                        img_display = img\n",
    "                        \n",
    "                    ax.imshow(img_display, cmap='gray' if len(img.shape) == 2 else None)\n",
    "                    ax.set_title(name.replace('_', ' ').title())\n",
    "                    ax.axis('off')\n",
    "                else:\n",
    "                    ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ All images displayed!\")\n",
    "        print(f\"üìä Total images created: {len(self.images)}\")\n",
    "\n",
    "# Create the example images instance\n",
    "print(\"üöÄ Initializing Example Images...\")\n",
    "example_images = ExampleImages()\n",
    "\n",
    "# Display all created images\n",
    "example_images.show_all_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c82d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Integration Helper: Connect Example Images to All Demonstrations\n",
    "\n",
    "def get_demo_image(image_type='default'):\n",
    "    \"\"\"\n",
    "    Helper function to get appropriate images for demonstrations\n",
    "    This ensures all our examples use proper images instead of empty arrays\n",
    "    \"\"\"\n",
    "    if image_type == 'color':\n",
    "        return example_images.get_image('shapes')\n",
    "    elif image_type == 'gray':\n",
    "        return example_images.get_image('shapes_gray')\n",
    "    elif image_type == 'natural':\n",
    "        return example_images.get_image('natural_scene')\n",
    "    elif image_type == 'natural_gray':\n",
    "        return example_images.get_image('natural_scene_gray')\n",
    "    elif image_type == 'noisy':\n",
    "        return example_images.get_image('noisy_gaussian')\n",
    "    elif image_type == 'texture':\n",
    "        return example_images.get_image('wave_pattern')\n",
    "    elif image_type == 'checkerboard':\n",
    "        return example_images.get_image('checkerboard')\n",
    "    elif image_type == 'gradient':\n",
    "        return example_images.get_image('horizontal_gradient')\n",
    "    else:\n",
    "        return example_images.get_image('shapes')\n",
    "\n",
    "# üéØ Update all demonstration classes to use real images\n",
    "print(\"üîó Setting up image integration for all demonstrations...\")\n",
    "\n",
    "# Test the integration\n",
    "test_img = get_demo_image('color')\n",
    "test_gray = get_demo_image('gray')\n",
    "\n",
    "print(f\"‚úÖ Color image shape: {test_img.shape}\")\n",
    "print(f\"‚úÖ Gray image shape: {test_gray.shape}\")\n",
    "print(\"üéâ Image integration ready - all demos will now use real images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795df27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Quick Demonstration: Example Images in Action\n",
    "print(\"üöÄ DEMO: Testing Example Images with Basic Operations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get a color image for testing\n",
    "demo_img = get_demo_image('natural')\n",
    "\n",
    "# Apply some basic operations to show they work\n",
    "gray_converted = cv2.cvtColor(demo_img, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(demo_img, (15, 15), 0)\n",
    "edges = cv2.Canny(gray_converted, 50, 150)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(cv2.cvtColor(demo_img, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Original Natural Scene')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Grayscale\n",
    "axes[0, 1].imshow(gray_converted, cmap='gray')\n",
    "axes[0, 1].set_title('Grayscale Conversion')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Blurred\n",
    "axes[1, 0].imshow(cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 0].set_title('Gaussian Blur')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Edge Detection\n",
    "axes[1, 1].imshow(edges, cmap='gray')\n",
    "axes[1, 1].set_title('Canny Edge Detection')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Example images are working perfectly!\")\n",
    "print(\"üéâ All computer vision operations can now use real images!\")\n",
    "print(\"üìù Note: All the following demonstrations will use these example images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba88aa",
   "metadata": {},
   "source": [
    "# üé® Enhanced Image Processing with Real Examples\n",
    "\n",
    "Now that we have our example images set up, let's demonstrate some fundamental image processing operations using real images instead of synthetic ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedImageProcessing:\n",
    "    \"\"\"\n",
    "    Enhanced image processing demonstrations using real example images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Enhanced Image Processing\"\n",
    "    \n",
    "    def filtering_operations_demo(self):\n",
    "        \"\"\"Demonstrate various filtering operations on real images\"\"\"\n",
    "        print(\"üéØ DEMO: Filtering Operations on Real Images\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Get our natural scene image\n",
    "        img = get_demo_image('natural')\n",
    "        \n",
    "        # Apply different filters\n",
    "        # Gaussian Blur - smoothing\n",
    "        gaussian_blur = cv2.GaussianBlur(img, (15, 15), 0)\n",
    "        \n",
    "        # Motion Blur - simulating camera shake\n",
    "        kernel_motion = np.zeros((15, 15))\n",
    "        kernel_motion[7, :] = np.ones(15)\n",
    "        kernel_motion = kernel_motion / 15\n",
    "        motion_blur = cv2.filter2D(img, -1, kernel_motion)\n",
    "        \n",
    "        # Sharpening filter\n",
    "        sharpen_kernel = np.array([[-1, -1, -1],\n",
    "                                  [-1,  9, -1],\n",
    "                                  [-1, -1, -1]])\n",
    "        sharpened = cv2.filter2D(img, -1, sharpen_kernel)\n",
    "        \n",
    "        # Edge enhancement\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        # Display results\n",
    "        images = [img, gaussian_blur, motion_blur, sharpened, edges_colored]\n",
    "        titles = ['Original', 'Gaussian Blur', 'Motion Blur', 'Sharpened', 'Edge Detection']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "        \n",
    "        for i, (image, title) in enumerate(zip(images, titles)):\n",
    "            if len(image.shape) == 3:\n",
    "                axes[i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                axes[i].imshow(image, cmap='gray')\n",
    "            axes[i].set_title(title)\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Filtering operations completed!\")\n",
    "        print(\"üí° Gaussian blur: smooths image, reduces noise\")\n",
    "        print(\"üí° Motion blur: simulates movement\")\n",
    "        print(\"üí° Sharpening: enhances edges and details\")\n",
    "        print(\"üí° Edge detection: finds boundaries between regions\")\n",
    "    \n",
    "    def color_space_transformations(self):\n",
    "        \"\"\"Demonstrate color space transformations\"\"\"\n",
    "        print(\"üåà DEMO: Color Space Transformations\")\n",
    "        print(\"=\" * 38)\n",
    "        \n",
    "        # Get color image\n",
    "        img = get_demo_image('color')\n",
    "        \n",
    "        # Convert to different color spaces\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Original RGB\n",
    "        axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 0].set_title('Original (RGB)')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # HSV\n",
    "        axes[0, 1].imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB))\n",
    "        axes[0, 1].set_title('HSV Color Space')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # LAB\n",
    "        axes[0, 2].imshow(cv2.cvtColor(lab, cv2.COLOR_LAB2RGB))\n",
    "        axes[0, 2].set_title('LAB Color Space')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        # Individual HSV channels\n",
    "        axes[1, 0].imshow(hsv[:, :, 0], cmap='hsv')\n",
    "        axes[1, 0].set_title('HSV - Hue Channel')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(hsv[:, :, 1], cmap='gray')\n",
    "        axes[1, 1].set_title('HSV - Saturation Channel')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(gray, cmap='gray')\n",
    "        axes[1, 2].set_title('Grayscale')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Color space transformations completed!\")\n",
    "        print(\"üí° HSV: Good for color-based segmentation\")\n",
    "        print(\"üí° LAB: Perceptually uniform color space\")\n",
    "        print(\"üí° Grayscale: Intensity information only\")\n",
    "    \n",
    "    def morphological_operations(self):\n",
    "        \"\"\"Demonstrate morphological operations\"\"\"\n",
    "        print(\"üîß DEMO: Morphological Operations\")\n",
    "        print(\"=\" * 34)\n",
    "        \n",
    "        # Get shapes image and convert to binary\n",
    "        img = get_demo_image('shapes_gray')\n",
    "        _, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Define kernel\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        \n",
    "        # Apply morphological operations\n",
    "        erosion = cv2.erode(binary, kernel, iterations=1)\n",
    "        dilation = cv2.dilate(binary, kernel, iterations=1)\n",
    "        opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "        closing = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Display results\n",
    "        images = [binary, erosion, dilation, opening, closing]\n",
    "        titles = ['Original Binary', 'Erosion', 'Dilation', 'Opening', 'Closing']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "        \n",
    "        for i, (image, title) in enumerate(zip(images, titles)):\n",
    "            axes[i].imshow(image, cmap='gray')\n",
    "            axes[i].set_title(title)\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Morphological operations completed!\")\n",
    "        print(\"üí° Erosion: shrinks white regions\")\n",
    "        print(\"üí° Dilation: expands white regions\")\n",
    "        print(\"üí° Opening: erosion followed by dilation (removes noise)\")\n",
    "        print(\"üí° Closing: dilation followed by erosion (fills gaps)\")\n",
    "    \n",
    "    def histogram_analysis(self):\n",
    "        \"\"\"Demonstrate histogram analysis and equalization\"\"\"\n",
    "        print(\"üìä DEMO: Histogram Analysis\")\n",
    "        print(\"=\" * 28)\n",
    "        \n",
    "        # Get natural scene images with different characteristics\n",
    "        bright_img = get_demo_image('natural')\n",
    "        \n",
    "        # Create a darker version for comparison\n",
    "        dark_img = cv2.convertScaleAbs(bright_img, alpha=0.5, beta=-50)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        bright_gray = cv2.cvtColor(bright_img, cv2.COLOR_BGR2GRAY)\n",
    "        dark_gray = cv2.cvtColor(dark_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply histogram equalization\n",
    "        bright_eq = cv2.equalizeHist(bright_gray)\n",
    "        dark_eq = cv2.equalizeHist(dark_gray)\n",
    "        \n",
    "        # Calculate histograms\n",
    "        hist_bright = cv2.calcHist([bright_gray], [0], None, [256], [0, 256])\n",
    "        hist_dark = cv2.calcHist([dark_gray], [0], None, [256], [0, 256])\n",
    "        hist_bright_eq = cv2.calcHist([bright_eq], [0], None, [256], [0, 256])\n",
    "        hist_dark_eq = cv2.calcHist([dark_eq], [0], None, [256], [0, 256])\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        \n",
    "        # Images\n",
    "        axes[0, 0].imshow(bright_gray, cmap='gray')\n",
    "        axes[0, 0].set_title('Bright Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(dark_gray, cmap='gray')\n",
    "        axes[0, 1].set_title('Dark Image')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(bright_eq, cmap='gray')\n",
    "        axes[0, 2].set_title('Bright Equalized')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        axes[0, 3].imshow(dark_eq, cmap='gray')\n",
    "        axes[0, 3].set_title('Dark Equalized')\n",
    "        axes[0, 3].axis('off')\n",
    "        \n",
    "        # Original histograms\n",
    "        axes[1, 0].plot(hist_bright, color='blue')\n",
    "        axes[1, 0].set_title('Bright Histogram')\n",
    "        axes[1, 0].set_xlim([0, 256])\n",
    "        \n",
    "        axes[1, 1].plot(hist_dark, color='red')\n",
    "        axes[1, 1].set_title('Dark Histogram')\n",
    "        axes[1, 1].set_xlim([0, 256])\n",
    "        \n",
    "        # Equalized histograms\n",
    "        axes[1, 2].plot(hist_bright_eq, color='blue')\n",
    "        axes[1, 2].set_title('Bright Eq. Histogram')\n",
    "        axes[1, 2].set_xlim([0, 256])\n",
    "        \n",
    "        axes[1, 3].plot(hist_dark_eq, color='red')\n",
    "        axes[1, 3].set_title('Dark Eq. Histogram')\n",
    "        axes[1, 3].set_xlim([0, 256])\n",
    "        \n",
    "        # Hide empty subplot row\n",
    "        for i in range(4):\n",
    "            axes[2, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Histogram analysis completed!\")\n",
    "        print(\"üí° Histograms show intensity distribution\")\n",
    "        print(\"üí° Equalization spreads intensities across full range\")\n",
    "        print(\"üí° Improves contrast in low-contrast images\")\n",
    "\n",
    "# Create instance and run demonstrations\n",
    "enhanced_processing = EnhancedImageProcessing()\n",
    "print(\"üé® Enhanced Image Processing demonstrations ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c598f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Run Enhanced Image Processing Demonstrations\n",
    "\n",
    "print(\"üéâ RUNNING ENHANCED IMAGE PROCESSING DEMOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run filtering operations\n",
    "enhanced_processing.filtering_operations_demo()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Run color space transformations  \n",
    "enhanced_processing.color_space_transformations()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Run morphological operations\n",
    "enhanced_processing.morphological_operations()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Run histogram analysis\n",
    "enhanced_processing.histogram_analysis()\n",
    "\n",
    "print(\"\\nüéä ALL ENHANCED DEMONSTRATIONS COMPLETED!\")\n",
    "print(\"üí° Your notebook now has real images for all computer vision operations!\")\n",
    "print(\"üìö You can run any of the individual demo methods above separately too.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b4cb6",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Image Processing and Early Vision\n",
    "\n",
    "## ü§î What is Image Processing?\n",
    "\n",
    "**Image Processing** is the foundation of computer vision! Think of it as teaching computers to \"see\" and understand images the way humans do. Just like how our eyes and brain work together to process visual information, image processing involves manipulating digital images to:\n",
    "\n",
    "- **Enhance image quality** (remove noise, improve brightness)\n",
    "- **Extract useful information** (find edges, detect shapes)\n",
    "- **Prepare images for analysis** (resize, convert formats)\n",
    "\n",
    "## üß† Why Do We Need Image Processing?\n",
    "\n",
    "### Real-World Problems:\n",
    "1. **Medical Imaging**: Enhance X-rays to help doctors spot diseases\n",
    "2. **Autonomous Cars**: Detect lane lines and traffic signs\n",
    "3. **Security**: Facial recognition in surveillance systems\n",
    "4. **Photography**: Instagram filters and photo editing apps\n",
    "5. **Manufacturing**: Quality control in production lines\n",
    "\n",
    "## üìö Key Concepts Explained:\n",
    "\n",
    "### üîß **Filtering** - Cleaning Up Images\n",
    "- **What it does**: Removes unwanted noise (like static on old TV)\n",
    "- **Why it matters**: Clean images = better analysis\n",
    "- **Real example**: Removing graininess from low-light photos\n",
    "\n",
    "### üéØ **Edge Detection** - Finding Boundaries\n",
    "- **What it does**: Identifies where objects begin and end\n",
    "- **Why it matters**: Edges help us recognize shapes and objects\n",
    "- **Real example**: Outlining a person in a photo for background removal\n",
    "\n",
    "### üîç **Feature Extraction** - Finding Important Details\n",
    "- **What it does**: Identifies key characteristics (corners, textures, patterns)\n",
    "- **Why it matters**: Features are like fingerprints for objects\n",
    "- **Real example**: Recognizing your face in a group photo\n",
    "\n",
    "## üõ†Ô∏è Tools We'll Use:\n",
    "\n",
    "### **Gaussian Blur**\n",
    "- **Purpose**: Smooths images, reduces noise\n",
    "- **How it works**: Averages each pixel with its neighbors\n",
    "- **When to use**: When images are too noisy or need softening\n",
    "\n",
    "### **Median Filter**\n",
    "- **Purpose**: Removes \"salt and pepper\" noise (random black/white dots)\n",
    "- **How it works**: Replaces each pixel with the median value of surrounding pixels\n",
    "- **When to use**: When dealing with impulsive noise\n",
    "\n",
    "### **Sobel Operators**\n",
    "- **Purpose**: Detects edges in specific directions\n",
    "- **How it works**: Uses mathematical kernels to find intensity changes\n",
    "- **When to use**: When you need to find vertical or horizontal edges\n",
    "\n",
    "### **Canny Edge Detection**\n",
    "- **Purpose**: Most advanced edge detector\n",
    "- **How it works**: Multi-step process with noise reduction and edge thinning\n",
    "- **When to use**: When you need the most accurate edge detection\n",
    "\n",
    "Let's start with hands-on examples to see these concepts in action! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3456ed",
   "metadata": {},
   "source": [
    "# üî¢ Sampling & Interpolation - From Continuous to Digital\n",
    "\n",
    "## ü§î What is Sampling & Interpolation?\n",
    "\n",
    "**Sampling** is how we convert the continuous, analog world into the discrete, digital world that computers can understand. **Interpolation** is the reverse - estimating missing values between known data points.\n",
    "\n",
    "### **Real-World Analogy**:\n",
    "- **Sampling**: Like taking snapshots of a movie - you capture discrete moments in time\n",
    "- **Interpolation**: Like creating smooth slow-motion between those snapshots\n",
    "\n",
    "## üìä Key Concepts:\n",
    "\n",
    "### **Digital Image Formation**:\n",
    "- **Continuous scene** ‚Üí **Discrete pixel grid**\n",
    "- **Spatial sampling**: How many pixels per inch (resolution)\n",
    "- **Quantization**: How many intensity levels per pixel (bit depth)\n",
    "\n",
    "### **Sampling Theory (Nyquist-Shannon)**:\n",
    "- **Nyquist frequency**: Minimum sampling rate to avoid aliasing\n",
    "- **Aliasing**: When high frequencies appear as low frequencies (like wagon wheels spinning backward in movies)\n",
    "- **Anti-aliasing**: Techniques to reduce sampling artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingInterpolationExamples:\n",
    "    \"\"\"\n",
    "    Sampling & Interpolation: Converting between continuous and discrete representations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Sampling and Interpolation\"\n",
    "    \n",
    "    def demonstrate_sampling_effects(self):\n",
    "        \"\"\"Show effects of different sampling rates\"\"\"\n",
    "        print(\"üîç EXAMPLE: Sampling Rate Effects\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create a high-resolution test image\n",
    "        x = np.linspace(0, 4*np.pi, 400)\n",
    "        y = np.linspace(0, 4*np.pi, 400)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Create a pattern with multiple frequencies\n",
    "        pattern = np.sin(X) * np.cos(Y) + 0.5 * np.sin(3*X) * np.sin(3*Y)\n",
    "        pattern = (pattern + 2) / 4  # Normalize to [0,1]\n",
    "        \n",
    "        # Demonstrate different sampling rates\n",
    "        sampling_factors = [1, 2, 4, 8]  # 1 = original, higher = more downsampling\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        for i, factor in enumerate(sampling_factors):\n",
    "            # Downsample\n",
    "            downsampled = pattern[::factor, ::factor]\n",
    "            \n",
    "            # Show downsampled version\n",
    "            axes[0, i].imshow(downsampled, cmap='gray')\n",
    "            axes[0, i].set_title(f'Sampling Factor: {factor}\\\\n{downsampled.shape[0]}x{downsampled.shape[1]} pixels')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Upsample back to original size using different interpolation methods\n",
    "            upsampled = cv2.resize(downsampled, (400, 400), interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            axes[1, i].imshow(upsampled, cmap='gray')\n",
    "            axes[1, i].set_title(f'Upsampled (Cubic)')\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Sampling demonstration completed!\")\n",
    "        print(\"üí° Notice how higher sampling factors lose detail\")\n",
    "        print(\"üí° Interpolation can't recover lost information\")\n",
    "    \n",
    "    def interpolation_methods_comparison(self):\n",
    "        \"\"\"Compare different interpolation methods\"\"\"\n",
    "        print(\"üöÄ ADVANCED: Interpolation Methods Comparison\")\n",
    "        print(\"-\" * 48)\n",
    "        \n",
    "        # Create a small test image with sharp features\n",
    "        small_img = np.zeros((10, 10))\n",
    "        small_img[2:8, 2:8] = 1.0  # White square\n",
    "        small_img[4:6, 4:6] = 0.5  # Gray center\n",
    "        \n",
    "        # Interpolation methods\n",
    "        methods = {\n",
    "            'Nearest Neighbor': cv2.INTER_NEAREST,\n",
    "            'Bilinear': cv2.INTER_LINEAR,\n",
    "            'Bicubic': cv2.INTER_CUBIC,\n",
    "            'Lanczos': cv2.INTER_LANCZOS4\n",
    "        }\n",
    "        \n",
    "        target_size = (100, 100)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Show original\n",
    "        axes[0, 0].imshow(small_img, cmap='gray')\n",
    "        axes[0, 0].set_title('Original (10x10)')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Show interpolated versions\n",
    "        for i, (name, method) in enumerate(methods.items()):\n",
    "            row = (i + 1) // 3\n",
    "            col = (i + 1) % 3\n",
    "            \n",
    "            interpolated = cv2.resize(small_img, target_size, interpolation=method)\n",
    "            \n",
    "            axes[row, col].imshow(interpolated, cmap='gray')\n",
    "            axes[row, col].set_title(f'{name}')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Interpolation comparison completed!\")\n",
    "        print(\"üí° Nearest neighbor: Blocky but preserves sharp edges\")\n",
    "        print(\"üí° Bilinear: Smoother but can blur details\")\n",
    "        print(\"üí° Bicubic: Good balance of smoothness and sharpness\")\n",
    "        print(\"üí° Lanczos: Best quality but most computationally expensive\")\n",
    "\n",
    "# Create instance and demonstrate\n",
    "sampling_demo = SamplingInterpolationExamples()\n",
    "print(\"üî¢ Sampling & Interpolation class initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b59bf1",
   "metadata": {},
   "source": [
    "# üé® Point Operations - Pixel-by-Pixel Transformations\n",
    "\n",
    "## ü§î What are Point Operations?\n",
    "\n",
    "**Point Operations** transform each pixel independently based only on its own value. Think of them as applying the same mathematical function to every pixel in the image.\n",
    "\n",
    "### **Key Characteristics**:\n",
    "- **Local**: Each pixel is processed independently\n",
    "- **Memory-less**: No need to consider neighboring pixels\n",
    "- **Fast**: Highly parallelizable operations\n",
    "- **Reversible**: Many can be undone (if no clipping occurs)\n",
    "\n",
    "## üõ†Ô∏è Common Point Operations:\n",
    "\n",
    "### **Brightness Adjustment**:\n",
    "- **Formula**: `new_pixel = old_pixel + constant`\n",
    "- **Effect**: Shifts entire histogram left/right\n",
    "- **Use case**: Correcting under/over-exposed images\n",
    "\n",
    "### **Contrast Adjustment**:\n",
    "- **Formula**: `new_pixel = gain * old_pixel`\n",
    "- **Effect**: Stretches/compresses histogram\n",
    "- **Use case**: Improving image dynamic range\n",
    "\n",
    "### **Gamma Correction**:\n",
    "- **Formula**: `new_pixel = old_pixel^(1/gamma)`\n",
    "- **Effect**: Non-linear brightness adjustment\n",
    "- **Use case**: Compensating for display characteristics\n",
    "\n",
    "### **Histogram Equalization**:\n",
    "- **Goal**: Distribute intensities more evenly\n",
    "- **Effect**: Improves contrast in low-contrast images\n",
    "- **Use case**: Medical imaging, surveillance footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointOperationsExamples:\n",
    "    \"\"\"\n",
    "    Point Operations: Pixel-by-pixel transformations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Point Operations\"\n",
    "    \n",
    "    def create_test_image(self):\n",
    "        \"\"\"Create a test image with various intensity regions\"\"\"\n",
    "        img = np.zeros((300, 400), dtype=np.uint8)\n",
    "        \n",
    "        # Create regions with different intensities\n",
    "        img[50:100, 50:150] = 50   # Dark region\n",
    "        img[50:100, 200:300] = 128  # Medium region\n",
    "        img[150:200, 50:150] = 200  # Bright region\n",
    "        img[150:200, 200:300] = 255  # Very bright region\n",
    "        \n",
    "        # Add some gradient\n",
    "        gradient = np.linspace(0, 255, 400).astype(np.uint8)\n",
    "        img[250:280, :] = gradient\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def demonstrate_point_operations(self):\n",
    "        \"\"\"Demonstrate various point operations\"\"\"\n",
    "        print(\"üé® EXAMPLE: Point Operations\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        img = self.create_test_image()\n",
    "        \n",
    "        # Apply different point operations\n",
    "        # 1. Brightness adjustment\n",
    "        bright_img = cv2.add(img, np.full_like(img, 50))  # Add 50 to all pixels\n",
    "        dark_img = cv2.subtract(img, np.full_like(img, 50))  # Subtract 50 from all pixels\n",
    "        \n",
    "        # 2. Contrast adjustment\n",
    "        contrast_high = cv2.multiply(img, 1.5)  # Increase contrast\n",
    "        contrast_low = cv2.multiply(img, 0.5)   # Decrease contrast\n",
    "        \n",
    "        # 3. Gamma correction\n",
    "        gamma_table = np.array([((i / 255.0) ** (1.0 / 0.5)) * 255 for i in range(256)]).astype(np.uint8)\n",
    "        gamma_corrected = cv2.LUT(img, gamma_table)\n",
    "        \n",
    "        # 4. Histogram equalization\n",
    "        hist_eq = cv2.equalizeHist(img)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "        \n",
    "        # Original and its histogram\n",
    "        axes[0, 0].imshow(img, cmap='gray')\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].hist(img.flatten(), bins=50, alpha=0.7, color='blue')\n",
    "        axes[0, 1].set_title('Original Histogram')\n",
    "        axes[0, 1].set_xlabel('Intensity')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        # Brightness adjustments\n",
    "        axes[0, 2].imshow(bright_img, cmap='gray')\n",
    "        axes[0, 2].set_title('Brightness +50')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        axes[1, 0].imshow(dark_img, cmap='gray')\n",
    "        axes[1, 0].set_title('Brightness -50')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Contrast adjustments\n",
    "        axes[1, 1].imshow(contrast_high, cmap='gray')\n",
    "        axes[1, 1].set_title('High Contrast (√ó1.5)')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(contrast_low, cmap='gray')\n",
    "        axes[1, 2].set_title('Low Contrast (√ó0.5)')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        # Gamma correction\n",
    "        axes[2, 0].imshow(gamma_corrected, cmap='gray')\n",
    "        axes[2, 0].set_title('Gamma Correction (Œ≥=0.5)')\n",
    "        axes[2, 0].axis('off')\n",
    "        \n",
    "        # Histogram equalization\n",
    "        axes[2, 1].imshow(hist_eq, cmap='gray')\n",
    "        axes[2, 1].set_title('Histogram Equalized')\n",
    "        axes[2, 1].axis('off')\n",
    "        \n",
    "        # Histogram equalization histogram\n",
    "        axes[2, 2].hist(hist_eq.flatten(), bins=50, alpha=0.7, color='green')\n",
    "        axes[2, 2].set_title('Equalized Histogram')\n",
    "        axes[2, 2].set_xlabel('Intensity')\n",
    "        axes[2, 2].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Point operations demonstration completed!\")\n",
    "        print(\"üí° Brightness: Shifts histogram left/right\")\n",
    "        print(\"üí° Contrast: Stretches/compresses histogram\")\n",
    "        print(\"üí° Gamma: Non-linear intensity mapping\")\n",
    "        print(\"üí° Histogram equalization: Spreads intensities evenly\")\n",
    "    \n",
    "    def adaptive_histogram_equalization(self):\n",
    "        \"\"\"Demonstrate Contrast Limited Adaptive Histogram Equalization (CLAHE)\"\"\"\n",
    "        print(\"üöÄ ADVANCED: Adaptive Histogram Equalization\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Create an image with varying local contrast\n",
    "        img = np.zeros((300, 300), dtype=np.uint8)\n",
    "        \n",
    "        # Different regions with different local contrasts\n",
    "        img[50:150, 50:150] = np.random.randint(40, 60, (100, 100))    # Low contrast dark\n",
    "        img[50:150, 200:300] = np.random.randint(100, 120, (100, 100))  # Low contrast medium\n",
    "        img[200:300, 50:150] = np.random.randint(190, 210, (100, 100))  # Low contrast bright\n",
    "        img[200:300, 200:300] = np.random.randint(0, 255, (100, 100))   # High contrast\n",
    "        \n",
    "        # Apply global histogram equalization\n",
    "        global_eq = cv2.equalizeHist(img)\n",
    "        \n",
    "        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        clahe_eq = clahe.apply(img)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        axes[0, 0].imshow(img, cmap='gray')\n",
    "        axes[0, 0].set_title('Original')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(global_eq, cmap='gray')\n",
    "        axes[0, 1].set_title('Global Histogram Equalization')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(clahe_eq, cmap='gray')\n",
    "        axes[0, 2].set_title('CLAHE')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        # Histograms\n",
    "        axes[1, 0].hist(img.flatten(), bins=50, alpha=0.7, color='blue')\n",
    "        axes[1, 0].set_title('Original Histogram')\n",
    "        \n",
    "        axes[1, 1].hist(global_eq.flatten(), bins=50, alpha=0.7, color='red')\n",
    "        axes[1, 1].set_title('Global Eq. Histogram')\n",
    "        \n",
    "        axes[1, 2].hist(clahe_eq.flatten(), bins=50, alpha=0.7, color='green')\n",
    "        axes[1, 2].set_title('CLAHE Histogram')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ CLAHE demonstration completed!\")\n",
    "        print(\"üí° Global equalization can over-enhance some regions\")\n",
    "        print(\"üí° CLAHE adapts to local image characteristics\")\n",
    "        print(\"üí° Clip limit prevents over-amplification of noise\")\n",
    "\n",
    "# Create instance and demonstrate\n",
    "point_ops_demo = PointOperationsExamples()\n",
    "print(\"üé® Point Operations class initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd159a79",
   "metadata": {},
   "source": [
    "# üåä Optical Flow - Motion Estimation\n",
    "\n",
    "## ü§î What is Optical Flow?\n",
    "\n",
    "**Optical Flow** estimates the motion of pixels between consecutive frames in a video sequence. It's like tracking how every point in an image moves from one frame to the next.\n",
    "\n",
    "### **Applications**:\n",
    "- **Video compression**: Only encode motion vectors instead of full frames\n",
    "- **Motion detection**: Security systems, activity recognition\n",
    "- **Object tracking**: Following specific objects through video\n",
    "- **Autonomous vehicles**: Understanding scene dynamics\n",
    "- **Medical imaging**: Tracking heart wall motion, blood flow\n",
    "\n",
    "## üî¨ Mathematical Foundation:\n",
    "\n",
    "### **Optical Flow Constraint Equation**:\n",
    "**Assumption**: Pixel intensities remain constant as they move\n",
    "**Equation**: `I(x,y,t) = I(x+dx, y+dy, t+dt)`\n",
    "\n",
    "### **Lucas-Kanade Method**:\n",
    "- **Assumption**: Flow is constant in local neighborhood\n",
    "- **Solves**: Small local motion accurately\n",
    "- **Good for**: Sparse features, real-time applications\n",
    "\n",
    "### **Horn-Schunck Method**:\n",
    "- **Assumption**: Flow varies smoothly everywhere\n",
    "- **Solves**: Dense flow field\n",
    "- **Good for**: Global motion understanding\n",
    "\n",
    "## üéØ Types of Optical Flow:\n",
    "\n",
    "### **Sparse Optical Flow**:\n",
    "- **Tracks**: Only important features (corners, edges)\n",
    "- **Advantage**: Fast, robust\n",
    "- **Use case**: Feature tracking, object following\n",
    "\n",
    "### **Dense Optical Flow**:\n",
    "- **Tracks**: Every pixel in the image\n",
    "- **Advantage**: Complete motion field\n",
    "- **Use case**: Motion analysis, video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpticalFlowExamples:\n",
    "    \"\"\"\n",
    "    Optical Flow: Motion estimation between video frames\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Optical Flow\"\n",
    "    \n",
    "    def create_moving_objects_sequence(self):\n",
    "        \"\"\"Create a synthetic video sequence with moving objects\"\"\"\n",
    "        frames = []\n",
    "        num_frames = 10\n",
    "        \n",
    "        for t in range(num_frames):\n",
    "            frame = np.zeros((300, 400), dtype=np.uint8)\n",
    "            \n",
    "            # Moving circle\n",
    "            circle_x = 50 + t * 15\n",
    "            circle_y = 150\n",
    "            cv2.circle(frame, (circle_x, circle_y), 20, 255, -1)\n",
    "            \n",
    "            # Moving rectangle\n",
    "            rect_x = 300 - t * 10\n",
    "            rect_y = 80 + t * 5\n",
    "            cv2.rectangle(frame, (rect_x, rect_y), (rect_x + 40, rect_y + 30), 128, -1)\n",
    "            \n",
    "            # Rotating line\n",
    "            center = (200, 200)\n",
    "            angle = t * 20  # degrees\n",
    "            length = 50\n",
    "            end_x = int(center[0] + length * np.cos(np.radians(angle)))\n",
    "            end_y = int(center[1] + length * np.sin(np.radians(angle)))\n",
    "            cv2.line(frame, center, (end_x, end_y), 180, 3)\n",
    "            \n",
    "            frames.append(frame)\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def lucas_kanade_optical_flow(self):\n",
    "        \"\"\"Demonstrate Lucas-Kanade sparse optical flow\"\"\"\n",
    "        print(\"üåä EXAMPLE: Lucas-Kanade Optical Flow\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create synthetic sequence\n",
    "        frames = self.create_moving_objects_sequence()\n",
    "        \n",
    "        # Parameters for corner detection\n",
    "        feature_params = dict(\n",
    "            maxCorners=100,\n",
    "            qualityLevel=0.3,\n",
    "            minDistance=7,\n",
    "            blockSize=7\n",
    "        )\n",
    "        \n",
    "        # Parameters for Lucas-Kanade optical flow\n",
    "        lk_params = dict(\n",
    "            winSize=(15, 15),\n",
    "            maxLevel=2,\n",
    "            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "        )\n",
    "        \n",
    "        # Take first frame and find corners\n",
    "        old_frame = frames[0]\n",
    "        p0 = cv2.goodFeaturesToTrack(old_frame, mask=None, **feature_params)\n",
    "        \n",
    "        # Create random colors for tracks\n",
    "        colors = np.random.randint(0, 255, (100, 3))\n",
    "        \n",
    "        # Create a mask image for drawing purposes\n",
    "        mask = np.zeros_like(cv2.cvtColor(old_frame, cv2.COLOR_GRAY2BGR))\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, frame in enumerate(frames[1:], 1):\n",
    "            frame_gray = frame\n",
    "            \n",
    "            # Calculate optical flow\n",
    "            if p0 is not None and len(p0) > 0:\n",
    "                p1, st, err = cv2.calcOpticalFlowPyrLK(old_frame, frame_gray, p0, None, **lk_params)\n",
    "                \n",
    "                # Select good points\n",
    "                if p1 is not None:\n",
    "                    good_new = p1[st == 1]\n",
    "                    good_old = p0[st == 1]\n",
    "                    \n",
    "                    # Draw the tracks\n",
    "                    for j, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                        a, b = new.ravel().astype(int)\n",
    "                        c, d = old.ravel().astype(int)\n",
    "                        mask = cv2.line(mask, (a, b), (c, d), colors[j].tolist(), 2)\n",
    "                        frame = cv2.circle(frame, (a, b), 5, colors[j].tolist(), -1)\n",
    "                    \n",
    "                    img = cv2.add(cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR), mask)\n",
    "                    results.append(img)\n",
    "                    \n",
    "                    # Update the previous frame and previous points\n",
    "                    old_frame = frame_gray.copy()\n",
    "                    p0 = good_new.reshape(-1, 1, 2)\n",
    "        \n",
    "        # Visualize results\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Show original frames\n",
    "        for i in range(3):\n",
    "            axes[0, i].imshow(frames[i * 3], cmap='gray')\n",
    "            axes[0, i].set_title(f'Frame {i * 3 + 1}')\n",
    "            axes[0, i].axis('off')\n",
    "        \n",
    "        # Show optical flow results\n",
    "        for i in range(3):\n",
    "            if i < len(results):\n",
    "                axes[1, i].imshow(cv2.cvtColor(results[i * 2], cv2.COLOR_BGR2RGB))\n",
    "                axes[1, i].set_title(f'Optical Flow - Frame {i * 2 + 2}')\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Lucas-Kanade optical flow completed!\")\n",
    "        print(\"üí° Tracks feature points across frames\")\n",
    "        print(\"üí° Shows motion vectors as colored trails\")\n",
    "    \n",
    "    def dense_optical_flow(self):\n",
    "        \"\"\"Demonstrate dense optical flow using Farneback method\"\"\"\n",
    "        print(\"üöÄ ADVANCED: Dense Optical Flow\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # Create synthetic sequence\n",
    "        frames = self.create_moving_objects_sequence()\n",
    "        \n",
    "        # Calculate dense optical flow between consecutive frames\n",
    "        flow_results = []\n",
    "        \n",
    "        for i in range(len(frames) - 1):\n",
    "            flow = cv2.calcOpticalFlowPyrLK(frames[i], frames[i + 1], None, None)\n",
    "            # Using Farneback instead\n",
    "            flow = cv2.calcOpticalFlowFarneback(\n",
    "                frames[i], frames[i + 1], None, \n",
    "                0.5, 3, 15, 3, 5, 1.2, 0\n",
    "            )\n",
    "            \n",
    "            # Convert flow to HSV for visualization\n",
    "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "            hsv = np.zeros((flow.shape[0], flow.shape[1], 3), dtype=np.uint8)\n",
    "            hsv[..., 1] = 255\n",
    "            hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "            hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "            \n",
    "            flow_results.append(rgb)\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Show original frames\n",
    "        for i in range(3):\n",
    "            idx = i * 2\n",
    "            if idx < len(frames):\n",
    "                axes[0, i].imshow(frames[idx], cmap='gray')\n",
    "                axes[0, i].set_title(f'Frame {idx + 1}')\n",
    "                axes[0, i].axis('off')\n",
    "        \n",
    "        # Show optical flow\n",
    "        for i in range(3):\n",
    "            idx = i * 2\n",
    "            if idx < len(flow_results):\n",
    "                axes[1, i].imshow(cv2.cvtColor(flow_results[idx], cv2.COLOR_BGR2RGB))\n",
    "                axes[1, i].set_title(f'Dense Flow {idx + 1}‚Üí{idx + 2}')\n",
    "                axes[1, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Dense optical flow completed!\")\n",
    "        print(\"üí° Color represents motion direction (hue) and speed (saturation)\")\n",
    "        print(\"üí° Every pixel has a motion vector\")\n",
    "        print(\"üí° Useful for understanding global scene motion\")\n",
    "\n",
    "# Create instance and demonstrate\n",
    "optical_flow_demo = OpticalFlowExamples()\n",
    "print(\"üåä Optical Flow class initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d93dbf",
   "metadata": {},
   "source": [
    "# üì∑ Image Formation - From 3D World to 2D Images\n",
    "\n",
    "**Image Formation** is the fundamental process of how we capture 3D world information into 2D digital images. Understanding this process is crucial for computer vision applications.\n",
    "\n",
    "## üîç Key Components\n",
    "\n",
    "### 1. **Perspective Projection**\n",
    "- **Definition**: Mathematical transformation that projects 3D scenes onto 2D planes\n",
    "- **Key Principle**: Objects farther away appear smaller\n",
    "- **Mathematical Model**: Uses homogeneous coordinates and projection matrices\n",
    "- **Applications**: Camera calibration, 3D reconstruction, augmented reality\n",
    "\n",
    "### 2. **Camera Models**\n",
    "- **Pinhole Camera**: Simplest model with a single point of light entry\n",
    "- **Lens Systems**: Real cameras use lenses to focus light\n",
    "- **Distortion Effects**: Barrel and pincushion distortion from lens imperfections\n",
    "- **Camera Parameters**: Intrinsic (focal length, principal point) and extrinsic (rotation, translation)\n",
    "\n",
    "### 3. **Light and Shading**\n",
    "- **Illumination Models**: How light sources affect object appearance\n",
    "- **Reflectance Properties**: Diffuse (Lambertian) and specular reflection\n",
    "- **Shadows and Highlights**: Created by light direction and surface orientation\n",
    "- **Color Temperature**: How light color affects perceived object colors\n",
    "\n",
    "### 4. **Color and Perception**\n",
    "- **Color Spaces**: RGB, HSV, LAB, YUV for different applications\n",
    "- **Human Visual System**: How we perceive colors and brightness\n",
    "- **Color Constancy**: Objects appear same color under different lighting\n",
    "- **Gamma Correction**: Compensating for display non-linearities\n",
    "\n",
    "## üöÄ Practical Applications\n",
    "- **Camera Calibration**: Determining precise camera parameters\n",
    "- **3D Reconstruction**: Recovering 3D structure from 2D images  \n",
    "- **Photometric Stereo**: Using lighting variations to estimate surface normals\n",
    "- **Color Correction**: Adjusting images for consistent appearance\n",
    "- **Virtual Reality**: Creating realistic synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a66a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFormationExamples:\n",
    "    \"\"\"\n",
    "    Image Formation: Understanding how 3D world becomes 2D images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Image Formation\"\n",
    "    \n",
    "    def perspective_projection_demo(self):\n",
    "        \"\"\"Demonstrate perspective projection effects\"\"\"\n",
    "        print(\"üìê EXAMPLE: Perspective Projection\")\n",
    "        print(\"-\" * 38)\n",
    "        \n",
    "        # Create 3D points representing a cube\n",
    "        cube_points = np.array([\n",
    "            [-1, -1, -1], [1, -1, -1], [1, 1, -1], [-1, 1, -1],  # Back face\n",
    "            [-1, -1, 1], [1, -1, 1], [1, 1, 1], [-1, 1, 1]       # Front face\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Camera parameters\n",
    "        focal_length = 500\n",
    "        principal_point = (250, 250)\n",
    "        \n",
    "        # Different distances to show perspective effect\n",
    "        distances = [3, 5, 8, 12]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, distance in enumerate(distances):\n",
    "            # Move cube to different distances\n",
    "            projected_points = []\n",
    "            \n",
    "            for point in cube_points:\n",
    "                # Translate cube away from camera\n",
    "                point_3d = point + [0, 0, distance]\n",
    "                \n",
    "                # Perspective projection: (X, Y, Z) -> (fX/Z, fY/Z)\n",
    "                if point_3d[2] > 0:  # Avoid division by zero\n",
    "                    x_proj = focal_length * point_3d[0] / point_3d[2] + principal_point[0]\n",
    "                    y_proj = focal_length * point_3d[1] / point_3d[2] + principal_point[1]\n",
    "                    projected_points.append([x_proj, y_proj])\n",
    "                else:\n",
    "                    projected_points.append([principal_point[0], principal_point[1]])\n",
    "            \n",
    "            projected_points = np.array(projected_points)\n",
    "            \n",
    "            # Draw the projected cube\n",
    "            axes[i].set_xlim(0, 500)\n",
    "            axes[i].set_ylim(0, 500)\n",
    "            axes[i].set_aspect('equal')\n",
    "            \n",
    "            # Draw cube edges\n",
    "            edges = [\n",
    "                [0, 1], [1, 2], [2, 3], [3, 0],  # Back face\n",
    "                [4, 5], [5, 6], [6, 7], [7, 4],  # Front face\n",
    "                [0, 4], [1, 5], [2, 6], [3, 7]   # Connecting edges\n",
    "            ]\n",
    "            \n",
    "            for edge in edges:\n",
    "                p1, p2 = projected_points[edge[0]], projected_points[edge[1]]\n",
    "                axes[i].plot([p1[0], p2[0]], [p1[1], p2[1]], 'b-', linewidth=2)\n",
    "            \n",
    "            # Draw projected points\n",
    "            axes[i].scatter(projected_points[:, 0], projected_points[:, 1], \n",
    "                          c='red', s=50, zorder=5)\n",
    "            \n",
    "            axes[i].set_title(f'Distance: {distance} units')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].invert_yaxis()  # Image coordinates\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Perspective projection completed!\")\n",
    "        print(\"üí° Objects appear smaller when farther away\")\n",
    "        print(\"üí° Parallel lines converge to vanishing points\")\n",
    "    \n",
    "    def camera_calibration_demo(self):\n",
    "        \"\"\"Demonstrate camera calibration concepts\"\"\"\n",
    "        print(\"üì∑ EXAMPLE: Camera Calibration\")\n",
    "        print(\"-\" * 33)\n",
    "        \n",
    "        # Create a checkerboard pattern\n",
    "        pattern_size = (8, 6)\n",
    "        square_size = 30  # mm\n",
    "        \n",
    "        # Generate 3D object points\n",
    "        objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)\n",
    "        objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)\n",
    "        objp *= square_size\n",
    "        \n",
    "        # Simulate camera parameters\n",
    "        camera_matrix = np.array([\n",
    "            [800, 0, 320],\n",
    "            [0, 800, 240],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        dist_coeffs = np.array([0.1, -0.2, 0, 0, 0], dtype=np.float32)\n",
    "        \n",
    "        # Different camera poses\n",
    "        poses = [\n",
    "            ([0, 0, 0], [0, 0, 500]),      # Frontal view\n",
    "            ([15, 0, 0], [0, 0, 500]),     # Slightly tilted\n",
    "            ([0, 20, 0], [50, 0, 500]),    # Rotated and translated\n",
    "            ([10, 15, 5], [-30, 30, 450]) # Complex pose\n",
    "        ]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (rotation, translation) in enumerate(poses):\n",
    "            # Convert rotation to rotation vector\n",
    "            rvec = np.array(rotation, dtype=np.float32) * np.pi / 180\n",
    "            tvec = np.array(translation, dtype=np.float32)\n",
    "            \n",
    "            # Project 3D points to image plane\n",
    "            image_points, _ = cv2.projectPoints(objp, rvec, tvec, camera_matrix, dist_coeffs)\n",
    "            image_points = image_points.reshape(-1, 2)\n",
    "            \n",
    "            # Create synthetic image\n",
    "            img = np.ones((480, 640, 3), dtype=np.uint8) * 240\n",
    "            \n",
    "            # Draw checkerboard pattern\n",
    "            for row in range(pattern_size[1]):\n",
    "                for col in range(pattern_size[0]):\n",
    "                    idx = row * pattern_size[0] + col\n",
    "                    if (row + col) % 2 == 0:  # Checkerboard pattern\n",
    "                        color = (0, 0, 0)  # Black squares\n",
    "                    else:\n",
    "                        color = (255, 255, 255)  # White squares\n",
    "                    \n",
    "                    # Get corners of this square\n",
    "                    if idx < len(image_points) - pattern_size[0] - 1:\n",
    "                        corners = [\n",
    "                            image_points[idx],\n",
    "                            image_points[idx + 1] if col < pattern_size[0] - 1 else image_points[idx],\n",
    "                            image_points[idx + pattern_size[0] + 1] if (col < pattern_size[0] - 1 and row < pattern_size[1] - 1) else image_points[idx],\n",
    "                            image_points[idx + pattern_size[0]] if row < pattern_size[1] - 1 else image_points[idx]\n",
    "                        ]\n",
    "                        \n",
    "                        # Draw filled polygon\n",
    "                        pts = np.array(corners, dtype=np.int32)\n",
    "                        cv2.fillPoly(img, [pts], color)\n",
    "            \n",
    "            # Draw corner points\n",
    "            for point in image_points:\n",
    "                if 0 <= point[0] < 640 and 0 <= point[1] < 480:\n",
    "                    cv2.circle(img, tuple(point.astype(int)), 3, (0, 255, 0), -1)\n",
    "            \n",
    "            axes[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            axes[i].set_title(f'Pose {i+1}: R={rotation}¬∞, T={translation}mm')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Camera calibration demo completed!\")\n",
    "        print(\"üí° Different camera poses create different image projections\")\n",
    "        print(\"üí° Calibration finds intrinsic and extrinsic parameters\")\n",
    "    \n",
    "    def color_spaces_demo(self):\n",
    "        \"\"\"Demonstrate different color spaces\"\"\"\n",
    "        print(\"üé® EXAMPLE: Color Spaces\")\n",
    "        print(\"-\" * 26)\n",
    "        \n",
    "        # Create a colorful test image\n",
    "        test_img = np.zeros((200, 200, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Create rainbow gradient\n",
    "        for i in range(200):\n",
    "            hue = int(i * 180 / 200)  # Hue from 0 to 180\n",
    "            hsv_color = np.array([[[hue, 255, 255]]], dtype=np.uint8)\n",
    "            rgb_color = cv2.cvtColor(hsv_color, cv2.COLOR_HSV2BGR)[0, 0]\n",
    "            test_img[:, i] = rgb_color\n",
    "        \n",
    "        # Add some geometric shapes with different colors\n",
    "        cv2.circle(test_img, (50, 50), 30, (255, 0, 0), -1)    # Blue circle\n",
    "        cv2.rectangle(test_img, (120, 30), (180, 90), (0, 255, 0), -1)  # Green rectangle\n",
    "        cv2.ellipse(test_img, (100, 150), (40, 25), 45, 0, 360, (0, 0, 255), -1)  # Red ellipse\n",
    "        \n",
    "        # Convert to different color spaces\n",
    "        rgb_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "        hsv_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2HSV)\n",
    "        lab_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2LAB)\n",
    "        yuv_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2YUV)\n",
    "        gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Visualize different color spaces\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # RGB\n",
    "        axes[0, 0].imshow(rgb_img)\n",
    "        axes[0, 0].set_title('RGB Color Space\\n(Red, Green, Blue)')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # HSV channels\n",
    "        axes[0, 1].imshow(hsv_img)\n",
    "        axes[0, 1].set_title('HSV Color Space\\n(Hue, Saturation, Value)')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # LAB\n",
    "        axes[0, 2].imshow(lab_img)\n",
    "        axes[0, 2].set_title('LAB Color Space\\n(Lightness, A, B)')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        # Individual HSV channels\n",
    "        axes[1, 0].imshow(hsv_img[:, :, 0], cmap='hsv')\n",
    "        axes[1, 0].set_title('HSV - Hue Channel')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(hsv_img[:, :, 1], cmap='gray')\n",
    "        axes[1, 1].set_title('HSV - Saturation Channel')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        axes[1, 2].imshow(gray_img, cmap='gray')\n",
    "        axes[1, 2].set_title('Grayscale')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Color spaces demonstration completed!\")\n",
    "        print(\"üí° RGB: Device-dependent, intuitive for displays\")\n",
    "        print(\"üí° HSV: Intuitive for humans, good for color-based segmentation\")\n",
    "        print(\"üí° LAB: Perceptually uniform, device-independent\")\n",
    "        print(\"üí° Different spaces are optimal for different tasks\")\n",
    "\n",
    "# Create instance and demonstrate\n",
    "image_formation_demo = ImageFormationExamples()\n",
    "print(\"üì∑ Image Formation class initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e718199",
   "metadata": {},
   "source": [
    "# üåê Enhanced 3D Computer Vision - Modern Techniques\n",
    "\n",
    "**3D Computer Vision** has evolved rapidly with neural approaches. Here we explore cutting-edge techniques for 3D scene understanding and rendering.\n",
    "\n",
    "## üß† Neural Radiance Fields (NeRFs)\n",
    "\n",
    "### **What are NeRFs?**\n",
    "- **Definition**: Neural networks that model 3D scenes as continuous functions\n",
    "- **Input**: 3D position (x, y, z) + viewing direction (Œ∏, œÜ)\n",
    "- **Output**: Color (RGB) + density (Œ±) at that point\n",
    "- **Revolutionary Idea**: No explicit 3D geometry - learned implicit representation\n",
    "\n",
    "### **How NeRFs Work**\n",
    "1. **Volume Rendering**: Ray marching through 3D space\n",
    "2. **Neural Function**: MLP maps (position, direction) ‚Üí (color, density)\n",
    "3. **Differentiable Rendering**: End-to-end training from 2D images\n",
    "4. **View Synthesis**: Generate novel views of captured scenes\n",
    "\n",
    "### **Key Advantages**\n",
    "- **Photo-realistic**: Extremely high-quality novel view synthesis\n",
    "- **Continuous**: Infinite resolution representation\n",
    "- **Compact**: Single network represents entire scene\n",
    "- **Flexible**: Works with any camera poses and scene types\n",
    "\n",
    "### **Applications**\n",
    "- **Virtual Reality**: Immersive scene exploration\n",
    "- **Film/Gaming**: Digital set extensions and environments\n",
    "- **Robotics**: Navigation and scene understanding\n",
    "- **Cultural Heritage**: 3D digitization of historical sites\n",
    "\n",
    "## ‚ö° Gaussian Splatting\n",
    "\n",
    "### **What is Gaussian Splatting?**\n",
    "- **Definition**: 3D scene representation using millions of 3D Gaussians\n",
    "- **Key Innovation**: Real-time rendering with neural optimization\n",
    "- **Representation**: Each Gaussian has position, covariance, color, opacity\n",
    "- **Speed**: 100x faster than NeRFs for similar quality\n",
    "\n",
    "### **Core Components**\n",
    "1. **3D Gaussians**: Ellipsoid primitives in 3D space\n",
    "2. **Differentiable Rasterization**: Fast GPU rendering pipeline\n",
    "3. **Adaptive Densification**: Automatically add/remove Gaussians\n",
    "4. **SfM Initialization**: Start from Structure-from-Motion points\n",
    "\n",
    "### **Advantages over NeRFs**\n",
    "- **Real-time Rendering**: 30+ FPS on consumer GPUs\n",
    "- **Faster Training**: Minutes vs hours for NeRFs\n",
    "- **Better Control**: Explicit 3D representation\n",
    "- **Quality**: Comparable or better visual results\n",
    "\n",
    "### **Recent Developments**\n",
    "- **Dynamic Gaussians**: Handling moving objects and deformation\n",
    "- **4D Gaussian Splatting**: Adding temporal dimension\n",
    "- **Compression**: Reducing memory footprint\n",
    "- **Editing**: Interactive scene modification\n",
    "\n",
    "## üî¨ Technical Comparison\n",
    "\n",
    "| Aspect | NeRFs | Gaussian Splatting |\n",
    "|--------|-------|-------------------|\n",
    "| **Rendering Speed** | Slow (seconds) | Fast (real-time) |\n",
    "| **Training Time** | Hours | Minutes |\n",
    "| **Memory Usage** | Low | Higher |\n",
    "| **Quality** | Excellent | Excellent |\n",
    "| **Editability** | Limited | Good |\n",
    "| **Scene Types** | All | Best for static |\n",
    "\n",
    "## üöÄ Implementation Considerations\n",
    "\n",
    "### **For NeRFs:**\n",
    "- **Libraries**: Nerfstudio, tiny-cuda-nn, Instant-NGP\n",
    "- **Hardware**: CUDA GPU recommended\n",
    "- **Data**: Multi-view images with camera poses\n",
    "- **Training**: 1-8 hours depending on scene complexity\n",
    "\n",
    "### **For Gaussian Splatting:**\n",
    "- **Libraries**: Official implementation, Nerfstudio integration\n",
    "- **Hardware**: Modern GPU with CUDA compute capability\n",
    "- **Data**: Same as NeRFs - multi-view images\n",
    "- **Training**: 10-30 minutes for most scenes\n",
    "\n",
    "## üìà Future Directions\n",
    "- **Generalizable Models**: Single model for multiple scenes\n",
    "- **Few-shot Learning**: Reducing required input images\n",
    "- **Dynamic Scenes**: Better handling of motion and deformation\n",
    "- **Semantic Understanding**: Combining with language models\n",
    "- **Mobile Deployment**: Optimization for edge devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86e2f5",
   "metadata": {},
   "source": [
    "# üéØ Enhanced Segmentation - Advanced Techniques\n",
    "\n",
    "**Image Segmentation** continues to evolve with both classical and modern approaches. Here we explore advanced segmentation methods.\n",
    "\n",
    "## üêç Active Contours (Snakes)\n",
    "\n",
    "### **What are Active Contours?**\n",
    "- **Definition**: Deformable curves that evolve to match object boundaries\n",
    "- **Physics-based**: Behave like elastic bands under forces\n",
    "- **Energy Minimization**: Minimize internal and external energy terms\n",
    "- **Interactive**: User provides initial contour, algorithm refines it\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "The snake energy function combines:\n",
    "1. **Internal Energy**: Keeps contour smooth and continuous\n",
    "   - Elasticity: Prevents stretching\n",
    "   - Rigidity: Prevents bending\n",
    "2. **External Energy**: Attracts contour to image features\n",
    "   - Image gradients: Edge attraction\n",
    "   - Region properties: Intensity homogeneity\n",
    "\n",
    "### **Types of Active Contours**\n",
    "\n",
    "#### **Parametric Snakes (Classic)**\n",
    "- **Representation**: Explicit curve parameterization v(s) = [x(s), y(s)]\n",
    "- **Evolution**: Gradient descent on energy functional\n",
    "- **Advantages**: Direct control, well-understood mathematics\n",
    "- **Limitations**: Topology cannot change, sensitive to initialization\n",
    "\n",
    "#### **Geometric Active Contours (Level Sets)**\n",
    "- **Representation**: Implicit surface œÜ(x,y,t) where zero level = contour\n",
    "- **Evolution**: Partial differential equations\n",
    "- **Advantages**: Topology changes handled naturally\n",
    "- **Applications**: Medical imaging, object tracking\n",
    "\n",
    "### **GrabCut Algorithm**\n",
    "- **Interactive Segmentation**: User marks foreground/background regions\n",
    "- **Graph Cuts**: Optimization using min-cut/max-flow\n",
    "- **Gaussian Mixture Models**: Model color distributions\n",
    "- **Iterative Refinement**: Alternates between model learning and segmentation\n",
    "\n",
    "## üîß Implementation Approaches\n",
    "\n",
    "### **Classical Active Contours**\n",
    "```python\n",
    "# Pseudo-code for snake evolution\n",
    "def evolve_snake(contour, image, alpha, beta, gamma):\n",
    "    for iteration in range(max_iterations):\n",
    "        # Compute internal forces\n",
    "        internal_force = alpha * elasticity_force + beta * rigidity_force\n",
    "        \n",
    "        # Compute external forces\n",
    "        external_force = gamma * gradient_force\n",
    "        \n",
    "        # Update contour\n",
    "        contour += dt * (internal_force + external_force)\n",
    "        \n",
    "        # Apply constraints\n",
    "        contour = apply_constraints(contour)\n",
    "    \n",
    "    return contour\n",
    "```\n",
    "\n",
    "### **Modern Deep Learning Approaches**\n",
    "- **U-Net Architecture**: Encoder-decoder with skip connections\n",
    "- **Mask R-CNN**: Instance segmentation combining detection and segmentation\n",
    "- **DeepLab**: Atrous convolution for multi-scale features\n",
    "- **Segment Anything Model (SAM)**: Foundation model for any object\n",
    "\n",
    "## üéØ Practical Applications\n",
    "\n",
    "### **Medical Imaging**\n",
    "- **Organ Segmentation**: Heart, liver, brain structures\n",
    "- **Tumor Detection**: Cancer diagnosis and treatment planning\n",
    "- **Surgical Planning**: Pre-operative visualization\n",
    "- **Disease Progression**: Monitoring changes over time\n",
    "\n",
    "### **Industrial Inspection**\n",
    "- **Defect Detection**: Manufacturing quality control\n",
    "- **Material Classification**: Sorting and grading\n",
    "- **Dimensional Measurement**: Precision part inspection\n",
    "- **Surface Analysis**: Texture and finish evaluation\n",
    "\n",
    "### **Autonomous Systems**\n",
    "- **Road Segmentation**: Drivable surface detection\n",
    "- **Obstacle Identification**: Safety-critical object detection\n",
    "- **Scene Understanding**: Environmental perception\n",
    "- **Path Planning**: Navigation decision making\n",
    "\n",
    "## üîç Evaluation Metrics\n",
    "\n",
    "### **Pixel-level Metrics**\n",
    "- **Intersection over Union (IoU)**: Overlap between prediction and ground truth\n",
    "- **Dice Coefficient**: 2 * |A ‚à© B| / (|A| + |B|)\n",
    "- **Pixel Accuracy**: Correctly classified pixels / Total pixels\n",
    "- **Mean Average Precision (mAP)**: For multi-class segmentation\n",
    "\n",
    "### **Boundary-level Metrics**\n",
    "- **Hausdorff Distance**: Maximum distance between boundary points\n",
    "- **Average Surface Distance**: Mean distance between boundaries\n",
    "- **Boundary F1-Score**: Precision and recall for boundary pixels\n",
    "- **Contour Accuracy**: Shape similarity measures\n",
    "\n",
    "## üöÄ Recent Advances\n",
    "\n",
    "### **Transformer-based Segmentation**\n",
    "- **SETR**: Segmentation Transformer using Vision Transformer backbone\n",
    "- **SegFormer**: Hierarchical transformer with lightweight decoder\n",
    "- **Mask2Former**: Universal segmentation architecture\n",
    "\n",
    "### **Few-shot and Zero-shot**\n",
    "- **Meta-learning**: Learning to segment new classes with few examples\n",
    "- **CLIP-based**: Using vision-language models for segmentation\n",
    "- **Prompt-based**: Text or visual prompts for segmentation guidance\n",
    "\n",
    "### **Real-time Segmentation**\n",
    "- **Mobile-friendly**: Optimized for edge devices\n",
    "- **Video Segmentation**: Temporal consistency across frames\n",
    "- **Interactive Refinement**: User feedback integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa3b37",
   "metadata": {},
   "source": [
    "# üöÄ Recent Topics in Computer Vision - Cutting Edge Research\n",
    "\n",
    "**Computer Vision** is rapidly evolving with breakthrough technologies. Here we explore the latest developments shaping the field.\n",
    "\n",
    "## üèóÔ∏è Foundation Models\n",
    "\n",
    "### **What are Foundation Models?**\n",
    "- **Definition**: Large-scale models trained on massive datasets that can be adapted to many tasks\n",
    "- **Paradigm Shift**: From task-specific models to general-purpose foundations\n",
    "- **Transfer Learning**: Pre-trained representations for downstream tasks\n",
    "- **Scale**: Billions of parameters, trained on millions/billions of images\n",
    "\n",
    "### **Key Foundation Models**\n",
    "\n",
    "#### **CLIP (Contrastive Language-Image Pre-training)**\n",
    "- **Multimodal**: Understands both images and text\n",
    "- **Zero-shot**: Can classify images without training examples\n",
    "- **Applications**: Image search, captioning, classification\n",
    "- **Training**: 400M image-text pairs from internet\n",
    "\n",
    "#### **DALL-E / Stable Diffusion**\n",
    "- **Text-to-Image**: Generate images from textual descriptions\n",
    "- **Creative AI**: Artistic and photorealistic image generation\n",
    "- **Controllable**: Fine-grained control over generated content\n",
    "- **Applications**: Art, design, content creation, prototyping\n",
    "\n",
    "#### **Segment Anything Model (SAM)**\n",
    "- **Universal Segmentation**: Segment any object in any image\n",
    "- **Prompt-based**: Point, box, or mask prompts\n",
    "- **Zero-shot**: Works on unseen objects and domains\n",
    "- **Training**: 11M images, 1B+ masks\n",
    "\n",
    "## üîó Vision-Language Models\n",
    "\n",
    "### **Multimodal Understanding**\n",
    "- **Visual Question Answering**: Answer questions about images\n",
    "- **Image Captioning**: Generate descriptive text for images\n",
    "- **Visual Reasoning**: Logic and inference from visual content\n",
    "- **Grounded Language**: Connect text descriptions to image regions\n",
    "\n",
    "### **Recent Architectures**\n",
    "- **BLIP-2**: Bootstrapped vision-language pre-training\n",
    "- **LLaVA**: Large Language and Vision Assistant\n",
    "- **GPT-4V**: Multimodal capabilities in large language models\n",
    "- **Flamingo**: Few-shot learning for vision-language tasks\n",
    "\n",
    "### **Applications**\n",
    "- **Accessibility**: Describing images for visually impaired users\n",
    "- **Content Moderation**: Understanding context in images and text\n",
    "- **Education**: Interactive learning with visual content\n",
    "- **Robotics**: Natural language control of robotic systems\n",
    "\n",
    "## ‚ö° Real-time Computer Vision\n",
    "\n",
    "### **Efficient Architectures**\n",
    "- **MobileNets**: Depthwise separable convolutions for mobile devices\n",
    "- **EfficientNet**: Compound scaling of depth, width, and resolution\n",
    "- **Vision Transformers (ViTs)**: Attention-based models for vision\n",
    "- **Neural Architecture Search**: Automated design of efficient networks\n",
    "\n",
    "### **Edge Computing**\n",
    "- **Model Quantization**: Reducing precision (FP32 ‚Üí INT8)\n",
    "- **Pruning**: Removing unnecessary network connections\n",
    "- **Knowledge Distillation**: Teaching small models from large ones\n",
    "- **Hardware Acceleration**: Specialized chips (TPUs, NPUs)\n",
    "\n",
    "### **Applications**\n",
    "- **Autonomous Vehicles**: Real-time perception and decision making\n",
    "- **Augmented Reality**: Live overlay of digital content\n",
    "- **Surveillance**: Real-time monitoring and alert systems\n",
    "- **Industrial Automation**: Quality control and process monitoring\n",
    "\n",
    "## üß† Self-Supervised Learning\n",
    "\n",
    "### **Learning without Labels**\n",
    "- **Contrastive Learning**: Learning representations by comparing samples\n",
    "- **Masked Image Modeling**: Predicting masked patches (like BERT for images)\n",
    "- **Rotation Prediction**: Learning features by predicting image rotations\n",
    "- **Jigsaw Puzzles**: Solving spatial arrangement tasks\n",
    "\n",
    "### **Recent Methods**\n",
    "- **SimCLR**: Simple framework for contrastive learning\n",
    "- **MAE (Masked Autoencoders)**: Reconstructing masked image patches\n",
    "- **DINO**: Self-distillation with no labels\n",
    "- **SwAV**: Swapping assignments between views\n",
    "\n",
    "### **Advantages**\n",
    "- **Data Efficiency**: Leverage large amounts of unlabeled data\n",
    "- **Generalization**: Learn robust representations\n",
    "- **Cost Effective**: Reduce annotation requirements\n",
    "- **Scalability**: Work with internet-scale datasets\n",
    "\n",
    "## üîÑ Generative AI in Vision\n",
    "\n",
    "### **Diffusion Models**\n",
    "- **DDPM**: Denoising Diffusion Probabilistic Models\n",
    "- **Score-based**: Learning gradients of data distribution\n",
    "- **Controllable Generation**: Text, layout, or sketch conditioning\n",
    "- **Applications**: Art, design, data augmentation, editing\n",
    "\n",
    "### **Generative Adversarial Networks (GANs)**\n",
    "- **StyleGAN**: High-quality face and image generation\n",
    "- **Progressive Growing**: Gradually increasing resolution\n",
    "- **Latent Space Manipulation**: Editing generated images\n",
    "- **Applications**: Synthetic data, art, face editing\n",
    "\n",
    "### **Video Generation**\n",
    "- **Temporal Consistency**: Maintaining coherence across frames\n",
    "- **Motion Modeling**: Understanding and generating realistic movement\n",
    "- **Applications**: Film, gaming, simulation, training data\n",
    "\n",
    "## üìä Emerging Applications\n",
    "\n",
    "### **Medical AI**\n",
    "- **Diagnostic Assistance**: Detecting diseases from medical images\n",
    "- **Drug Discovery**: Molecular structure analysis and prediction\n",
    "- **Surgical Robotics**: Real-time guidance during operations\n",
    "- **Personalized Medicine**: Tailored treatments based on imaging\n",
    "\n",
    "### **Climate and Environment**\n",
    "- **Satellite Monitoring**: Tracking deforestation, urban growth\n",
    "- **Wildlife Conservation**: Animal tracking and population monitoring\n",
    "- **Disaster Response**: Damage assessment from aerial imagery\n",
    "- **Agriculture**: Crop monitoring and yield prediction\n",
    "\n",
    "### **Scientific Discovery**\n",
    "- **Astronomy**: Galaxy classification, exoplanet detection\n",
    "- **Material Science**: Crystal structure prediction\n",
    "- **Biology**: Cell tracking, protein folding analysis\n",
    "- **Physics**: Particle detection, experimental analysis\n",
    "\n",
    "## üîÆ Future Directions\n",
    "\n",
    "### **Technical Challenges**\n",
    "- **Robustness**: Handling distribution shifts and adversarial attacks\n",
    "- **Interpretability**: Understanding model decisions and biases\n",
    "- **Efficiency**: Reducing computational requirements\n",
    "- **Privacy**: Federated learning and differential privacy\n",
    "\n",
    "### **Societal Impact**\n",
    "- **Ethical AI**: Addressing bias and fairness in vision systems\n",
    "- **Regulation**: Developing standards for AI safety and accountability\n",
    "- **Education**: Training next generation of CV researchers and practitioners\n",
    "- **Democratization**: Making advanced CV accessible to everyone\n",
    "\n",
    "### **Research Frontiers**\n",
    "- **Embodied AI**: Robots that understand and interact with the world\n",
    "- **Neural-Symbolic**: Combining neural networks with symbolic reasoning\n",
    "- **Continual Learning**: Models that learn continuously without forgetting\n",
    "- **Multimodal Foundation Models**: Understanding text, images, audio, and video together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad91976",
   "metadata": {},
   "source": [
    "# üéì Course Summary & Next Steps\n",
    "\n",
    "## üìö What We've Covered\n",
    "\n",
    "Congratulations! You've journeyed through the comprehensive world of Computer Vision. Let's recap what you've learned:\n",
    "\n",
    "### **üî¢ Foundation Topics**\n",
    "- **Sampling & Interpolation**: Converting continuous images to discrete grids and filling missing values\n",
    "- **Point Operations**: Pixel-wise transformations for brightness, contrast, and enhancement\n",
    "- **Filtering**: Convolution operations for smoothing, sharpening, and feature detection\n",
    "\n",
    "### **üéØ Core Computer Vision**\n",
    "- **Image Processing**: Fundamental operations for image manipulation and enhancement\n",
    "- **Fitting & Alignment**: Geometric transformations and feature matching\n",
    "- **Segmentation**: Partitioning images into meaningful regions and objects\n",
    "\n",
    "### **üß† Advanced Techniques**\n",
    "- **Deep Learning**: Neural networks for classification, detection, and feature learning\n",
    "- **3D Computer Vision**: Stereo vision, structure from motion, and modern neural rendering\n",
    "- **Generative AI**: Creating and synthesizing new visual content\n",
    "\n",
    "### **üöÄ Cutting-Edge Research**\n",
    "- **Neural Radiance Fields (NeRFs)**: Implicit 3D scene representations\n",
    "- **Gaussian Splatting**: Real-time 3D rendering with point-based primitives\n",
    "- **Foundation Models**: Large-scale pre-trained models for general vision tasks\n",
    "- **Vision-Language Models**: Multimodal understanding of images and text\n",
    "\n",
    "## üí° Key Learning Outcomes\n",
    "\n",
    "After completing this course, you should be able to:\n",
    "\n",
    "1. **üîß Implement Core Algorithms**\n",
    "   - Apply fundamental image processing operations\n",
    "   - Implement feature detection and matching algorithms\n",
    "   - Build segmentation and classification systems\n",
    "\n",
    "2. **üß† Understand Modern Approaches**\n",
    "   - Design and train deep learning models for vision tasks\n",
    "   - Work with pre-trained foundation models\n",
    "   - Apply transfer learning for specific applications\n",
    "\n",
    "3. **üéØ Solve Real-World Problems**\n",
    "   - Medical image analysis and diagnosis\n",
    "   - Autonomous vehicle perception systems\n",
    "   - Industrial quality control and inspection\n",
    "   - Augmented reality and entertainment applications\n",
    "\n",
    "4. **üìä Evaluate and Optimize**\n",
    "   - Choose appropriate metrics for different tasks\n",
    "   - Optimize models for real-time performance\n",
    "   - Address challenges like bias, robustness, and interpretability\n",
    "\n",
    "## üõ†Ô∏è Practical Skills Developed\n",
    "\n",
    "### **Programming Proficiency**\n",
    "- **OpenCV**: Industry-standard computer vision library\n",
    "- **PyTorch/TensorFlow**: Deep learning frameworks\n",
    "- **NumPy/SciPy**: Scientific computing foundations\n",
    "- **Matplotlib/Plotly**: Visualization and analysis tools\n",
    "\n",
    "### **Mathematical Understanding**\n",
    "- **Linear Algebra**: Transformations, projections, and optimization\n",
    "- **Signal Processing**: Filtering, sampling, and frequency analysis\n",
    "- **Statistics**: Probability distributions, estimation, and inference\n",
    "- **Optimization**: Gradient descent, loss functions, and regularization\n",
    "\n",
    "### **Engineering Practices**\n",
    "- **Data Pipeline**: Collection, preprocessing, and augmentation\n",
    "- **Model Development**: Design, training, validation, and testing\n",
    "- **Deployment**: Edge computing, optimization, and monitoring\n",
    "- **Ethics**: Bias detection, fairness, and responsible AI\n",
    "\n",
    "## üåü Industry Applications\n",
    "\n",
    "### **üè• Healthcare**\n",
    "- **Medical Imaging**: X-rays, MRIs, CT scans analysis\n",
    "- **Pathology**: Automated disease detection\n",
    "- **Surgery**: Real-time guidance and assistance\n",
    "- **Drug Discovery**: Molecular structure analysis\n",
    "\n",
    "### **üöó Autonomous Systems**\n",
    "- **Self-Driving Cars**: Perception and decision making\n",
    "- **Drones**: Navigation and surveillance\n",
    "- **Robotics**: Manipulation and interaction\n",
    "- **Smart Cities**: Traffic monitoring and optimization\n",
    "\n",
    "### **üè≠ Industry 4.0**\n",
    "- **Quality Control**: Automated inspection systems\n",
    "- **Predictive Maintenance**: Equipment monitoring\n",
    "- **Supply Chain**: Inventory and logistics optimization\n",
    "- **Safety**: Hazard detection and prevention\n",
    "\n",
    "### **üéÆ Entertainment & Media**\n",
    "- **Gaming**: Real-time rendering and interaction\n",
    "- **Film**: Visual effects and post-production\n",
    "- **Social Media**: Content understanding and moderation\n",
    "- **AR/VR**: Immersive experiences and metaverse\n",
    "\n",
    "## üöÄ Next Steps in Your Journey\n",
    "\n",
    "### **üìñ Continued Learning**\n",
    "1. **Specialized Courses**\n",
    "   - Medical Image Analysis\n",
    "   - Autonomous Vehicle Perception\n",
    "   - 3D Computer Vision and Graphics\n",
    "   - Natural Language Processing (for multimodal AI)\n",
    "\n",
    "2. **Research Areas**\n",
    "   - Neural Architecture Search\n",
    "   - Federated Learning for Vision\n",
    "   - Explainable AI in Computer Vision\n",
    "   - Quantum Machine Learning\n",
    "\n",
    "3. **Practical Projects**\n",
    "   - Build an end-to-end vision application\n",
    "   - Contribute to open-source computer vision projects\n",
    "   - Participate in Kaggle competitions\n",
    "   - Develop mobile or web-based vision apps\n",
    "\n",
    "### **üéØ Career Paths**\n",
    "- **Computer Vision Engineer**: Developing vision systems for products\n",
    "- **Research Scientist**: Advancing the state-of-the-art in academia or industry\n",
    "- **Machine Learning Engineer**: Building scalable ML/CV systems\n",
    "- **Product Manager**: Leading vision-powered product development\n",
    "- **Consultant**: Helping organizations adopt computer vision technologies\n",
    "\n",
    "### **üåê Community & Resources**\n",
    "- **Conferences**: CVPR, ICCV, ECCV, NeurIPS\n",
    "- **Journals**: TPAMI, IJCV, Computer Vision and Image Understanding\n",
    "- **Online Communities**: Reddit r/MachineLearning, Papers with Code\n",
    "- **Open Source**: Contribute to OpenCV, PyTorch, TensorFlow projects\n",
    "\n",
    "## üéâ Final Words\n",
    "\n",
    "Computer Vision is a rapidly evolving field at the intersection of mathematics, computer science, and artificial intelligence. The techniques you've learned here form the foundation for:\n",
    "\n",
    "- **üî¨ Scientific Discovery**: From astronomy to biology\n",
    "- **üåç Social Impact**: Healthcare, accessibility, environmental monitoring\n",
    "- **üíº Economic Value**: Automation, efficiency, new product categories\n",
    "- **üé® Creative Expression**: Art, design, and entertainment\n",
    "\n",
    "Remember that the field continues to evolve rapidly. Stay curious, keep learning, and don't hesitate to experiment with new ideas. The future of computer vision is being written by researchers and practitioners like you!\n",
    "\n",
    "### **üöÄ \"The best way to predict the future is to create it.\"**\n",
    "\n",
    "Thank you for joining this comprehensive journey through Computer Vision. Now go forth and build amazing things! üåü\n",
    "\n",
    "---\n",
    "\n",
    "*For questions, updates, or contributions to this course material, please reach out to the course instructors or contribute to the course repository.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de701b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessingExamples:\n",
    "    \"\"\"\n",
    "    Image Processing: Manipulate digital images to produce enhanced versions\n",
    "    Key concepts: Filtering, Edge Detection, Feature Extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Image Processing and Early Vision\"\n",
    "        \n",
    "    def load_sample_image(self):\n",
    "        \"\"\"Load a sample image for demonstration\"\"\"\n",
    "        # Create a synthetic image if no real image available\n",
    "        img = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Add some geometric shapes\n",
    "        cv2.rectangle(img, (50, 50), (150, 150), (255, 0, 0), -1)  # Blue rectangle\n",
    "        cv2.circle(img, (200, 200), 50, (0, 255, 0), -1)  # Green circle\n",
    "        cv2.line(img, (0, 0), (300, 300), (0, 0, 255), 5)  # Red line\n",
    "        \n",
    "        # Add some noise\n",
    "        noise = np.random.randint(0, 50, img.shape, dtype=np.uint8)\n",
    "        img = cv2.add(img, noise)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def basic_filtering_example(self):\n",
    "        \"\"\"Basic Example: Apply Gaussian blur and median filter\"\"\"\n",
    "        print(\"üîç BASIC EXAMPLE: Image Filtering\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        img = self.load_sample_image()\n",
    "        \n",
    "        # Apply Gaussian blur (removes high-frequency noise)\n",
    "        gaussian_blur = cv2.GaussianBlur(img, (15, 15), 0)\n",
    "        \n",
    "        # Apply median filter (removes salt-and-pepper noise)\n",
    "        median_filter = cv2.medianBlur(img, 5)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(cv2.cvtColor(gaussian_blur, cv2.COLOR_BGR2RGB))\n",
    "        axes[1].set_title('Gaussian Blur (œÉ=15)')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(cv2.cvtColor(median_filter, cv2.COLOR_BGR2RGB))\n",
    "        axes[2].set_title('Median Filter (kernel=5)')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Basic filtering applied successfully!\")\n",
    "        print(\"üí° Gaussian blur: Reduces noise by averaging neighboring pixels\")\n",
    "        print(\"üí° Median filter: Removes impulse noise while preserving edges\")\n",
    "\n",
    "# Create instance for image processing examples\n",
    "img_processor = ImageProcessingExamples()\n",
    "print(\"üì∑ Image Processing class initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5815142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì BEGINNER'S GUIDE: Understanding the Code Below\n",
    "\"\"\"\n",
    "This section shows you HOW to implement image processing techniques.\n",
    "Don't worry if it looks complex - we'll break it down step by step!\n",
    "\n",
    "KEY PROGRAMMING CONCEPTS YOU'LL LEARN:\n",
    "1. Object-Oriented Programming (classes and methods)\n",
    "2. NumPy for numerical operations\n",
    "3. OpenCV for computer vision\n",
    "4. Matplotlib for visualization\n",
    "\n",
    "READING TIP: Focus on the comments (lines starting with #) \n",
    "to understand what each part does!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ee55a",
   "metadata": {},
   "source": [
    "# üéì Complete Beginner's Learning Guide\n",
    "\n",
    "## üìñ How to Use This Notebook Effectively:\n",
    "\n",
    "### **Step 1: Read Theory First** üìö\n",
    "- Each section starts with detailed explanations\n",
    "- Understand the \"why\" before the \"how\"\n",
    "- Don't rush - take time to absorb concepts\n",
    "\n",
    "### **Step 2: Run Code Cells** ‚ñ∂Ô∏è\n",
    "- Click on each code cell and press `Shift + Enter` to run it\n",
    "- Watch the output carefully\n",
    "- Try to predict what will happen before running\n",
    "\n",
    "### **Step 3: Experiment** üî¨\n",
    "- Modify parameters and see what changes\n",
    "- Break things and fix them - it's the best way to learn!\n",
    "- Ask \"What if I change this number?\"\n",
    "\n",
    "### **Step 4: Visualize Results** üëÅÔ∏è\n",
    "- Look at every plot and image carefully\n",
    "- Compare before/after results\n",
    "- Try to explain why you see certain patterns\n",
    "\n",
    "## üõ†Ô∏è Essential Tools You'll Master:\n",
    "\n",
    "### **NumPy** - The Math Foundation\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Creating arrays (like lists but for math)\n",
    "array = np.array([1, 2, 3, 4])\n",
    "\n",
    "# Mathematical operations\n",
    "result = array * 2  # Multiply all elements by 2\n",
    "\n",
    "# Why it matters: Images are just arrays of numbers!\n",
    "```\n",
    "\n",
    "### **OpenCV** - Computer Vision Powerhouse\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "# Reading images\n",
    "img = cv2.imread('image.jpg')\n",
    "\n",
    "# Basic operations\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "blurred = cv2.GaussianBlur(img, (15, 15), 0)  # Apply blur\n",
    "\n",
    "# Why it matters: OpenCV has tools for almost every vision task!\n",
    "```\n",
    "\n",
    "### **Matplotlib** - Visualization Magic\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Displaying images\n",
    "plt.imshow(image)\n",
    "plt.title('My Image')\n",
    "plt.show()\n",
    "\n",
    "# Creating plots\n",
    "plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\n",
    "plt.xlabel('X axis')\n",
    "plt.ylabel('Y axis')\n",
    "\n",
    "# Why it matters: Seeing results is crucial for understanding!\n",
    "```\n",
    "\n",
    "## üéØ Learning Objectives by Section:\n",
    "\n",
    "### **1Ô∏è‚É£ Image Processing**\n",
    "**By the end, you'll understand:**\n",
    "- How computers represent images as numbers\n",
    "- What filters do and why we need them\n",
    "- How edge detection reveals object boundaries\n",
    "- The difference between noise and important details\n",
    "\n",
    "**Key takeaway:** Images are just arrays of numbers that we can manipulate mathematically!\n",
    "\n",
    "### **2Ô∏è‚É£ Fitting and Alignment**\n",
    "**By the end, you'll understand:**\n",
    "- Why real-world data is messy\n",
    "- How to find patterns despite noise\n",
    "- When to use different fitting methods\n",
    "- The importance of robust algorithms\n",
    "\n",
    "**Key takeaway:** Good algorithms work even when data is imperfect!\n",
    "\n",
    "### **3Ô∏è‚É£ Segmentation**\n",
    "**By the end, you'll understand:**\n",
    "- How to separate objects from backgrounds\n",
    "- Different strategies for different image types\n",
    "- The trade-offs between simple and complex methods\n",
    "- Why preprocessing matters\n",
    "\n",
    "**Key takeaway:** Breaking images into meaningful parts is often the first step in analysis!\n",
    "\n",
    "### **4Ô∏è‚É£ Deep Learning**\n",
    "**By the end, you'll understand:**\n",
    "- How neural networks \"learn\" to see\n",
    "- Why CNNs are perfect for images\n",
    "- The power of transfer learning\n",
    "- How to evaluate model performance\n",
    "\n",
    "**Key takeaway:** Modern AI can learn patterns humans never explicitly programmed!\n",
    "\n",
    "### **5Ô∏è‚É£ 3D Vision**\n",
    "**By the end, you'll understand:**\n",
    "- How depth perception works\n",
    "- Why two viewpoints are better than one\n",
    "- The mathematics behind 3D reconstruction\n",
    "- Applications in robotics and AR/VR\n",
    "\n",
    "**Key takeaway:** Understanding 3D space from 2D images is one of AI's greatest achievements!\n",
    "\n",
    "### **6Ô∏è‚É£ Generative AI**\n",
    "**By the end, you'll understand:**\n",
    "- How AI creates new content\n",
    "- Different approaches to generation\n",
    "- The creative potential of AI\n",
    "- Ethical considerations\n",
    "\n",
    "**Key takeaway:** AI is becoming a creative partner, not just an analytical tool!\n",
    "\n",
    "## üí° Study Tips for Success:\n",
    "\n",
    "### **Before You Start:**\n",
    "1. **Set up environment**: Make sure all libraries are installed\n",
    "2. **Find examples**: Look up additional images online to test with\n",
    "3. **Join communities**: Reddit, Discord, Stack Overflow for help\n",
    "\n",
    "### **While Learning:**\n",
    "1. **Take notes**: Write down key insights in your own words\n",
    "2. **Draw diagrams**: Sketch out concepts to solidify understanding\n",
    "3. **Practice regularly**: Set aside time each day for hands-on coding\n",
    "4. **Teach others**: Explain concepts to friends or in online forums\n",
    "\n",
    "### **After Each Section:**\n",
    "1. **Summarize**: What were the 3 most important things you learned?\n",
    "2. **Apply**: Try the techniques on your own images\n",
    "3. **Explore**: Look up real-world applications of each method\n",
    "4. **Connect**: How does this relate to other sections?\n",
    "\n",
    "## üöÄ Your Computer Vision Journey Starts Here!\n",
    "\n",
    "Remember: Everyone was a beginner once. The key is persistence, curiosity, and lots of practice. Don't get discouraged if something doesn't click immediately - computer vision combines mathematics, programming, and intuition, and it takes time to develop all three.\n",
    "\n",
    "**Ready to become a computer vision expert? Let's dive in!** üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the basic filtering example\n",
    "img_processor.basic_filtering_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add advanced filtering method to the class\n",
    "def advanced_filtering_example(self):\n",
    "    \"\"\"Advanced Example: Custom convolution kernels and edge detection\"\"\"\n",
    "    print(\"üöÄ ADVANCED EXAMPLE: Custom Convolution and Edge Detection\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    img = cv2.cvtColor(self.load_sample_image(), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Custom convolution kernels\n",
    "    # Sobel X kernel (detects vertical edges)\n",
    "    sobel_x = np.array([[-1, 0, 1],\n",
    "                       [-2, 0, 2],\n",
    "                       [-1, 0, 1]], dtype=np.float32)\n",
    "    \n",
    "    # Sobel Y kernel (detects horizontal edges)\n",
    "    sobel_y = np.array([[-1, -2, -1],\n",
    "                       [ 0,  0,  0],\n",
    "                       [ 1,  2,  1]], dtype=np.float32)\n",
    "    \n",
    "    # Laplacian kernel (detects all edges)\n",
    "    laplacian = np.array([[ 0, -1,  0],\n",
    "                         [-1,  4, -1],\n",
    "                         [ 0, -1,  0]], dtype=np.float32)\n",
    "    \n",
    "    # Apply convolutions\n",
    "    edges_x = cv2.filter2D(img, -1, sobel_x)\n",
    "    edges_y = cv2.filter2D(img, -1, sobel_y)\n",
    "    edges_laplacian = cv2.filter2D(img, -1, laplacian)\n",
    "    \n",
    "    # Combine Sobel X and Y for gradient magnitude\n",
    "    gradient_magnitude = np.sqrt(edges_x**2 + edges_y**2)\n",
    "    \n",
    "    # Canny edge detection (advanced edge detector)\n",
    "    canny_edges = cv2.Canny(img, 50, 150)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    axes[0, 0].imshow(img, cmap='gray')\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(edges_x, cmap='gray')\n",
    "    axes[0, 1].set_title('Sobel X (Vertical Edges)')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(edges_y, cmap='gray')\n",
    "    axes[0, 2].set_title('Sobel Y (Horizontal Edges)')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(gradient_magnitude, cmap='gray')\n",
    "    axes[1, 0].set_title('Gradient Magnitude')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(edges_laplacian, cmap='gray')\n",
    "    axes[1, 1].set_title('Laplacian Edges')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(canny_edges, cmap='gray')\n",
    "    axes[1, 2].set_title('Canny Edge Detection')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Advanced edge detection completed!\")\n",
    "    print(\"üí° Sobel operators: Detect edges in specific directions\")\n",
    "    print(\"üí° Laplacian: Second derivative operator for edge detection\")\n",
    "    print(\"üí° Canny: Multi-stage edge detection with non-maximum suppression\")\n",
    "\n",
    "# Add the method to the class\n",
    "ImageProcessingExamples.advanced_filtering_example = advanced_filtering_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the advanced filtering example\n",
    "img_processor.advanced_filtering_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c85fab",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Fitting and Alignment\n",
    "\n",
    "## ü§î What is Fitting and Alignment?\n",
    "\n",
    "Imagine you're trying to draw the best line through a bunch of scattered points on a graph. That's essentially what **fitting** does! In computer vision, we often need to:\n",
    "\n",
    "- **Find patterns** in messy, real-world data\n",
    "- **Align images** taken from different angles or times\n",
    "- **Remove outliers** (bad data points that don't belong)\n",
    "- **Estimate geometric transformations** between images\n",
    "\n",
    "## üåü Why is This Important?\n",
    "\n",
    "### Real-World Applications:\n",
    "1. **Panorama Photography**: Stitching multiple photos into one wide image\n",
    "2. **Medical Imaging**: Aligning CT scans taken at different times\n",
    "3. **Augmented Reality**: Placing virtual objects in real scenes\n",
    "4. **Satellite Mapping**: Combining images from different satellites\n",
    "5. **Motion Tracking**: Following objects across video frames\n",
    "\n",
    "## üìö Key Concepts Explained:\n",
    "\n",
    "### üìê **Least Squares Fitting**\n",
    "- **What it does**: Finds the \"best\" line/curve through data points\n",
    "- **How it works**: Minimizes the sum of squared distances from points to the line\n",
    "- **Pros**: Simple, fast, mathematically elegant\n",
    "- **Cons**: Sensitive to outliers (one bad point can ruin everything!)\n",
    "- **Real example**: Finding the trend line in stock prices\n",
    "\n",
    "### üõ°Ô∏è **RANSAC (RANdom SAmple Consensus)**\n",
    "- **What it does**: Robust fitting that ignores outliers\n",
    "- **How it works**: \n",
    "  1. Randomly pick a small subset of data\n",
    "  2. Fit a model to this subset\n",
    "  3. See how many other points agree with this model\n",
    "  4. Keep the model with the most \"votes\"\n",
    "- **Pros**: Very robust to outliers\n",
    "- **Cons**: More complex, requires parameter tuning\n",
    "- **Real example**: Finding lane lines despite shadows and road markings\n",
    "\n",
    "### üîÑ **Homography**\n",
    "- **What it does**: Maps one image plane to another\n",
    "- **How it works**: Uses 8 parameters to transform rectangular shapes\n",
    "- **When to use**: When objects are on a flat surface (like documents, signs)\n",
    "- **Real example**: Scanning documents with your phone camera\n",
    "\n",
    "### üß© **Image Stitching**\n",
    "- **What it does**: Combines multiple overlapping images\n",
    "- **How it works**: \n",
    "  1. Find common features between images\n",
    "  2. Calculate the transformation between them\n",
    "  3. Blend the images seamlessly\n",
    "- **Real example**: Creating 360¬∞ panoramic photos\n",
    "\n",
    "## üéØ When to Use Which Method?\n",
    "\n",
    "### Use **Least Squares** when:\n",
    "- ‚úÖ Your data is relatively clean\n",
    "- ‚úÖ You don't expect many outliers\n",
    "- ‚úÖ You need a quick solution\n",
    "\n",
    "### Use **RANSAC** when:\n",
    "- ‚úÖ Your data has many outliers\n",
    "- ‚úÖ Robustness is more important than speed\n",
    "- ‚úÖ Working with real-world, noisy data\n",
    "\n",
    "## üí° Pro Tips for Beginners:\n",
    "1. **Start simple**: Always try least squares first\n",
    "2. **Visualize your data**: Plot points to see if outliers are obvious\n",
    "3. **Understand your noise**: Different noise types need different solutions\n",
    "4. **Parameter tuning**: RANSAC needs careful threshold setting\n",
    "\n",
    "Let's see these concepts in action with real examples! üîß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97af5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FittingAlignmentExamples:\n",
    "    \"\"\"\n",
    "    Fitting and Alignment: Find geometric transformations between images\n",
    "    Key concepts: Least Squares, RANSAC, Homography, Image Stitching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Fitting and Alignment\"\n",
    "    \n",
    "    def basic_line_fitting_example(self):\n",
    "        \"\"\"Basic Example: Line fitting using least squares\"\"\"\n",
    "        print(\"üîç BASIC EXAMPLE: Line Fitting with Least Squares\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Generate noisy line data\n",
    "        np.random.seed(42)\n",
    "        x_true = np.linspace(0, 10, 50)\n",
    "        y_true = 2 * x_true + 1  # True line: y = 2x + 1\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.normal(0, 2, len(x_true))\n",
    "        y_noisy = y_true + noise\n",
    "        \n",
    "        # Add some outliers\n",
    "        outlier_indices = np.random.choice(len(x_true), 5, replace=False)\n",
    "        y_noisy[outlier_indices] += np.random.normal(0, 10, 5)\n",
    "        \n",
    "        # Least squares fitting\n",
    "        A = np.vstack([x_true, np.ones(len(x_true))]).T\n",
    "        m, c = np.linalg.lstsq(A, y_noisy, rcond=None)[0]\n",
    "        \n",
    "        print(f\"True line: y = 2x + 1\")\n",
    "        print(f\"Fitted line: y = {m:.2f}x + {c:.2f}\")\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(x_true, y_noisy, alpha=0.6, label='Noisy Data')\n",
    "        plt.plot(x_true, y_true, 'g-', linewidth=2, label='True Line')\n",
    "        plt.plot(x_true, m*x_true + c, 'r--', linewidth=2, label='Fitted Line')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title('Line Fitting with Least Squares')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Basic line fitting completed!\")\n",
    "        print(\"üí° Least squares minimizes sum of squared residuals\")\n",
    "\n",
    "# Create instance for fitting and alignment examples\n",
    "fitting_processor = FittingAlignmentExamples()\n",
    "print(\"üìê Fitting and Alignment class initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the basic line fitting example\n",
    "fitting_processor.basic_line_fitting_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RANSAC method to the fitting class\n",
    "def advanced_ransac_example(self):\n",
    "    \"\"\"Advanced Example: RANSAC for robust line fitting\"\"\"\n",
    "    print(\"üöÄ ADVANCED EXAMPLE: RANSAC for Robust Fitting\")\n",
    "    print(\"-\" * 48)\n",
    "    \n",
    "    # Generate data with many outliers\n",
    "    np.random.seed(42)\n",
    "    n_inliers = 100\n",
    "    n_outliers = 50\n",
    "    \n",
    "    # Inliers: points near the line y = 2x + 1\n",
    "    x_inliers = np.random.uniform(0, 10, n_inliers)\n",
    "    y_inliers = 2 * x_inliers + 1 + np.random.normal(0, 0.5, n_inliers)\n",
    "    \n",
    "    # Outliers: random points\n",
    "    x_outliers = np.random.uniform(0, 10, n_outliers)\n",
    "    y_outliers = np.random.uniform(-5, 25, n_outliers)\n",
    "    \n",
    "    # Combine data\n",
    "    x_data = np.concatenate([x_inliers, x_outliers])\n",
    "    y_data = np.concatenate([y_inliers, y_outliers])\n",
    "    \n",
    "    # RANSAC implementation\n",
    "    def ransac_line_fitting(x, y, max_iterations=1000, threshold=1.0):\n",
    "        best_inliers = None\n",
    "        best_model = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            # Randomly sample 2 points\n",
    "            sample_indices = np.random.choice(len(x), 2, replace=False)\n",
    "            x_sample = x[sample_indices]\n",
    "            y_sample = y[sample_indices]\n",
    "            \n",
    "            # Fit line to sample\n",
    "            if x_sample[1] != x_sample[0]:  # Avoid division by zero\n",
    "                m = (y_sample[1] - y_sample[0]) / (x_sample[1] - x_sample[0])\n",
    "                c = y_sample[0] - m * x_sample[0]\n",
    "                \n",
    "                # Calculate distances to line\n",
    "                distances = np.abs(y - (m * x + c)) / np.sqrt(1 + m**2)\n",
    "                \n",
    "                # Find inliers\n",
    "                inliers = distances < threshold\n",
    "                score = np.sum(inliers)\n",
    "                \n",
    "                # Update best model\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_model = (m, c)\n",
    "                    best_inliers = inliers\n",
    "        \n",
    "        return best_model, best_inliers\n",
    "    \n",
    "    # Apply RANSAC\n",
    "    (m_ransac, c_ransac), inliers = ransac_line_fitting(x_data, y_data)\n",
    "    \n",
    "    # Standard least squares for comparison\n",
    "    A = np.vstack([x_data, np.ones(len(x_data))]).T\n",
    "    m_ls, c_ls = np.linalg.lstsq(A, y_data, rcond=None)[0]\n",
    "    \n",
    "    print(f\"True line: y = 2x + 1\")\n",
    "    print(f\"Least squares: y = {m_ls:.2f}x + {c_ls:.2f}\")\n",
    "    print(f\"RANSAC: y = {m_ransac:.2f}x + {c_ransac:.2f}\")\n",
    "    print(f\"RANSAC found {np.sum(inliers)} inliers out of {len(x_data)} points\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(x_data[inliers], y_data[inliers], \n",
    "               c='blue', alpha=0.6, label='Inliers')\n",
    "    plt.scatter(x_data[~inliers], y_data[~inliers], \n",
    "               c='red', alpha=0.6, label='Outliers')\n",
    "    \n",
    "    # Plot lines\n",
    "    x_line = np.linspace(0, 10, 100)\n",
    "    plt.plot(x_line, 2*x_line + 1, 'g-', linewidth=3, label='True Line')\n",
    "    plt.plot(x_line, m_ls*x_line + c_ls, 'r--', linewidth=2, label='Least Squares')\n",
    "    plt.plot(x_line, m_ransac*x_line + c_ransac, 'b:', linewidth=3, label='RANSAC')\n",
    "    \n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('RANSAC vs Least Squares with Outliers')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ RANSAC fitting completed!\")\n",
    "    print(\"üí° RANSAC is robust to outliers by iteratively finding the best model\")\n",
    "    print(\"üí° Least squares fails with many outliers, RANSAC succeeds\")\n",
    "\n",
    "# Add the method to the class\n",
    "FittingAlignmentExamples.advanced_ransac_example = advanced_ransac_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RANSAC example\n",
    "fitting_processor.advanced_ransac_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78ccfb",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Segmentation\n",
    "\n",
    "## ü§î What is Segmentation?\n",
    "\n",
    "**Segmentation** is like using digital scissors to cut out different parts of an image! It's the process of dividing an image into meaningful regions or objects. Think of it as:\n",
    "\n",
    "- **Coloring by numbers**: Each region gets a different \"color\" (label)\n",
    "- **Digital puzzle pieces**: Breaking the image into separate components\n",
    "- **Smart selection tools**: Like Photoshop's magic wand, but much smarter!\n",
    "\n",
    "## üåü Why Do We Need Segmentation?\n",
    "\n",
    "### Real-World Applications:\n",
    "1. **Medical Imaging**: Separating tumors from healthy tissue in MRI scans\n",
    "2. **Autonomous Vehicles**: Identifying roads, cars, pedestrians, traffic signs\n",
    "3. **Agriculture**: Counting crops, detecting plant diseases\n",
    "4. **Social Media**: Background removal for profile pictures\n",
    "5. **Manufacturing**: Quality control - finding defects in products\n",
    "6. **Satellite Imagery**: Monitoring deforestation, urban development\n",
    "\n",
    "## üìö Segmentation Methods Explained:\n",
    "\n",
    "### üéØ **Thresholding** - The Simplest Approach\n",
    "**Basic Idea**: \"If a pixel is bright enough, it's an object; if it's dark, it's background\"\n",
    "\n",
    "#### **Binary Thresholding**:\n",
    "- **How it works**: Pick a threshold value (e.g., 128). Pixels above = white, pixels below = black\n",
    "- **When to use**: High contrast images (text on paper, objects on plain backgrounds)\n",
    "- **Example**: Scanning handwritten documents\n",
    "\n",
    "#### **Otsu's Method**:\n",
    "- **How it works**: Automatically finds the best threshold by analyzing the image histogram\n",
    "- **Why it's smart**: No need to guess the threshold value!\n",
    "- **When to use**: When you're not sure what threshold to pick\n",
    "\n",
    "#### **Color-based Thresholding**:\n",
    "- **How it works**: Instead of brightness, use color ranges (e.g., \"all red objects\")\n",
    "- **Color spaces**: \n",
    "  - **RGB**: Red, Green, Blue (like computer monitors)\n",
    "  - **HSV**: Hue, Saturation, Value (more intuitive for humans)\n",
    "- **When to use**: When objects have distinct colors\n",
    "\n",
    "### üé® **K-means Clustering** - Grouping Similar Pixels\n",
    "**Basic Idea**: \"Group pixels that look similar together\"\n",
    "\n",
    "- **How it works**: \n",
    "  1. Choose number of groups (k)\n",
    "  2. Algorithm finds k \"cluster centers\"\n",
    "  3. Each pixel joins the nearest cluster\n",
    "  4. Repeat until clusters stabilize\n",
    "\n",
    "- **Pros**: \n",
    "  - No need to set thresholds\n",
    "  - Works with complex images\n",
    "  - Unsupervised (no training needed)\n",
    "\n",
    "- **Cons**:\n",
    "  - Need to choose k (number of clusters)\n",
    "  - Can be sensitive to initialization\n",
    "  - Assumes spherical clusters\n",
    "\n",
    "- **Real example**: Grouping pixels in a nature photo into sky, trees, grass, water\n",
    "\n",
    "### üåä **Watershed Algorithm** - Like Water Flowing Downhill\n",
    "**Basic Idea**: \"Imagine the image as a topographic map, and water flows from peaks to valleys\"\n",
    "\n",
    "- **How it works**:\n",
    "  1. Treat image intensity as elevation\n",
    "  2. \"Pour water\" from local minima\n",
    "  3. Where different waters meet = boundaries\n",
    "  4. Creates natural object separation\n",
    "\n",
    "- **When to use**: Separating touching objects (like overlapping coins)\n",
    "- **Advantage**: Good at separating connected objects\n",
    "- **Challenge**: Can create too many regions (over-segmentation)\n",
    "\n",
    "### üß† **Deep Learning Segmentation** - The Modern Approach\n",
    "**Examples**: U-Net, Mask R-CNN, DeepLab\n",
    "\n",
    "- **Semantic Segmentation**: \"This pixel belongs to 'car', this one to 'road'\"\n",
    "- **Instance Segmentation**: \"This pixel belongs to 'car #1', this one to 'car #2'\"\n",
    "- **Panoptic Segmentation**: Combines both semantic and instance\n",
    "\n",
    "## üéØ Choosing the Right Method:\n",
    "\n",
    "### Use **Thresholding** when:\n",
    "- ‚úÖ Simple images with clear contrast\n",
    "- ‚úÖ Limited computational resources\n",
    "- ‚úÖ Real-time applications\n",
    "- ‚úÖ Text or document processing\n",
    "\n",
    "### Use **K-means** when:\n",
    "- ‚úÖ Images with distinct color regions\n",
    "- ‚úÖ Don't know exact threshold values\n",
    "- ‚úÖ Need unsupervised approach\n",
    "- ‚úÖ Preprocessing for other algorithms\n",
    "\n",
    "### Use **Watershed** when:\n",
    "- ‚úÖ Objects are touching/overlapping\n",
    "- ‚úÖ Need precise boundaries\n",
    "- ‚úÖ Working with grayscale images\n",
    "- ‚úÖ Cell counting, particle analysis\n",
    "\n",
    "### Use **Deep Learning** when:\n",
    "- ‚úÖ Complex, real-world images\n",
    "- ‚úÖ Have labeled training data\n",
    "- ‚úÖ Need highest accuracy\n",
    "- ‚úÖ Can afford computational cost\n",
    "\n",
    "## üí° Beginner Tips:\n",
    "1. **Start simple**: Begin with thresholding on high-contrast images\n",
    "2. **Preprocessing matters**: Clean your image first (blur, enhance contrast)\n",
    "3. **Combine methods**: Often, multiple techniques work better together\n",
    "4. **Visualize results**: Always look at your segmentation masks\n",
    "5. **Iterate and improve**: Segmentation often needs fine-tuning\n",
    "\n",
    "Let's explore these techniques with hands-on examples! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceae2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationExamples:\n",
    "    \"\"\"\n",
    "    Segmentation: Partition images into meaningful regions\n",
    "    Key concepts: Thresholding, Region Growing, K-means, Deep Learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Segmentation\"\n",
    "    \n",
    "    def create_sample_image(self):\n",
    "        \"\"\"Create a sample image for segmentation\"\"\"\n",
    "        img = np.zeros((200, 200, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Background (dark blue)\n",
    "        img[:, :] = [20, 20, 80]\n",
    "        \n",
    "        # Object 1 (red circle)\n",
    "        cv2.circle(img, (60, 60), 30, (0, 0, 200), -1)\n",
    "        \n",
    "        # Object 2 (green rectangle)\n",
    "        cv2.rectangle(img, (120, 40), (180, 100), (0, 200, 0), -1)\n",
    "        \n",
    "        # Object 3 (yellow triangle)\n",
    "        pts = np.array([[100, 120], [80, 160], [120, 160]], np.int32)\n",
    "        cv2.fillPoly(img, [pts], (0, 200, 200))\n",
    "        \n",
    "        # Add some noise\n",
    "        noise = np.random.randint(-20, 20, img.shape, dtype=np.int16)\n",
    "        img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def basic_thresholding_example(self):\n",
    "        \"\"\"Basic Example: Color-based thresholding\"\"\"\n",
    "        print(\"üîç BASIC EXAMPLE: Color-based Thresholding\")\n",
    "        print(\"-\" * 42)\n",
    "        \n",
    "        img = self.create_sample_image()\n",
    "        \n",
    "        # Convert to different color spaces\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Simple grayscale thresholding\n",
    "        _, binary_thresh = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Otsu's automatic thresholding\n",
    "        _, otsu_thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Color-based thresholding (segment red objects)\n",
    "        # Define range for red color in HSV\n",
    "        lower_red1 = np.array([0, 50, 50])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([170, 50, 50])\n",
    "        upper_red2 = np.array([180, 255, 255])\n",
    "        \n",
    "        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "        red_mask = cv2.bitwise_or(mask1, mask2)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(gray, cmap='gray')\n",
    "        axes[0, 1].set_title('Grayscale')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[0, 2].imshow(binary_thresh, cmap='gray')\n",
    "        axes[0, 2].set_title('Binary Threshold (T=100)')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        axes[1, 0].imshow(otsu_thresh, cmap='gray')\n",
    "        axes[1, 0].set_title('Otsu Threshold (Automatic)')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(red_mask, cmap='gray')\n",
    "        axes[1, 1].set_title('Red Color Mask')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        # Apply red mask to original image\n",
    "        result = img.copy()\n",
    "        result[red_mask == 0] = [0, 0, 0]\n",
    "        axes[1, 2].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "        axes[1, 2].set_title('Red Objects Only')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Basic thresholding completed!\")\n",
    "        print(\"üí° Thresholding converts grayscale/color images to binary\")\n",
    "        print(\"üí° Otsu's method automatically finds optimal threshold\")\n",
    "        print(\"üí° HSV color space better for color-based segmentation\")\n",
    "\n",
    "# Create instance for segmentation examples\n",
    "segmentation_processor = SegmentationExamples()\n",
    "print(\"üé® Segmentation class initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the basic thresholding example\n",
    "segmentation_processor.basic_thresholding_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add K-means clustering method to segmentation class\n",
    "def advanced_clustering_example(self):\n",
    "    \"\"\"Advanced Example: K-means clustering for segmentation\"\"\"\n",
    "    print(\"üöÄ ADVANCED EXAMPLE: K-means Clustering Segmentation\")\n",
    "    print(\"-\" * 52)\n",
    "    \n",
    "    img = self.create_sample_image()\n",
    "    \n",
    "    # Prepare data for K-means\n",
    "    # Reshape image to be a list of pixels\n",
    "    data = img.reshape((-1, 3))\n",
    "    data = np.float32(data)\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    k = 4  # Number of clusters\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    # Convert back to uint8 and reshape\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    "    segmented_img = segmented_data.reshape(img.shape)\n",
    "    \n",
    "    # Create individual masks for each cluster\n",
    "    masks = []\n",
    "    for i in range(k):\n",
    "        mask = (labels.flatten() == i).reshape(img.shape[:2])\n",
    "        masks.append(mask.astype(np.uint8) * 255)\n",
    "    \n",
    "    # Advanced: Watershed segmentation\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply threshold to get binary image\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Remove noise\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    \n",
    "    # Find sure background area\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "    \n",
    "    # Find sure foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "    \n",
    "    # Find unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "    # Marker labeling\n",
    "    _, markers = cv2.connectedComponents(sure_fg)\n",
    "    markers = markers + 1\n",
    "    markers[unknown == 255] = 0\n",
    "    \n",
    "    # Apply watershed\n",
    "    img_watershed = img.copy()\n",
    "    markers = cv2.watershed(img_watershed, markers)\n",
    "    img_watershed[markers == -1] = [255, 0, 0]  # Mark boundaries in red\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(cv2.cvtColor(segmented_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 1].set_title(f'K-means (k={k})')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(masks[0], cmap='gray')\n",
    "    axes[0, 2].set_title('Cluster 1 Mask')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[0, 3].imshow(masks[1], cmap='gray')\n",
    "    axes[0, 3].set_title('Cluster 2 Mask')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(binary, cmap='gray')\n",
    "    axes[1, 0].set_title('Binary Threshold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(dist_transform, cmap='gray')\n",
    "    axes[1, 1].set_title('Distance Transform')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(markers, cmap='jet')\n",
    "    axes[1, 2].set_title('Watershed Markers')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    axes[1, 3].imshow(cv2.cvtColor(img_watershed, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 3].set_title('Watershed Segmentation')\n",
    "    axes[1, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Advanced segmentation completed!\")\n",
    "    print(\"üí° K-means groups pixels by color similarity\")\n",
    "    print(\"üí° Watershed uses morphology to separate touching objects\")\n",
    "    print(\"üí° Distance transform helps find object centers\")\n",
    "\n",
    "# Add the method to the class\n",
    "SegmentationExamples.advanced_clustering_example = advanced_clustering_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf29fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the advanced clustering example\n",
    "segmentation_processor.advanced_clustering_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0fb142",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Deep Learning for Vision\n",
    "\n",
    "## ü§î What is Deep Learning for Vision?\n",
    "\n",
    "**Deep Learning** is like teaching a computer to see and understand images the way humans do, but using artificial neural networks. Imagine if you could train a computer by showing it millions of pictures, just like how children learn by looking at picture books!\n",
    "\n",
    "## üß† Why Deep Learning Changed Everything:\n",
    "\n",
    "### Before Deep Learning (Traditional CV):\n",
    "- üë®‚Äçüíª **Manual feature engineering**: Humans had to tell computers what to look for\n",
    "- üìè **Limited patterns**: Could only detect simple shapes and edges\n",
    "- üêå **Slow progress**: Each new task required starting from scratch\n",
    "- üò§ **Frustrating results**: Worked in labs but failed in real world\n",
    "\n",
    "### After Deep Learning:\n",
    "- ü§ñ **Automatic feature learning**: Computers figure out what's important by themselves\n",
    "- üéØ **Complex pattern recognition**: Can understand scenes, emotions, context\n",
    "- üöÄ **Rapid development**: Same techniques work across many vision tasks\n",
    "- üåü **Human-level performance**: Often exceeds human accuracy!\n",
    "\n",
    "## üèóÔ∏è Building Blocks Explained:\n",
    "\n",
    "### üîç **Convolutional Neural Networks (CNNs)**\n",
    "**Think of it as**: A digital magnifying glass that scans the entire image\n",
    "\n",
    "#### **Convolutional Layers**:\n",
    "- **What they do**: Detect local features (edges, corners, textures)\n",
    "- **How they work**: Small filters slide across the image\n",
    "- **Why it's smart**: Same filter can detect a feature anywhere in the image\n",
    "- **Example**: A filter that detects horizontal lines will find them whether they're at the top or bottom of the image\n",
    "\n",
    "#### **Pooling Layers**:\n",
    "- **What they do**: Reduce image size while keeping important information\n",
    "- **Types**:\n",
    "  - **Max Pooling**: Keeps the strongest signal in each region\n",
    "  - **Average Pooling**: Keeps the average signal in each region\n",
    "- **Why it helps**: Makes the network focus on \"what\" rather than \"where\"\n",
    "- **Example**: Whether a cat's eye is at pixel (100,100) or (102,98) doesn't matter - it's still a cat!\n",
    "\n",
    "#### **Fully Connected Layers**:\n",
    "- **What they do**: Make final decisions based on all the features\n",
    "- **How they work**: Every neuron connects to every neuron in the previous layer\n",
    "- **Role**: Like a judge who looks at all the evidence and makes a verdict\n",
    "\n",
    "### üéØ **Classification** - \"What's in this image?\"\n",
    "- **Goal**: Assign a label to the entire image\n",
    "- **Examples**: \"This is a dog\", \"This is a cat\", \"This is a car\"\n",
    "- **Output**: Probability scores for each possible class\n",
    "- **Real applications**: Photo tagging, medical diagnosis, quality control\n",
    "\n",
    "### üîÑ **Transfer Learning** - Standing on the Shoulders of Giants\n",
    "**Basic Idea**: \"Why start from scratch when someone already taught a network to see?\"\n",
    "\n",
    "#### **How it works**:\n",
    "1. **Start with a pre-trained model**: Someone already trained it on millions of images\n",
    "2. **Freeze early layers**: Keep the basic feature detectors (edges, shapes)\n",
    "3. **Retrain final layers**: Adapt the decision-making part for your specific task\n",
    "4. **Fine-tune if needed**: Slightly adjust the entire network for your data\n",
    "\n",
    "#### **Why it's revolutionary**:\n",
    "- ‚ö° **Much faster training**: Days instead of weeks\n",
    "- üìä **Less data needed**: Thousands instead of millions of images\n",
    "- üí∞ **Cost effective**: Saves computational resources\n",
    "- üéØ **Better results**: Often performs better than training from scratch\n",
    "\n",
    "### üîç **Object Detection** - \"What's in this image and where?\"\n",
    "- **Goal**: Find and classify multiple objects in one image\n",
    "- **Output**: Bounding boxes + labels for each object\n",
    "- **Examples**: \"There's a person at (10,20) and a car at (100,150)\"\n",
    "- **Challenges**: Objects can overlap, have different sizes, appear partially\n",
    "\n",
    "### ü§ñ **Transformers** - The New Kid on the Block\n",
    "**Originally from**: Natural Language Processing (like ChatGPT)\n",
    "**Now applied to**: Computer vision with great success!\n",
    "\n",
    "#### **Vision Transformers (ViTs)**:\n",
    "- **Key idea**: Treat image patches like words in a sentence\n",
    "- **Advantage**: Can see relationships between distant parts of an image\n",
    "- **Example**: Understanding that a steering wheel belongs to the car in the background\n",
    "\n",
    "## üé® Network Architectures Explained:\n",
    "\n",
    "### **LeNet** (1990s):\n",
    "- **Historical significance**: First CNN for digit recognition\n",
    "- **Structure**: Simple: Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool ‚Üí FC\n",
    "- **Modern equivalent**: Good for learning basics\n",
    "\n",
    "### **AlexNet** (2012):\n",
    "- **Breakthrough moment**: Won ImageNet competition by huge margin\n",
    "- **Innovation**: Deep network + GPU training + ReLU activations\n",
    "- **Impact**: Started the deep learning revolution\n",
    "\n",
    "### **ResNet** (2015):\n",
    "- **Problem solved**: Very deep networks were hard to train\n",
    "- **Innovation**: Skip connections (\"shortcuts\") between layers\n",
    "- **Result**: Networks can now be 100+ layers deep\n",
    "- **Why it matters**: Deeper = better feature learning\n",
    "\n",
    "### **EfficientNet** (2019):\n",
    "- **Goal**: Best accuracy with least computational cost\n",
    "- **Innovation**: Balanced scaling of depth, width, and resolution\n",
    "- **Impact**: Great for mobile devices and resource-constrained environments\n",
    "\n",
    "## üõ†Ô∏è Training Process Demystified:\n",
    "\n",
    "### **1. Forward Pass**: \"Making a Guess\"\n",
    "- Image goes through all layers\n",
    "- Network produces a prediction\n",
    "- Like a student answering a test question\n",
    "\n",
    "### **2. Loss Calculation**: \"How Wrong Was the Guess?\"\n",
    "- Compare prediction with correct answer\n",
    "- Calculate a \"wrongness score\"\n",
    "- Common losses: Cross-entropy, Mean Squared Error\n",
    "\n",
    "### **3. Backward Pass**: \"Learning from Mistakes\"\n",
    "- Figure out which neurons were most responsible for the error\n",
    "- Use calculus (backpropagation) to compute gradients\n",
    "- Like understanding which study topics led to wrong answers\n",
    "\n",
    "### **4. Parameter Update**: \"Getting Better\"\n",
    "- Adjust network weights to reduce future errors\n",
    "- Use optimizers (Adam, SGD) to make smart updates\n",
    "- Like studying harder on topics you got wrong\n",
    "\n",
    "## üí° Beginner's Deep Learning Roadmap:\n",
    "\n",
    "### **Level 1: Understand the Basics**\n",
    "- üìö Learn what neurons and layers do\n",
    "- üîß Understand forward and backward passes\n",
    "- üéØ Practice with simple datasets (MNIST digits)\n",
    "\n",
    "### **Level 2: Build Simple Networks**\n",
    "- üèóÔ∏è Create basic CNNs from scratch\n",
    "- üìä Learn to evaluate and visualize results\n",
    "- üé® Experiment with different architectures\n",
    "\n",
    "### **Level 3: Master Transfer Learning**\n",
    "- üöÄ Use pre-trained models (ResNet, EfficientNet)\n",
    "- üîÑ Learn fine-tuning strategies\n",
    "- üìà Apply to real-world problems\n",
    "\n",
    "### **Level 4: Advanced Techniques**\n",
    "- üéØ Object detection and segmentation\n",
    "- ü§ñ Explore transformers and attention mechanisms\n",
    "- üèÜ Participate in competitions (Kaggle, DrivenData)\n",
    "\n",
    "## üö® Common Beginner Mistakes to Avoid:\n",
    "\n",
    "1. **Starting too complex**: Begin with simple problems and datasets\n",
    "2. **Ignoring data quality**: Garbage in = garbage out\n",
    "3. **Not visualizing results**: Always look at what your model is learning\n",
    "4. **Overfitting**: Model memorizes training data but fails on new data\n",
    "5. **Insufficient data**: Deep learning needs lots of examples\n",
    "6. **Wrong evaluation metrics**: Accuracy isn't always the right measure\n",
    "\n",
    "Let's dive into hands-on examples to see these concepts come to life! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningExamples:\n",
    "    \"\"\"\n",
    "    Deep Learning for Vision: Neural networks for image understanding\n",
    "    Key concepts: CNNs, Classification, Object Detection, Transformers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Deep Learning for Vision\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def basic_cnn_example(self):\n",
    "        \"\"\"Basic Example: Simple CNN for image classification\"\"\"\n",
    "        print(\"üîç BASIC EXAMPLE: Simple CNN for Classification\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Simple CNN architecture\n",
    "        class SimpleCNN(nn.Module):\n",
    "            def __init__(self, num_classes=3):\n",
    "                super(SimpleCNN, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "                self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "                self.pool = nn.MaxPool2d(2, 2)\n",
    "                self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "                self.fc2 = nn.Linear(128, num_classes)\n",
    "                self.dropout = nn.Dropout(0.5)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = self.pool(F.relu(self.conv1(x)))\n",
    "                x = self.pool(F.relu(self.conv2(x)))\n",
    "                x = x.view(-1, 32 * 8 * 8)\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                return x\n",
    "        \n",
    "        # Create synthetic dataset\n",
    "        class SyntheticDataset(Dataset):\n",
    "            def __init__(self, num_samples=300):\n",
    "                self.num_samples = num_samples\n",
    "                self.data = []\n",
    "                self.labels = []\n",
    "                \n",
    "                for _ in range(num_samples):\n",
    "                    # Random class (0, 1, or 2)\n",
    "                    class_label = np.random.randint(0, 3)\n",
    "                    \n",
    "                    # Create 32x32 image\n",
    "                    img = np.zeros((32, 32), dtype=np.float32)\n",
    "                    \n",
    "                    if class_label == 0:  # Vertical lines\n",
    "                        for i in range(5):\n",
    "                            x = np.random.randint(2, 30)\n",
    "                            img[:, x-1:x+2] = 1.0\n",
    "                    elif class_label == 1:  # Horizontal lines\n",
    "                        for i in range(5):\n",
    "                            y = np.random.randint(2, 30)\n",
    "                            img[y-1:y+2, :] = 1.0\n",
    "                    else:  # Diagonal pattern\n",
    "                        for i in range(32):\n",
    "                            for j in range(32):\n",
    "                                if (i + j) % 8 < 4:\n",
    "                                    img[i, j] = 1.0\n",
    "                    \n",
    "                    # Add noise\n",
    "                    noise = np.random.normal(0, 0.1, img.shape)\n",
    "                    img = np.clip(img + noise, 0, 1)\n",
    "                    \n",
    "                    self.data.append(img)\n",
    "                    self.labels.append(class_label)\n",
    "            \n",
    "            def __len__(self):\n",
    "                return self.num_samples\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                return torch.tensor(self.data[idx]).unsqueeze(0), torch.tensor(self.labels[idx])\n",
    "        \n",
    "        # Create datasets and loaders\n",
    "        train_dataset = SyntheticDataset(240)\n",
    "        test_dataset = SyntheticDataset(60)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SimpleCNN(num_classes=3).to(self.device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        losses = []\n",
    "        \n",
    "        print(\"Training CNN...\")\n",
    "        for epoch in range(10):\n",
    "            epoch_loss = 0\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            losses.append(avg_loss)\n",
    "            if epoch % 2 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        # Plot training loss\n",
    "        axes[0, 0].plot(losses)\n",
    "        axes[0, 0].set_title('Training Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Show sample images and predictions\n",
    "        test_batch = []\n",
    "        test_labels = []\n",
    "        for i in range(min(8, len(test_dataset))):\n",
    "            img, label = test_dataset[i]\n",
    "            test_batch.append(img)\n",
    "            test_labels.append(label)\n",
    "        \n",
    "        # Create a grid of test images\n",
    "        grid = torch.stack(test_batch[:8])\n",
    "        grid = grid.view(2, 4, 32, 32)\n",
    "        grid = grid.permute(0, 2, 1, 3).contiguous().view(64, 128)\n",
    "        \n",
    "        axes[0, 1].imshow(grid, cmap='gray')\n",
    "        axes[0, 1].set_title('Test Images (Top row: 0-3, Bottom row: 4-7)')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Get predictions for visualization\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_tensor = torch.stack(test_batch[:8]).to(self.device)\n",
    "            predictions = model(test_tensor)\n",
    "            _, predicted_classes = torch.max(predictions, 1)\n",
    "        \n",
    "        # Show individual test images with predictions\n",
    "        class_names = ['Vertical Lines', 'Horizontal Lines', 'Diagonal Pattern']\n",
    "        for i in range(6):\n",
    "            row = (i // 3) + 0\n",
    "            col = (i % 3) + 2\n",
    "            if col > 3:\n",
    "                row = 1\n",
    "                col = col - 4\n",
    "            \n",
    "            axes[row, col].imshow(test_batch[i].squeeze(), cmap='gray')\n",
    "            true_label = class_names[test_labels[i]]\n",
    "            pred_label = class_names[predicted_classes[i].cpu()]\n",
    "            color = 'green' if test_labels[i] == predicted_classes[i].cpu() else 'red'\n",
    "            axes[row, col].set_title(f'True: {true_label}\\\\nPred: {pred_label}', color=color, fontsize=8)\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Basic CNN training completed!\")\n",
    "        print(\"üí° CNNs use convolutional layers to detect local features\")\n",
    "        print(\"üí° Pooling layers reduce spatial dimensions\")\n",
    "        print(\"üí° Fully connected layers perform final classification\")\n",
    "\n",
    "# Create instance for deep learning examples\n",
    "dl_processor = DeepLearningExamples()\n",
    "print(\"üß† Deep Learning class initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the basic CNN example\n",
    "dl_processor.basic_cnn_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac6fb1",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ 3D Vision\n",
    "\n",
    "## ü§î What is 3D Vision?\n",
    "\n",
    "**3D Vision** is about understanding the three-dimensional world from 2D images - just like how your two eyes work together to perceive depth! It's one of the most challenging and exciting areas of computer vision.\n",
    "\n",
    "## üëÄ How Human Vision Works (The Inspiration):\n",
    "\n",
    "### **Binocular Vision**:\n",
    "- **Two eyes**: Slightly different viewpoints (about 6.5cm apart)\n",
    "- **Brain processing**: Compares the differences between left and right eye images\n",
    "- **Depth perception**: Brain calculates how far away objects are\n",
    "- **Parallax effect**: Closer objects seem to move more when you shift your head\n",
    "\n",
    "### **Monocular Cues** (Single Eye):\n",
    "- **Perspective**: Parallel lines converge in the distance\n",
    "- **Occlusion**: Closer objects block distant ones\n",
    "- **Size**: Known objects appear smaller when farther away\n",
    "- **Shadows**: Help understand 3D shape and position\n",
    "\n",
    "## üåü Why 3D Vision Matters:\n",
    "\n",
    "### **Revolutionary Applications**:\n",
    "1. **Autonomous Vehicles**: Understanding 3D space to navigate safely\n",
    "2. **Robotics**: Grasping objects, avoiding obstacles\n",
    "3. **Augmented Reality**: Placing virtual objects in real space\n",
    "4. **Medical Imaging**: 3D reconstruction of organs for surgery planning\n",
    "5. **Entertainment**: 3D movies, VR gaming, motion capture\n",
    "6. **Manufacturing**: Quality control with 3D measurements\n",
    "7. **Archaeology**: 3D documentation of historical sites\n",
    "\n",
    "## üìö Core 3D Vision Concepts:\n",
    "\n",
    "### üì∑ **Camera Models** - Understanding How Cameras Work\n",
    "\n",
    "#### **Pinhole Camera Model**:\n",
    "- **Basic principle**: Light travels in straight lines through a tiny hole\n",
    "- **Projection**: 3D world ‚Üí 2D image plane\n",
    "- **Mathematics**: Uses perspective projection equations\n",
    "- **Key insight**: All 3D points on a line through the camera center project to the same 2D point\n",
    "\n",
    "#### **Camera Parameters**:\n",
    "**Intrinsic Parameters** (internal camera properties):\n",
    "- **Focal length**: How \"zoomed in\" the camera is\n",
    "- **Principal point**: Center of the image (usually image center)\n",
    "- **Distortion**: How the lens warps straight lines\n",
    "\n",
    "**Extrinsic Parameters** (camera position in world):\n",
    "- **Rotation**: Which way the camera is pointing\n",
    "- **Translation**: Where the camera is located\n",
    "\n",
    "### üîß **Camera Calibration** - Teaching Computers About Camera Properties\n",
    "**Goal**: Find the intrinsic and extrinsic parameters\n",
    "\n",
    "#### **How it works**:\n",
    "1. **Take photos**: Multiple images of a known pattern (chessboard)\n",
    "2. **Find corners**: Detect the pattern in each image\n",
    "3. **Solve equations**: Use known 3D positions and observed 2D positions\n",
    "4. **Output**: Camera matrix that maps 3D ‚Üí 2D\n",
    "\n",
    "#### **Why it's crucial**:\n",
    "- **Undistort images**: Remove lens distortion\n",
    "- **Accurate measurements**: Convert pixels to real-world units\n",
    "- **3D reconstruction**: Essential for stereo vision\n",
    "\n",
    "### üëÅÔ∏èüëÅÔ∏è **Stereo Vision** - Two Camera Approach\n",
    "\n",
    "#### **Basic Concept**:\n",
    "- **Two cameras**: Like human eyes, slightly apart\n",
    "- **Same scene**: Both cameras look at the same objects\n",
    "- **Disparity**: How much an object appears to shift between cameras\n",
    "- **Depth relationship**: Closer objects have larger disparity\n",
    "\n",
    "#### **The Stereo Process**:\n",
    "1. **Calibrate cameras**: Find internal parameters and relative positions\n",
    "2. **Rectify images**: Make them appear as if cameras are perfectly aligned\n",
    "3. **Find correspondences**: Match pixels between left and right images\n",
    "4. **Calculate disparity**: Measure pixel shifts\n",
    "5. **Compute depth**: Use disparity to calculate distance\n",
    "\n",
    "#### **Challenges**:\n",
    "- **Correspondence problem**: Which pixel in left image matches which in right?\n",
    "- **Occlusions**: Some areas visible in only one camera\n",
    "- **Textureless regions**: Smooth areas are hard to match\n",
    "- **Lighting differences**: Shadows, reflections can confuse matching\n",
    "\n",
    "### üèóÔ∏è **Structure from Motion (SfM)** - 3D from Multiple Views\n",
    "\n",
    "#### **The Magic**: Reconstruct 3D scene from multiple 2D images taken from different positions\n",
    "\n",
    "#### **How it works**:\n",
    "1. **Feature detection**: Find distinctive points in each image\n",
    "2. **Feature matching**: Find the same points across multiple images\n",
    "3. **Estimate motion**: Calculate camera positions for each image\n",
    "4. **Triangulation**: Use multiple views to compute 3D positions\n",
    "5. **Bundle adjustment**: Refine everything simultaneously for best fit\n",
    "\n",
    "#### **Applications**:\n",
    "- **Photogrammetry**: Creating 3D models from photos\n",
    "- **Drone mapping**: Aerial surveys and mapping\n",
    "- **Cultural heritage**: Preserving monuments in 3D\n",
    "- **Visual effects**: Creating 3D models for movies\n",
    "\n",
    "### üåê **SLAM (Simultaneous Localization and Mapping)**\n",
    "\n",
    "#### **The Challenge**: Robot exploring unknown environment needs to:\n",
    "- **Localize**: Figure out where it is\n",
    "- **Map**: Build a map of the environment\n",
    "- **Chicken-and-egg problem**: Need map to localize, need location to map!\n",
    "\n",
    "#### **Solution Approaches**:\n",
    "**Visual SLAM**:\n",
    "- Use cameras to track features\n",
    "- Build 3D map while tracking camera position\n",
    "- Examples: ORB-SLAM, RTAB-Map\n",
    "\n",
    "**LiDAR SLAM**:\n",
    "- Use laser sensors for precise distance measurements\n",
    "- More accurate but expensive\n",
    "- Examples: LOAM, LeGO-LOAM\n",
    "\n",
    "### üìê **Epipolar Geometry** - The Mathematics of Two Views\n",
    "\n",
    "#### **Key Concepts**:\n",
    "**Epipolar Lines**: \n",
    "- For any point in one image, its corresponding point in the other image must lie on a specific line\n",
    "- Reduces 2D search to 1D search\n",
    "\n",
    "**Essential Matrix**:\n",
    "- Encodes the relative position and orientation between two cameras\n",
    "- Fundamental for stereo vision and SfM\n",
    "\n",
    "**Fundamental Matrix**:\n",
    "- Like essential matrix but for uncalibrated cameras\n",
    "- More general but less precise\n",
    "\n",
    "## üéØ Choosing the Right 3D Method:\n",
    "\n",
    "### Use **Stereo Vision** when:\n",
    "- ‚úÖ You can control camera setup\n",
    "- ‚úÖ Need real-time depth estimation\n",
    "- ‚úÖ Working in structured environments\n",
    "- ‚úÖ Have two synchronized cameras\n",
    "\n",
    "### Use **Structure from Motion** when:\n",
    "- ‚úÖ Have multiple unstructured photos\n",
    "- ‚úÖ Need high-quality 3D reconstruction\n",
    "- ‚úÖ Can process offline\n",
    "- ‚úÖ Working with existing photo collections\n",
    "\n",
    "### Use **SLAM** when:\n",
    "- ‚úÖ Robot navigation is the goal\n",
    "- ‚úÖ Environment is unknown\n",
    "- ‚úÖ Need real-time performance\n",
    "- ‚úÖ Both mapping and localization are needed\n",
    "\n",
    "## üí° Beginner's 3D Vision Journey:\n",
    "\n",
    "### **Stage 1: Understanding Basics**\n",
    "- üìê Learn camera models and projections\n",
    "- üéØ Practice camera calibration\n",
    "- üëÄ Understand stereo geometry\n",
    "\n",
    "### **Stage 2: Hands-on Practice**\n",
    "- üì∑ Calibrate your own camera\n",
    "- üîç Try stereo matching algorithms\n",
    "- üìä Visualize disparity maps\n",
    "\n",
    "### **Stage 3: Advanced Techniques**\n",
    "- üèóÔ∏è Implement Structure from Motion\n",
    "- ü§ñ Explore SLAM algorithms\n",
    "- üéÆ Build AR/VR applications\n",
    "\n",
    "### **Stage 4: Cutting-edge Research**\n",
    "- üß† Deep learning for 3D (NeRF, 3D GANs)\n",
    "- ‚òÅÔ∏è Point cloud processing\n",
    "- üåü Multi-modal 3D understanding\n",
    "\n",
    "## üö® Common Pitfalls for Beginners:\n",
    "\n",
    "1. **Skipping calibration**: Always calibrate your cameras first!\n",
    "2. **Ignoring lighting**: Consistent lighting crucial for matching\n",
    "3. **Poor baseline**: Cameras too close = poor depth, too far = correspondence problems\n",
    "4. **Forgetting scale**: Stereo gives relative depth, not absolute measurements\n",
    "5. **Oversimplifying**: Real-world 3D vision is much harder than tutorials suggest\n",
    "\n",
    "Let's explore these fascinating 3D concepts with practical examples! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeDVisionExamples:\n",
    "    \"\"\"\n",
    "    3D Vision: Reconstruct 3D information from 2D images\n",
    "    Key concepts: Camera calibration, Stereo vision, Structure from Motion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"3D Vision\"\n",
    "    \n",
    "    def basic_stereo_vision_example(self):\n",
    "        \"\"\"Basic Example: Stereo depth estimation\"\"\"\n",
    "        print(\"üîç BASIC EXAMPLE: Stereo Depth Estimation\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create synthetic stereo pair\n",
    "        def create_stereo_pair():\n",
    "            # Create a scene with objects at different depths\n",
    "            height, width = 200, 300\n",
    "            left_img = np.zeros((height, width), dtype=np.uint8)\n",
    "            right_img = np.zeros((height, width), dtype=np.uint8)\n",
    "            \n",
    "            # Background\n",
    "            left_img.fill(50)\n",
    "            right_img.fill(50)\n",
    "            \n",
    "            # Object 1: Close rectangle (large disparity)\n",
    "            cv2.rectangle(left_img, (50, 50), (100, 100), 200, -1)\n",
    "            cv2.rectangle(right_img, (40, 50), (90, 100), 200, -1)  # 10 pixel shift\n",
    "            \n",
    "            # Object 2: Medium distance circle (medium disparity)\n",
    "            cv2.circle(left_img, (180, 80), 25, 150, -1)\n",
    "            cv2.circle(right_img, (175, 80), 25, 150, -1)  # 5 pixel shift\n",
    "            \n",
    "            # Object 3: Far triangle (small disparity)\n",
    "            pts_left = np.array([[150, 120], [130, 160], [170, 160]], np.int32)\n",
    "            pts_right = np.array([[148, 120], [128, 160], [168, 160]], np.int32)  # 2 pixel shift\n",
    "            cv2.fillPoly(left_img, [pts_left], 100)\n",
    "            cv2.fillPoly(right_img, [pts_right], 100)\n",
    "            \n",
    "            # Add some texture/noise\n",
    "            noise_left = np.random.randint(0, 30, (height, width), dtype=np.uint8)\n",
    "            noise_right = np.random.randint(0, 30, (height, width), dtype=np.uint8)\n",
    "            left_img = cv2.add(left_img, noise_left)\n",
    "            right_img = cv2.add(right_img, noise_right)\n",
    "            \n",
    "            return left_img, right_img\n",
    "        \n",
    "        left_img, right_img = create_stereo_pair()\n",
    "        \n",
    "        # Compute disparity using OpenCV's StereoBM\n",
    "        stereo = cv2.StereoBM_create(numDisparities=16*5, blockSize=21)\n",
    "        disparity = stereo.compute(left_img, right_img)\n",
    "        \n",
    "        # Normalize disparity for visualization\n",
    "        disparity_norm = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "        \n",
    "        # Create depth map (inverse of disparity)\n",
    "        depth_map = np.zeros_like(disparity_norm)\n",
    "        depth_map[disparity > 0] = 255 - disparity_norm[disparity > 0]\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        axes[0, 0].imshow(left_img, cmap='gray')\n",
    "        axes[0, 0].set_title('Left Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(right_img, cmap='gray')\n",
    "        axes[0, 1].set_title('Right Image')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        axes[1, 0].imshow(disparity_norm, cmap='hot')\n",
    "        axes[1, 0].set_title('Disparity Map')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        axes[1, 1].imshow(depth_map, cmap='viridis')\n",
    "        axes[1, 1].set_title('Depth Map')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Basic stereo vision completed!\")\n",
    "        print(\"üí° Stereo vision uses disparity (pixel shift) to estimate depth\")\n",
    "        print(\"üí° Closer objects have larger disparity\")\n",
    "        print(\"üí° Block matching finds corresponding pixels between images\")\n",
    "\n",
    "# Create instance for 3D vision examples\n",
    "vision_3d_processor = ThreeDVisionExamples()\n",
    "print(\"üåê 3D Vision class initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6160bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the stereo vision example\n",
    "vision_3d_processor.basic_stereo_vision_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5533040",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ Generative AI for Vision\n",
    "\n",
    "## ü§î What is Generative AI for Vision?\n",
    "\n",
    "**Generative AI** is like having a digital artist that can create new images, modify existing ones, or even imagine things that never existed! Unlike traditional computer vision that **analyzes** images, generative AI **creates** them.\n",
    "\n",
    "## üé® The Creative Revolution in AI:\n",
    "\n",
    "### **From Analysis to Creation**:\n",
    "- **Traditional CV**: \"What's in this image?\" (Recognition)\n",
    "- **Generative AI**: \"Create an image of...\" (Generation)\n",
    "- **Game changer**: AI becomes a creative partner, not just an analytical tool\n",
    "\n",
    "### **The Magic Behind It**:\n",
    "- **Learning patterns**: AI studies millions of images to understand visual patterns\n",
    "- **Understanding concepts**: Learns relationships between objects, styles, and contexts\n",
    "- **Creative synthesis**: Combines learned patterns in novel ways\n",
    "\n",
    "## üåü Revolutionary Applications:\n",
    "\n",
    "### **Art and Entertainment**:\n",
    "1. **AI Art**: DALL-E, Midjourney, Stable Diffusion creating stunning artwork\n",
    "2. **Movie VFX**: Generating realistic backgrounds, creatures, effects\n",
    "3. **Game Development**: Creating textures, characters, entire virtual worlds\n",
    "4. **Fashion Design**: Generating new clothing patterns and styles\n",
    "\n",
    "### **Practical Applications**:\n",
    "1. **Data Augmentation**: Creating more training data for ML models\n",
    "2. **Medical Imaging**: Generating synthetic medical scans for research\n",
    "3. **Architecture**: Visualizing building designs and layouts\n",
    "4. **Product Design**: Rapid prototyping of new products\n",
    "5. **Education**: Creating custom illustrations for learning materials\n",
    "\n",
    "## üèóÔ∏è Core Generative Models Explained:\n",
    "\n",
    "### üîÑ **Autoencoders** - The Compression Artists\n",
    "\n",
    "#### **Basic Concept**: \"Compress then reconstruct\"\n",
    "- **Encoder**: Squeezes image into a small representation (like a summary)\n",
    "- **Decoder**: Reconstructs the original image from the summary\n",
    "- **Learning**: Model learns to preserve important details while discarding noise\n",
    "\n",
    "#### **Types of Autoencoders**:\n",
    "\n",
    "**Vanilla Autoencoder**:\n",
    "- **Goal**: Perfect reconstruction\n",
    "- **Use case**: Denoising, compression, dimensionality reduction\n",
    "- **Limitation**: Can only recreate what it has seen before\n",
    "\n",
    "**Variational Autoencoder (VAE)**:\n",
    "- **Innovation**: Learns a probability distribution, not just single representations\n",
    "- **Advantage**: Can generate new images by sampling from learned distribution\n",
    "- **Use case**: Generating new faces, handwriting, artistic styles\n",
    "\n",
    "**Denoising Autoencoder**:\n",
    "- **Training trick**: Add noise to inputs, train to recover clean images\n",
    "- **Result**: Robust to noise, better feature learning\n",
    "- **Applications**: Image restoration, removing artifacts\n",
    "\n",
    "### ‚öîÔ∏è **Generative Adversarial Networks (GANs)** - The Art Forgers\n",
    "\n",
    "#### **The Revolutionary Idea**: Two neural networks competing against each other\n",
    "\n",
    "**Generator** (The Forger):\n",
    "- **Goal**: Create fake images that look real\n",
    "- **Input**: Random noise\n",
    "- **Output**: Synthetic images\n",
    "- **Training**: Tries to fool the discriminator\n",
    "\n",
    "**Discriminator** (The Detective):\n",
    "- **Goal**: Distinguish real images from fake ones\n",
    "- **Input**: Real or generated images\n",
    "- **Output**: \"Real\" or \"Fake\" classification\n",
    "- **Training**: Gets better at detecting fakes\n",
    "\n",
    "#### **The Competition**:\n",
    "1. **Round 1**: Generator creates obvious fakes, discriminator easily spots them\n",
    "2. **Learning**: Both networks improve through training\n",
    "3. **Arms race**: Generator gets better at faking, discriminator gets better at detecting\n",
    "4. **Equilibrium**: Generator creates images so good that discriminator can't tell the difference\n",
    "\n",
    "#### **Famous GAN Variants**:\n",
    "\n",
    "**DCGAN (Deep Convolutional GAN)**:\n",
    "- **Innovation**: Used convolutional layers for better image generation\n",
    "- **Impact**: First GAN to generate high-quality images\n",
    "\n",
    "**StyleGAN**:\n",
    "- **Breakthrough**: Incredible control over generated image style\n",
    "- **Features**: Can change age, gender, hair color, expression independently\n",
    "- **Result**: Ultra-realistic fake human faces\n",
    "\n",
    "**CycleGAN**:\n",
    "- **Purpose**: Image-to-image translation without paired data\n",
    "- **Examples**: Photos ‚Üí Paintings, Summer ‚Üí Winter, Horses ‚Üí Zebras\n",
    "- **Magic**: Learns transformations from unpaired image sets\n",
    "\n",
    "### üåä **Diffusion Models** - The New Champions\n",
    "\n",
    "#### **The Process**: Like watching a photo slowly emerge from static\n",
    "\n",
    "**Forward Process** (Adding Noise):\n",
    "- Start with a real image\n",
    "- Gradually add random noise\n",
    "- Eventually becomes pure noise\n",
    "\n",
    "**Reverse Process** (Generation):\n",
    "- Start with pure noise\n",
    "- Gradually remove noise\n",
    "- End up with a clear, realistic image\n",
    "\n",
    "#### **Why They're Revolutionary**:\n",
    "- **Quality**: Often better than GANs\n",
    "- **Stability**: Easier to train than GANs\n",
    "- **Control**: Excellent for conditional generation\n",
    "- **Versatility**: Work well for images, audio, video, 3D\n",
    "\n",
    "#### **Famous Diffusion Models**:\n",
    "**Stable Diffusion**:\n",
    "- **Capability**: Text-to-image generation\n",
    "- **Accessibility**: Can run on consumer hardware\n",
    "- **Impact**: Democratized AI art creation\n",
    "\n",
    "**DALL-E 2**:\n",
    "- **Strength**: Incredible text understanding\n",
    "- **Features**: Can edit specific parts of images\n",
    "- **Quality**: Photorealistic results\n",
    "\n",
    "### üéØ **Self-Supervised Learning** - Learning Without Labels\n",
    "\n",
    "#### **The Challenge**: Labeled data is expensive and time-consuming to create\n",
    "\n",
    "#### **The Solution**: Make the model learn from the data itself\n",
    "\n",
    "**Common Techniques**:\n",
    "\n",
    "**Masked Image Modeling**:\n",
    "- **Process**: Hide parts of an image, train model to fill in the gaps\n",
    "- **Benefit**: Learns to understand image structure and context\n",
    "- **Example**: MAE (Masked Autoencoders)\n",
    "\n",
    "**Contrastive Learning**:\n",
    "- **Idea**: Similar images should have similar representations\n",
    "- **Process**: Train model to bring similar images closer, push different ones apart\n",
    "- **Applications**: Learning visual representations without labels\n",
    "\n",
    "**Rotation Prediction**:\n",
    "- **Task**: Rotate images and train model to predict rotation angle\n",
    "- **Learning**: Model must understand object orientation and structure\n",
    "\n",
    "## üîß Technical Deep Dive:\n",
    "\n",
    "### **Loss Functions in Generative Models**:\n",
    "\n",
    "**Reconstruction Loss**:\n",
    "- **Purpose**: Ensure generated images look like the originals\n",
    "- **Common types**: L1 (sharp edges), L2 (smooth), Perceptual (human-like)\n",
    "\n",
    "**Adversarial Loss**:\n",
    "- **Purpose**: Make generated images indistinguishable from real ones\n",
    "- **Challenge**: Hard to optimize, can be unstable\n",
    "\n",
    "**Perceptual Loss**:\n",
    "- **Innovation**: Compare high-level features, not just pixels\n",
    "- **Advantage**: Better captures human perception of similarity\n",
    "\n",
    "### **Evaluation Metrics**:\n",
    "\n",
    "**Inception Score (IS)**:\n",
    "- **Measures**: Quality and diversity of generated images\n",
    "- **How**: Uses pre-trained classifier to evaluate realism\n",
    "\n",
    "**Fr√©chet Inception Distance (FID)**:\n",
    "- **Compares**: Statistical properties of real vs generated images\n",
    "- **Better**: Lower FID = more realistic images\n",
    "\n",
    "**Human Evaluation**:\n",
    "- **Gold standard**: Human judges rate image quality\n",
    "- **Challenge**: Expensive and subjective\n",
    "\n",
    "## üéØ Choosing the Right Generative Model:\n",
    "\n",
    "### Use **Autoencoders** when:\n",
    "- ‚úÖ Need image compression or denoising\n",
    "- ‚úÖ Want to understand data structure\n",
    "- ‚úÖ Building a foundation for other models\n",
    "- ‚úÖ Limited computational resources\n",
    "\n",
    "### Use **GANs** when:\n",
    "- ‚úÖ Need high-quality, realistic images\n",
    "- ‚úÖ Want fast generation at inference time\n",
    "- ‚úÖ Have stable training setup\n",
    "- ‚úÖ Quality is more important than training stability\n",
    "\n",
    "### Use **Diffusion Models** when:\n",
    "- ‚úÖ Want the highest quality results\n",
    "- ‚úÖ Need fine-grained control over generation\n",
    "- ‚úÖ Can afford longer generation times\n",
    "- ‚úÖ Working with text-to-image tasks\n",
    "\n",
    "## üí° Beginner's Generative AI Roadmap:\n",
    "\n",
    "### **Level 1: Foundation**\n",
    "- üîß Understand basic autoencoders\n",
    "- üìä Learn about loss functions and training\n",
    "- üé® Experiment with simple datasets (MNIST, CIFAR)\n",
    "\n",
    "### **Level 2: Intermediate**\n",
    "- ‚öîÔ∏è Explore GANs and adversarial training\n",
    "- üéØ Try image-to-image translation\n",
    "- üìà Learn evaluation metrics\n",
    "\n",
    "### **Level 3: Advanced**\n",
    "- üåä Dive into diffusion models\n",
    "- üé® Experiment with text-to-image generation\n",
    "- üî¨ Explore cutting-edge architectures\n",
    "\n",
    "### **Level 4: Expert**\n",
    "- üöÄ Contribute to open-source projects\n",
    "- üìù Read and implement latest research papers\n",
    "- üèÜ Create your own novel architectures\n",
    "\n",
    "## ‚ö†Ô∏è Ethical Considerations:\n",
    "\n",
    "### **Potential Misuse**:\n",
    "- **Deepfakes**: Fake videos of real people\n",
    "- **Misinformation**: Generating fake news images\n",
    "- **Copyright**: Training on copyrighted material\n",
    "- **Bias**: Perpetuating societal biases in training data\n",
    "\n",
    "### **Responsible Development**:\n",
    "- **Watermarking**: Marking AI-generated content\n",
    "- **Bias detection**: Testing for unfair biases\n",
    "- **Consent**: Respecting people's image rights\n",
    "- **Transparency**: Being clear about AI involvement\n",
    "\n",
    "## üö® Common Beginner Challenges:\n",
    "\n",
    "1. **Mode collapse in GANs**: Generator produces limited variety\n",
    "2. **Training instability**: Models failing to converge\n",
    "3. **Evaluation difficulty**: How to measure generation quality?\n",
    "4. **Computational requirements**: Large models need powerful hardware\n",
    "5. **Hyperparameter sensitivity**: Small changes, big differences in results\n",
    "\n",
    "Let's explore these cutting-edge generative techniques with hands-on examples! üé®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b988e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeAIExamples:\n",
    "    \"\"\"\n",
    "    Generative AI for Vision: Generate new visual content\n",
    "    Key concepts: GANs, VAEs, Diffusion Models, Self-supervision\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Generative AI for Vision\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def basic_autoencoder_example(self):\n",
    "        \"\"\"Basic Example: Simple Autoencoder for image reconstruction\"\"\"\n",
    "        print(\"üîç BASIC EXAMPLE: Autoencoder for Image Reconstruction\")\n",
    "        print(\"-\" * 52)\n",
    "        \n",
    "        # Simple autoencoder architecture\n",
    "        class SimpleAutoencoder(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(SimpleAutoencoder, self).__init__()\n",
    "                # Encoder\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Conv2d(1, 16, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2, 2),\n",
    "                    nn.Conv2d(16, 8, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(2, 2),\n",
    "                    nn.Conv2d(8, 4, 3, padding=1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                \n",
    "                # Decoder\n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(4, 8, 2, stride=2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.ConvTranspose2d(8, 16, 2, stride=2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.ConvTranspose2d(16, 1, 3, padding=1),\n",
    "                    nn.Sigmoid()\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                encoded = self.encoder(x)\n",
    "                decoded = self.decoder(encoded)\n",
    "                return decoded\n",
    "        \n",
    "        # Create synthetic dataset\n",
    "        def create_synthetic_shapes(num_samples=500):\n",
    "            data = []\n",
    "            for _ in range(num_samples):\n",
    "                # Create 32x32 image\n",
    "                img = np.zeros((32, 32), dtype=np.float32)\n",
    "                \n",
    "                # Random shape type\n",
    "                shape_type = np.random.randint(0, 3)\n",
    "                \n",
    "                if shape_type == 0:  # Circle\n",
    "                    center = (np.random.randint(8, 24), np.random.randint(8, 24))\n",
    "                    radius = np.random.randint(3, 8)\n",
    "                    cv2.circle(img, center, radius, 1.0, -1)\n",
    "                elif shape_type == 1:  # Rectangle\n",
    "                    x1, y1 = np.random.randint(4, 16), np.random.randint(4, 16)\n",
    "                    x2, y2 = x1 + np.random.randint(8, 16), y1 + np.random.randint(8, 16)\n",
    "                    x2, y2 = min(x2, 31), min(y2, 31)\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), 1.0, -1)\n",
    "                else:  # Triangle\n",
    "                    pts = np.array([\n",
    "                        [np.random.randint(8, 24), np.random.randint(4, 12)],\n",
    "                        [np.random.randint(4, 16), np.random.randint(20, 28)],\n",
    "                        [np.random.randint(16, 28), np.random.randint(20, 28)]\n",
    "                    ], np.int32)\n",
    "                    cv2.fillPoly(img, [pts], 1.0)\n",
    "                \n",
    "                # Add noise\n",
    "                noise = np.random.normal(0, 0.1, img.shape)\n",
    "                img = np.clip(img + noise, 0, 1)\n",
    "                \n",
    "                data.append(img)\n",
    "            \n",
    "            return np.array(data)\n",
    "        \n",
    "        # Generate dataset\n",
    "        data = create_synthetic_shapes(400)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        dataset = torch.tensor(data).unsqueeze(1).float()  # Add channel dimension\n",
    "        train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = SimpleAutoencoder().to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        losses = []\n",
    "        \n",
    "        print(\"Training autoencoder...\")\n",
    "        for epoch in range(20):\n",
    "            epoch_loss = 0\n",
    "            for batch in train_loader:\n",
    "                batch = batch.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                reconstructed = model(batch)\n",
    "                loss = criterion(reconstructed, batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            losses.append(avg_loss)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {avg_loss:.6f}')\n",
    "        \n",
    "        # Test reconstruction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_batch = dataset[:8].to(self.device)\n",
    "            reconstructions = model(test_batch)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(3, 8, figsize=(20, 8))\n",
    "        \n",
    "        for i in range(8):\n",
    "            # Original\n",
    "            axes[0, i].imshow(dataset[i].squeeze(), cmap='gray')\n",
    "            axes[0, i].set_title(f'Original {i+1}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Reconstructed\n",
    "            axes[1, i].imshow(reconstructions[i].cpu().squeeze(), cmap='gray')\n",
    "            axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            # Difference\n",
    "            diff = torch.abs(dataset[i] - reconstructions[i].cpu()).squeeze()\n",
    "            axes[2, i].imshow(diff, cmap='hot')\n",
    "            axes[2, i].set_title(f'Difference {i+1}')\n",
    "            axes[2, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot training loss\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(losses)\n",
    "        plt.title('Autoencoder Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Autoencoder training completed!\")\n",
    "        print(\"üí° Autoencoders learn compressed representations of data\")\n",
    "        print(\"üí° The encoder compresses, the decoder reconstructs\")\n",
    "        print(\"üí° Can be used for denoising, compression, and feature learning\")\n",
    "\n",
    "# Create instance for generative AI examples\n",
    "gen_ai_processor = GenerativeAIExamples()\n",
    "print(\"üé® Generative AI class initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the autoencoder example\n",
    "gen_ai_processor.basic_autoencoder_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration - Run all examples in sequence\n",
    "print(\"üéØ RUNNING ALL COMPUTER VISION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"1Ô∏è‚É£ IMAGE PROCESSING EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\\\nüì∑ Running basic filtering...\")\n",
    "img_processor.basic_filtering_example()\n",
    "\n",
    "print(\"\\\\nüîß Running advanced edge detection...\")\n",
    "img_processor.advanced_filtering_example()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"2Ô∏è‚É£ FITTING AND ALIGNMENT EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\\\nüìê Running line fitting...\")\n",
    "fitting_processor.basic_line_fitting_example()\n",
    "\n",
    "print(\"\\\\nüõ°Ô∏è Running RANSAC...\")\n",
    "fitting_processor.advanced_ransac_example()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"3Ô∏è‚É£ SEGMENTATION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\\\nüé® Running thresholding...\")\n",
    "segmentation_processor.basic_thresholding_example()\n",
    "\n",
    "print(\"\\\\nüéØ Running clustering...\")\n",
    "segmentation_processor.advanced_clustering_example()\n",
    "\n",
    "print(\"\\\\nüéâ ALL EXAMPLES COMPLETED SUCCESSFULLY!\")\n",
    "print(\"üéì You've mastered the fundamentals of Computer Vision!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
