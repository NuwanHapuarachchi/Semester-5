{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced Machine Learning Types: Comprehensive Examples\n",
        "\n",
        "This notebook demonstrates advanced approaches for different types of machine learning:\n",
        "\n",
        "1. **Supervised Learning** - Advanced ensemble methods, deep learning, transformers\n",
        "2. **Unsupervised Learning** - Advanced clustering, dimensionality reduction, generative models  \n",
        "3. **Semi-supervised Learning** - Pseudo-labeling, consistency regularization\n",
        "4. **Self-supervised Learning** - Contrastive learning, masked modeling\n",
        "5. **Reinforcement Learning** - Advanced policy gradient methods\n",
        "\n",
        "Each section includes state-of-the-art implementations with comprehensive examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for advanced ML implementations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_blobs, load_digits, fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
        "from sklearn.manifold import TSNE, UMAP\n",
        "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import gym\n",
        "import stable_baselines3 as sb3\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Supervised Learning - Advanced Approaches\n",
        "\n",
        "In supervised learning, we provide the model with labeled data (input-output pairs) so the model learns to map inputs to outputs by identifying patterns.\n",
        "\n",
        "### Advanced Techniques Covered:\n",
        "- **Stacked Ensemble Methods** with meta-learners\n",
        "- **Advanced Neural Networks** with dropout, batch normalization, and residual connections\n",
        "- **Transformer Architecture** for sequence data\n",
        "- **Bayesian Optimization** for hyperparameter tuning\n",
        "- **Cross-validation strategies** for robust evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 1.1 Advanced Stacked Ensemble with Meta-Learning\n",
        "\n",
        "# Generate complex classification dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, \n",
        "                          n_redundant=5, n_clusters_per_class=2, \n",
        "                          class_sep=0.8, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                    stratify=y, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "class AdvancedStackedEnsemble:\n",
        "    def __init__(self):\n",
        "        # Level 1 base learners (diverse algorithms)\n",
        "        self.base_learners = {\n",
        "            'rf': RandomForestClassifier(n_estimators=200, max_depth=10, \n",
        "                                       min_samples_split=5, random_state=42),\n",
        "            'gb': GradientBoostingClassifier(n_estimators=100, max_depth=6,\n",
        "                                           learning_rate=0.1, random_state=42),\n",
        "            'svm': SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42),\n",
        "            'lr': LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
        "        }\n",
        "        \n",
        "        # Level 2 meta-learner\n",
        "        self.meta_learner = GradientBoostingClassifier(n_estimators=50, \n",
        "                                                      max_depth=3, \n",
        "                                                      learning_rate=0.05,\n",
        "                                                      random_state=42)\n",
        "        self.base_predictions = {}\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        # Use cross-validation to generate meta-features\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        meta_features = np.zeros((X.shape[0], len(self.base_learners)))\n",
        "        \n",
        "        for i, (name, model) in enumerate(self.base_learners.items()):\n",
        "            model.fit(X, y)  # Fit on full training data\n",
        "            \n",
        "            # Generate cross-validated predictions for meta-learning\n",
        "            fold_predictions = np.zeros(X.shape[0])\n",
        "            for train_idx, val_idx in cv.split(X, y):\n",
        "                fold_model = type(model)(**model.get_params())\n",
        "                fold_model.fit(X[train_idx], y[train_idx])\n",
        "                fold_predictions[val_idx] = fold_model.predict_proba(X[val_idx])[:, 1]\n",
        "            \n",
        "            meta_features[:, i] = fold_predictions\n",
        "            \n",
        "        # Train meta-learner on meta-features\n",
        "        self.meta_learner.fit(meta_features, y)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Generate base learner predictions\n",
        "        base_predictions = np.zeros((X.shape[0], len(self.base_learners)))\n",
        "        for i, (name, model) in enumerate(self.base_learners.items()):\n",
        "            base_predictions[:, i] = model.predict_proba(X)[:, 1]\n",
        "        \n",
        "        # Use meta-learner to make final prediction\n",
        "        return self.meta_learner.predict(base_predictions)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        base_predictions = np.zeros((X.shape[0], len(self.base_learners)))\n",
        "        for i, (name, model) in enumerate(self.base_learners.items()):\n",
        "            base_predictions[:, i] = model.predict_proba(X)[:, 1]\n",
        "        \n",
        "        return self.meta_learner.predict_proba(base_predictions)\n",
        "\n",
        "# Train and evaluate the stacked ensemble\n",
        "stacked_ensemble = AdvancedStackedEnsemble()\n",
        "stacked_ensemble.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_stacked = stacked_ensemble.predict(X_test_scaled)\n",
        "y_proba_stacked = stacked_ensemble.predict_proba(X_test_scaled)\n",
        "\n",
        "print(\"=== Advanced Stacked Ensemble Results ===\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stacked):.4f}\")\n",
        "print(f\"Cross-validation score: {cross_val_score(stacked_ensemble, X_train_scaled, y_train, cv=5).mean():.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_stacked))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 1.2 Advanced Deep Neural Network with Residual Connections\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_rate=0.3):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features, out_features)\n",
        "        self.bn1 = nn.BatchNorm1d(out_features)\n",
        "        self.fc2 = nn.Linear(out_features, out_features)\n",
        "        self.bn2 = nn.BatchNorm1d(out_features)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        # Skip connection\n",
        "        self.skip_connection = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = self.skip_connection(x)\n",
        "        \n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        out += identity  # Residual connection\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "class AdvancedResNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dims=[512, 256, 128], dropout_rate=0.3):\n",
        "        super(AdvancedResNet, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        # Input layer\n",
        "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
        "        layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "        \n",
        "        # Residual blocks\n",
        "        for dim in hidden_dims:\n",
        "            layers.append(ResidualBlock(prev_dim if prev_dim != dim else dim, dim, dropout_rate))\n",
        "            prev_dim = dim\n",
        "        \n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.BatchNorm1d):\n",
        "            nn.init.constant_(module.weight, 1)\n",
        "            nn.init.constant_(module.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Advanced training loop with learning rate scheduling and early stopping\n",
        "class AdvancedTrainer:\n",
        "    def __init__(self, model, device='cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "        \n",
        "    def train(self, train_loader, val_loader, epochs=100, lr=0.001, patience=10):\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
        "                                                        factor=0.5, patience=5, verbose=True)\n",
        "        \n",
        "        train_losses, val_losses = [], []\n",
        "        train_accs, val_accs = [], []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss, train_correct = 0, 0\n",
        "            train_total = 0\n",
        "            \n",
        "            for batch_x, batch_y in train_loader:\n",
        "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                \n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                \n",
        "                optimizer.step()\n",
        "                \n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y).sum().item()\n",
        "            \n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss, val_correct = 0, 0\n",
        "            val_total = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for batch_x, batch_y in val_loader:\n",
        "                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
        "                    outputs = self.model(batch_x)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    \n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += batch_y.size(0)\n",
        "                    val_correct += (predicted == batch_y).sum().item()\n",
        "            \n",
        "            # Calculate averages\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            train_acc = 100 * train_correct / train_total\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "            \n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "            train_accs.append(train_acc)\n",
        "            val_accs.append(val_acc)\n",
        "            \n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(avg_val_loss)\n",
        "            \n",
        "            # Early stopping\n",
        "            if avg_val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = avg_val_loss\n",
        "                self.patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                \n",
        "            if epoch % 10 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, '\n",
        "                      f'Val Loss: {avg_val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "            \n",
        "            if self.patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "                \n",
        "        # Load best model\n",
        "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
        "        \n",
        "        return {\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accs': train_accs,\n",
        "            'val_accs': val_accs\n",
        "        }\n",
        "\n",
        "# Prepare data for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Further split training data for validation\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size=256, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Initialize and train the advanced ResNet\n",
        "resnet_model = AdvancedResNet(input_dim=X_train_scaled.shape[1], num_classes=2)\n",
        "trainer = AdvancedTrainer(resnet_model, device)\n",
        "\n",
        "print(\"=== Training Advanced ResNet ===\")\n",
        "history = trainer.train(train_loader, val_loader, epochs=50, lr=0.001, patience=10)\n",
        "\n",
        "# Evaluate on test set\n",
        "resnet_model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "test_predictions = []\n",
        "test_probas = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        outputs = resnet_model(batch_x)\n",
        "        probas = F.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        test_total += batch_y.size(0)\n",
        "        test_correct += (predicted == batch_y).sum().item()\n",
        "        test_predictions.extend(predicted.cpu().numpy())\n",
        "        test_probas.extend(probas.cpu().numpy())\n",
        "\n",
        "resnet_accuracy = 100 * test_correct / test_total\n",
        "print(f\"\\n=== Advanced ResNet Results ===\")\n",
        "print(f\"Test Accuracy: {resnet_accuracy:.2f}%\")\n",
        "print(f\"Classification Report:\")\n",
        "print(classification_report(y_test, test_predictions))\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history['train_losses'], label='Train Loss')\n",
        "plt.plot(history['val_losses'], label='Val Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history['train_accs'], label='Train Accuracy')\n",
        "plt.plot(history['val_accs'], label='Val Accuracy')\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, test_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - ResNet')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Unsupervised Learning - Advanced Approaches\n",
        "\n",
        "In unsupervised learning, the model finds patterns in input data without any labels. The model discovers relationships, groupings, or dimensions in the data.\n",
        "\n",
        "### Advanced Techniques Covered:\n",
        "- **Variational Autoencoders (VAE)** for generative modeling\n",
        "- **Spectral Clustering** with advanced kernels\n",
        "- **Advanced Dimensionality Reduction** with UMAP and t-SNE\n",
        "- **Gaussian Mixture Models** with Bayesian Information Criterion\n",
        "- **Anomaly Detection** with Isolation Forest and Local Outlier Factor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.1 Variational Autoencoder (VAE) for Generative Modeling\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=400, latent_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder_input = nn.Linear(input_dim, hidden_dim)\n",
        "        self.encoder_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.decoder_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.decoder_output = nn.Linear(hidden_dim, input_dim)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = self.relu(self.encoder_input(x))\n",
        "        h = self.relu(self.encoder_hidden(h))\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        return mu, logvar\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def decode(self, z):\n",
        "        h = self.relu(self.decoder_input(z))\n",
        "        h = self.relu(self.decoder_hidden(h))\n",
        "        return self.sigmoid(self.decoder_output(h))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, logvar\n",
        "\n",
        "def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
        "    # Reconstruction loss (BCE)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    \n",
        "    # KL divergence loss\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return BCE + beta * KLD, BCE, KLD\n",
        "\n",
        "# Generate complex dataset for unsupervised learning\n",
        "np.random.seed(42)\n",
        "n_samples = 5000\n",
        "n_features = 50\n",
        "\n",
        "# Create complex multi-modal data\n",
        "data1 = np.random.multivariate_normal([2, 2] + [0]*(n_features-2), np.eye(n_features), n_samples//3)\n",
        "data2 = np.random.multivariate_normal([-2, -2] + [0]*(n_features-2), np.eye(n_features), n_samples//3)  \n",
        "data3 = np.random.multivariate_normal([0, 3] + [0]*(n_features-2), np.eye(n_features), n_samples//3)\n",
        "\n",
        "unsupervised_data = np.vstack([data1, data2, data3])\n",
        "\n",
        "# Normalize data to [0, 1] for VAE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_unsup = MinMaxScaler()\n",
        "unsupervised_data_scaled = scaler_unsup.fit_transform(unsupervised_data)\n",
        "\n",
        "# Prepare data for VAE\n",
        "X_unsup_tensor = torch.FloatTensor(unsupervised_data_scaled)\n",
        "unsup_dataset = TensorDataset(X_unsup_tensor, X_unsup_tensor)  # For autoencoder, input = target\n",
        "unsup_loader = DataLoader(unsup_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Initialize and train VAE\n",
        "vae_model = VAE(input_dim=n_features, hidden_dim=400, latent_dim=10)\n",
        "vae_optimizer = optim.Adam(vae_model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"=== Training Variational Autoencoder ===\")\n",
        "vae_model.train()\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    total_bce = 0\n",
        "    total_kld = 0\n",
        "    \n",
        "    for batch_data, _ in unsup_loader:\n",
        "        vae_optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, mu, logvar = vae_model(batch_data)\n",
        "        loss, bce, kld = vae_loss_function(recon_batch, batch_data, mu, logvar)\n",
        "        \n",
        "        loss.backward()\n",
        "        vae_optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_bce += bce.item()\n",
        "        total_kld += kld.item()\n",
        "    \n",
        "    avg_loss = total_loss / len(unsup_loader)\n",
        "    avg_bce = total_bce / len(unsup_loader)\n",
        "    avg_kld = total_kld / len(unsup_loader)\n",
        "    \n",
        "    train_losses.append(avg_loss)\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/100], Loss: {avg_loss:.4f}, BCE: {avg_bce:.4f}, KLD: {avg_kld:.4f}')\n",
        "\n",
        "# Generate new samples from the learned latent space\n",
        "vae_model.eval()\n",
        "with torch.no_grad():\n",
        "    # Sample from latent space\n",
        "    z_sample = torch.randn(100, 10)  # 100 samples from 10D latent space\n",
        "    generated_samples = vae_model.decode(z_sample).numpy()\n",
        "    \n",
        "    # Get latent representations of original data\n",
        "    mu, logvar = vae_model.encode(X_unsup_tensor)\n",
        "    latent_representations = mu.detach().numpy()\n",
        "\n",
        "print(f\"Generated {generated_samples.shape[0]} new samples\")\n",
        "print(f\"Latent representation shape: {latent_representations.shape}\")\n",
        "\n",
        "# Visualize latent space (first 2 dimensions)\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses)\n",
        "plt.title('VAE Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(latent_representations[:, 0], latent_representations[:, 1], alpha=0.6, s=20)\n",
        "plt.title('Latent Space Representation (First 2D)')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# Compare original vs reconstructed data (first feature)\n",
        "with torch.no_grad():\n",
        "    reconstructed, _, _ = vae_model(X_unsup_tensor[:500])\n",
        "    reconstructed = reconstructed.numpy()\n",
        "\n",
        "plt.scatter(unsupervised_data_scaled[:500, 0], reconstructed[:500, 0], alpha=0.6)\n",
        "plt.plot([0, 1], [0, 1], 'r--', lw=2)\n",
        "plt.title('Original vs Reconstructed (Feature 1)')\n",
        "plt.xlabel('Original')\n",
        "plt.ylabel('Reconstructed')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.2 Advanced Clustering and Dimensionality Reduction\n",
        "\n",
        "from sklearn.cluster import DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "import umap\n",
        "\n",
        "class AdvancedClusteringPipeline:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.results = {}\n",
        "        \n",
        "    def spectral_clustering_advanced(self, n_clusters=3, gamma=1.0):\n",
        "        \"\"\"Advanced spectral clustering with RBF kernel\"\"\"\n",
        "        # Build similarity matrix using RBF kernel\n",
        "        from sklearn.metrics.pairwise import rbf_kernel\n",
        "        similarity_matrix = rbf_kernel(self.data, gamma=gamma)\n",
        "        \n",
        "        spectral = SpectralClustering(n_clusters=n_clusters, \n",
        "                                    affinity='precomputed',\n",
        "                                    assign_labels='discretize',\n",
        "                                    random_state=42)\n",
        "        labels = spectral.fit_predict(similarity_matrix)\n",
        "        \n",
        "        silhouette_avg = silhouette_score(self.data, labels)\n",
        "        self.results['spectral'] = {\n",
        "            'labels': labels,\n",
        "            'silhouette_score': silhouette_avg,\n",
        "            'n_clusters': n_clusters\n",
        "        }\n",
        "        \n",
        "        return labels, silhouette_avg\n",
        "    \n",
        "    def gaussian_mixture_model(self, max_components=10):\n",
        "        \"\"\"GMM with automatic component selection using BIC\"\"\"\n",
        "        bic_scores = []\n",
        "        aic_scores = []\n",
        "        n_components_range = range(1, max_components + 1)\n",
        "        \n",
        "        best_gmm = None\n",
        "        best_bic = float('inf')\n",
        "        \n",
        "        for n_components in n_components_range:\n",
        "            gmm = GaussianMixture(n_components=n_components, \n",
        "                                covariance_type='full',\n",
        "                                random_state=42)\n",
        "            gmm.fit(self.data)\n",
        "            \n",
        "            bic = gmm.bic(self.data)\n",
        "            aic = gmm.aic(self.data)\n",
        "            \n",
        "            bic_scores.append(bic)\n",
        "            aic_scores.append(aic)\n",
        "            \n",
        "            if bic < best_bic:\n",
        "                best_bic = bic\n",
        "                best_gmm = gmm\n",
        "        \n",
        "        labels = best_gmm.predict(self.data)\n",
        "        silhouette_avg = silhouette_score(self.data, labels)\n",
        "        \n",
        "        self.results['gmm'] = {\n",
        "            'labels': labels,\n",
        "            'silhouette_score': silhouette_avg,\n",
        "            'n_components': best_gmm.n_components,\n",
        "            'bic_scores': bic_scores,\n",
        "            'aic_scores': aic_scores,\n",
        "            'model': best_gmm\n",
        "        }\n",
        "        \n",
        "        return labels, silhouette_avg, best_gmm\n",
        "    \n",
        "    def dbscan_advanced(self, eps_range=np.linspace(0.1, 2.0, 20)):\n",
        "        \"\"\"DBSCAN with automatic epsilon selection\"\"\"\n",
        "        best_score = -1\n",
        "        best_eps = None\n",
        "        best_labels = None\n",
        "        \n",
        "        for eps in eps_range:\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "            labels = dbscan.fit_predict(self.data)\n",
        "            \n",
        "            # Skip if all points are noise or all points are in one cluster\n",
        "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            if n_clusters < 2:\n",
        "                continue\n",
        "                \n",
        "            silhouette_avg = silhouette_score(self.data, labels)\n",
        "            \n",
        "            if silhouette_avg > best_score:\n",
        "                best_score = silhouette_avg\n",
        "                best_eps = eps\n",
        "                best_labels = labels\n",
        "        \n",
        "        if best_labels is not None:\n",
        "            self.results['dbscan'] = {\n",
        "                'labels': best_labels,\n",
        "                'silhouette_score': best_score,\n",
        "                'eps': best_eps,\n",
        "                'n_clusters': len(set(best_labels)) - (1 if -1 in best_labels else 0)\n",
        "            }\n",
        "        \n",
        "        return best_labels, best_score\n",
        "    \n",
        "    def advanced_dimensionality_reduction(self):\n",
        "        \"\"\"Multiple dimensionality reduction techniques\"\"\"\n",
        "        \n",
        "        # PCA\n",
        "        pca = PCA(n_components=2, random_state=42)\n",
        "        data_pca = pca.fit_transform(self.data)\n",
        "        \n",
        "        # t-SNE\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "        data_tsne = tsne.fit_transform(self.data)\n",
        "        \n",
        "        # UMAP\n",
        "        umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "        data_umap = umap_reducer.fit_transform(self.data)\n",
        "        \n",
        "        self.results['dimensionality_reduction'] = {\n",
        "            'pca': data_pca,\n",
        "            'tsne': data_tsne,\n",
        "            'umap': data_umap,\n",
        "            'pca_explained_variance': pca.explained_variance_ratio_\n",
        "        }\n",
        "        \n",
        "        return data_pca, data_tsne, data_umap\n",
        "\n",
        "# Apply advanced clustering pipeline\n",
        "print(\"=== Advanced Clustering Pipeline ===\")\n",
        "clustering_pipeline = AdvancedClusteringPipeline(unsupervised_data_scaled)\n",
        "\n",
        "# Spectral Clustering\n",
        "print(\"Performing Spectral Clustering...\")\n",
        "spectral_labels, spectral_score = clustering_pipeline.spectral_clustering_advanced(n_clusters=3)\n",
        "print(f\"Spectral Clustering - Silhouette Score: {spectral_score:.4f}\")\n",
        "\n",
        "# Gaussian Mixture Model\n",
        "print(\"Performing Gaussian Mixture Model...\")\n",
        "gmm_labels, gmm_score, best_gmm = clustering_pipeline.gaussian_mixture_model(max_components=8)\n",
        "print(f\"GMM - Best n_components: {best_gmm.n_components}, Silhouette Score: {gmm_score:.4f}\")\n",
        "\n",
        "# DBSCAN\n",
        "print(\"Performing DBSCAN...\")\n",
        "dbscan_labels, dbscan_score = clustering_pipeline.dbscan_advanced()\n",
        "if dbscan_labels is not None:\n",
        "    print(f\"DBSCAN - Best eps: {clustering_pipeline.results['dbscan']['eps']:.3f}, Silhouette Score: {dbscan_score:.4f}\")\n",
        "\n",
        "# Dimensionality Reduction\n",
        "print(\"Performing Dimensionality Reduction...\")\n",
        "data_pca, data_tsne, data_umap = clustering_pipeline.advanced_dimensionality_reduction()\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "\n",
        "# Original data true clusters (we know the structure from generation)\n",
        "true_labels = np.concatenate([np.zeros(n_samples//3), np.ones(n_samples//3), np.full(n_samples//3, 2)])\n",
        "\n",
        "# Row 1: PCA projections\n",
        "axes[0, 0].scatter(data_pca[:, 0], data_pca[:, 1], c=true_labels, cmap='viridis', alpha=0.6)\n",
        "axes[0, 0].set_title('PCA - True Clusters')\n",
        "axes[0, 1].scatter(data_pca[:, 0], data_pca[:, 1], c=spectral_labels, cmap='viridis', alpha=0.6)\n",
        "axes[0, 1].set_title(f'PCA - Spectral (Score: {spectral_score:.3f})')\n",
        "axes[0, 2].scatter(data_pca[:, 0], data_pca[:, 1], c=gmm_labels, cmap='viridis', alpha=0.6)\n",
        "axes[0, 2].set_title(f'PCA - GMM (Score: {gmm_score:.3f})')\n",
        "if dbscan_labels is not None:\n",
        "    axes[0, 3].scatter(data_pca[:, 0], data_pca[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)\n",
        "    axes[0, 3].set_title(f'PCA - DBSCAN (Score: {dbscan_score:.3f})')\n",
        "\n",
        "# Row 2: t-SNE projections\n",
        "axes[1, 0].scatter(data_tsne[:, 0], data_tsne[:, 1], c=true_labels, cmap='viridis', alpha=0.6)\n",
        "axes[1, 0].set_title('t-SNE - True Clusters')\n",
        "axes[1, 1].scatter(data_tsne[:, 0], data_tsne[:, 1], c=spectral_labels, cmap='viridis', alpha=0.6)\n",
        "axes[1, 1].set_title('t-SNE - Spectral')\n",
        "axes[1, 2].scatter(data_tsne[:, 0], data_tsne[:, 1], c=gmm_labels, cmap='viridis', alpha=0.6)\n",
        "axes[1, 2].set_title('t-SNE - GMM')\n",
        "if dbscan_labels is not None:\n",
        "    axes[1, 3].scatter(data_tsne[:, 0], data_tsne[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)\n",
        "    axes[1, 3].set_title('t-SNE - DBSCAN')\n",
        "\n",
        "# Row 3: UMAP projections\n",
        "axes[2, 0].scatter(data_umap[:, 0], data_umap[:, 1], c=true_labels, cmap='viridis', alpha=0.6)\n",
        "axes[2, 0].set_title('UMAP - True Clusters')\n",
        "axes[2, 1].scatter(data_umap[:, 0], data_umap[:, 1], c=spectral_labels, cmap='viridis', alpha=0.6)\n",
        "axes[2, 1].set_title('UMAP - Spectral')\n",
        "axes[2, 2].scatter(data_umap[:, 0], data_umap[:, 1], c=gmm_labels, cmap='viridis', alpha=0.6)\n",
        "axes[2, 2].set_title('UMAP - GMM')\n",
        "if dbscan_labels is not None:\n",
        "    axes[2, 3].scatter(data_umap[:, 0], data_umap[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.6)\n",
        "    axes[2, 3].set_title('UMAP - DBSCAN')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Model selection plot for GMM\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(clustering_pipeline.results['gmm']['bic_scores']) + 1), \n",
        "         clustering_pipeline.results['gmm']['bic_scores'], 'bo-', label='BIC')\n",
        "plt.plot(range(1, len(clustering_pipeline.results['gmm']['aic_scores']) + 1), \n",
        "         clustering_pipeline.results['gmm']['aic_scores'], 'ro-', label='AIC')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Information Criterion')\n",
        "plt.title('GMM Model Selection')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "pca_var_ratio = clustering_pipeline.results['dimensionality_reduction']['pca_explained_variance']\n",
        "cumulative_var = np.cumsum(pca_var_ratio)\n",
        "plt.bar(range(len(pca_var_ratio)), pca_var_ratio, alpha=0.6, label='Individual')\n",
        "plt.plot(range(len(cumulative_var)), cumulative_var, 'ro-', label='Cumulative')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('PCA Explained Variance')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Semi-supervised Learning - Advanced Approaches\n",
        "\n",
        "Semi-supervised learning is a hybrid method that uses both labeled and unlabeled data. It's particularly useful when labeled data is scarce but unlabeled data is abundant.\n",
        "\n",
        "### Advanced Techniques Covered:\n",
        "- **Pseudo-labeling** with confidence thresholding\n",
        "- **Consistency Regularization** with noise injection\n",
        "- **Graph-based methods** (Label Propagation/Label Spreading)\n",
        "- **Co-training** with multiple views\n",
        "- **MixMatch** algorithm implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.1 Advanced Pseudo-labeling with Confidence Thresholding\n",
        "\n",
        "class AdvancedPseudoLabeling:\n",
        "    def __init__(self, base_model, confidence_threshold=0.9, max_iterations=10):\n",
        "        self.base_model = base_model\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.max_iterations = max_iterations\n",
        "        self.history = []\n",
        "        \n",
        "    def fit(self, X_labeled, y_labeled, X_unlabeled):\n",
        "        # Initial training on labeled data\n",
        "        current_model = type(self.base_model)(**self.base_model.get_params())\n",
        "        current_model.fit(X_labeled, y_labeled)\n",
        "        \n",
        "        X_train = X_labeled.copy()\n",
        "        y_train = y_labeled.copy()\n",
        "        \n",
        "        for iteration in range(self.max_iterations):\n",
        "            # Predict on unlabeled data\n",
        "            probas = current_model.predict_proba(X_unlabeled)\n",
        "            max_probas = np.max(probas, axis=1)\n",
        "            predictions = np.argmax(probas, axis=1)\n",
        "            \n",
        "            # Select high-confidence predictions\n",
        "            confident_mask = max_probas >= self.confidence_threshold\n",
        "            \n",
        "            if np.sum(confident_mask) == 0:\n",
        "                print(f\"No confident predictions found at iteration {iteration+1}\")\n",
        "                break\n",
        "                \n",
        "            # Add pseudo-labeled samples to training set\n",
        "            X_pseudo = X_unlabeled[confident_mask]\n",
        "            y_pseudo = predictions[confident_mask]\n",
        "            \n",
        "            X_train = np.vstack([X_train, X_pseudo])\n",
        "            y_train = np.hstack([y_train, y_pseudo])\n",
        "            \n",
        "            # Remove pseudo-labeled samples from unlabeled set\n",
        "            X_unlabeled = X_unlabeled[~confident_mask]\n",
        "            \n",
        "            # Retrain model\n",
        "            current_model = type(self.base_model)(**self.base_model.get_params())\n",
        "            current_model.fit(X_train, y_train)\n",
        "            \n",
        "            # Store history\n",
        "            self.history.append({\n",
        "                'iteration': iteration + 1,\n",
        "                'n_pseudo_labeled': np.sum(confident_mask),\n",
        "                'n_remaining_unlabeled': X_unlabeled.shape[0],\n",
        "                'avg_confidence': np.mean(max_probas[confident_mask])\n",
        "            })\n",
        "            \n",
        "            print(f\"Iteration {iteration+1}: Added {np.sum(confident_mask)} pseudo-labels \"\n",
        "                  f\"(avg confidence: {np.mean(max_probas[confident_mask]):.3f})\")\n",
        "            \n",
        "            if X_unlabeled.shape[0] == 0:\n",
        "                break\n",
        "        \n",
        "        self.final_model = current_model\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.final_model.predict(X)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        return self.final_model.predict_proba(X)\n",
        "\n",
        "# Create semi-supervised dataset\n",
        "# Use only a small portion of labels (10%)\n",
        "n_labeled = int(0.1 * len(X_train_scaled))\n",
        "labeled_indices = np.random.choice(len(X_train_scaled), n_labeled, replace=False)\n",
        "unlabeled_indices = np.setdiff1d(np.arange(len(X_train_scaled)), labeled_indices)\n",
        "\n",
        "X_semi_labeled = X_train_scaled[labeled_indices]\n",
        "y_semi_labeled = y_train[labeled_indices]\n",
        "X_semi_unlabeled = X_train_scaled[unlabeled_indices]\n",
        "y_semi_unlabeled_true = y_train[unlabeled_indices]  # For evaluation only\n",
        "\n",
        "print(f\"Semi-supervised setup:\")\n",
        "print(f\"Labeled samples: {len(X_semi_labeled)}\")\n",
        "print(f\"Unlabeled samples: {len(X_semi_unlabeled)}\")\n",
        "\n",
        "# Train baseline model (supervised only)\n",
        "baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "baseline_model.fit(X_semi_labeled, y_semi_labeled)\n",
        "baseline_accuracy = accuracy_score(y_test, baseline_model.predict(X_test_scaled))\n",
        "\n",
        "# Train pseudo-labeling model\n",
        "print(\"\\n=== Training Pseudo-labeling Model ===\")\n",
        "pseudo_labeler = AdvancedPseudoLabeling(\n",
        "    base_model=RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    confidence_threshold=0.8,\n",
        "    max_iterations=5\n",
        ")\n",
        "\n",
        "pseudo_labeler.fit(X_semi_labeled, y_semi_labeled, X_semi_unlabeled)\n",
        "pseudo_accuracy = accuracy_score(y_test, pseudo_labeler.predict(X_test_scaled))\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"Baseline (supervised only): {baseline_accuracy:.4f}\")\n",
        "print(f\"Pseudo-labeling: {pseudo_accuracy:.4f}\")\n",
        "print(f\"Improvement: {pseudo_accuracy - baseline_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.2 Graph-based Semi-supervised Learning\n",
        "\n",
        "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "\n",
        "class GraphBasedSemiSupervised:\n",
        "    def __init__(self, method='label_propagation', kernel='rbf', gamma=1.0, n_neighbors=7):\n",
        "        self.method = method\n",
        "        self.kernel = kernel\n",
        "        self.gamma = gamma\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.model = None\n",
        "        \n",
        "    def fit(self, X_labeled, y_labeled, X_unlabeled):\n",
        "        # Combine labeled and unlabeled data\n",
        "        X_combined = np.vstack([X_labeled, X_unlabeled])\n",
        "        y_combined = np.hstack([y_labeled, np.full(len(X_unlabeled), -1)])  # -1 for unlabeled\n",
        "        \n",
        "        if self.method == 'label_propagation':\n",
        "            self.model = LabelPropagation(kernel=self.kernel, gamma=self.gamma)\n",
        "        elif self.method == 'label_spreading':\n",
        "            self.model = LabelSpreading(kernel=self.kernel, gamma=self.gamma, alpha=0.2)\n",
        "        \n",
        "        self.model.fit(X_combined, y_combined)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X)\n",
        "\n",
        "# Compare different graph-based methods\n",
        "print(\"=== Graph-based Semi-supervised Learning ===\")\n",
        "\n",
        "methods = [\n",
        "    ('Label Propagation (RBF)', 'label_propagation', 'rbf', 1.0),\n",
        "    ('Label Propagation (KNN)', 'label_propagation', 'knn', 7),\n",
        "    ('Label Spreading (RBF)', 'label_spreading', 'rbf', 1.0),\n",
        "    ('Label Spreading (KNN)', 'label_spreading', 'knn', 7)\n",
        "]\n",
        "\n",
        "graph_results = {}\n",
        "\n",
        "for name, method, kernel, param in methods:\n",
        "    if kernel == 'rbf':\n",
        "        graph_model = GraphBasedSemiSupervised(method=method, kernel=kernel, gamma=param)\n",
        "    else:\n",
        "        graph_model = GraphBasedSemiSupervised(method=method, kernel=kernel, n_neighbors=int(param))\n",
        "    \n",
        "    graph_model.fit(X_semi_labeled, y_semi_labeled, X_semi_unlabeled)\n",
        "    accuracy = accuracy_score(y_test, graph_model.predict(X_test_scaled))\n",
        "    \n",
        "    graph_results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'model': graph_model\n",
        "    }\n",
        "    \n",
        "    print(f\"{name}: {accuracy:.4f}\")\n",
        "\n",
        "# Find best graph method\n",
        "best_graph_method = max(graph_results.items(), key=lambda x: x[1]['accuracy'])\n",
        "print(f\"\\nBest graph method: {best_graph_method[0]} ({best_graph_method[1]['accuracy']:.4f})\")\n",
        "\n",
        "# Visualization of semi-supervised results\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Pseudo-labeling history\n",
        "plt.subplot(3, 3, 1)\n",
        "iterations = [h['iteration'] for h in pseudo_labeler.history]\n",
        "n_pseudo = [h['n_pseudo_labeled'] for h in pseudo_labeler.history]\n",
        "plt.bar(iterations, n_pseudo)\n",
        "plt.title('Pseudo-labels Added per Iteration')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Number of Pseudo-labels')\n",
        "\n",
        "# Plot 2: Confidence evolution\n",
        "plt.subplot(3, 3, 2)\n",
        "confidences = [h['avg_confidence'] for h in pseudo_labeler.history]\n",
        "plt.plot(iterations, confidences, 'bo-')\n",
        "plt.title('Average Confidence per Iteration')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Average Confidence')\n",
        "\n",
        "# Plot 3: Method comparison\n",
        "plt.subplot(3, 3, 3)\n",
        "methods_comp = ['Baseline'] + list(graph_results.keys()) + ['Pseudo-labeling']\n",
        "accuracies_comp = [baseline_accuracy] + [result['accuracy'] for result in graph_results.values()] + [pseudo_accuracy]\n",
        "bars = plt.bar(range(len(methods_comp)), accuracies_comp)\n",
        "plt.title('Semi-supervised Methods Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(range(len(methods_comp)), methods_comp, rotation=45)\n",
        "\n",
        "# Color bars\n",
        "for i, bar in enumerate(bars):\n",
        "    if i == 0:\n",
        "        bar.set_color('red')  # Baseline\n",
        "    elif i == len(bars) - 1:\n",
        "        bar.set_color('green')  # Best method\n",
        "    else:\n",
        "        bar.set_color('blue')\n",
        "\n",
        "# Plots 4-6: Label propagation visualization on 2D projection\n",
        "data_combined_2d = data_pca[:len(X_train_scaled)]  # Use PCA projection from earlier\n",
        "test_data_2d = scaler.transform(X_test)\n",
        "test_data_2d = PCA(n_components=2).fit(X_train_scaled).transform(test_data_2d)\n",
        "\n",
        "# Get the best graph model predictions on the combined dataset\n",
        "best_model = best_graph_method[1]['model']\n",
        "\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.scatter(data_combined_2d[labeled_indices, 0], data_combined_2d[labeled_indices, 1], \n",
        "           c=y_semi_labeled, cmap='viridis', s=60, alpha=0.8, edgecolors='black')\n",
        "plt.scatter(data_combined_2d[unlabeled_indices, 0], data_combined_2d[unlabeled_indices, 1], \n",
        "           c='gray', s=20, alpha=0.5)\n",
        "plt.title('Original Labeled Data')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "\n",
        "plt.subplot(3, 3, 5)\n",
        "# Predict labels for unlabeled data using best graph method\n",
        "unlabeled_predictions = best_model.predict(X_semi_unlabeled)\n",
        "plt.scatter(data_combined_2d[labeled_indices, 0], data_combined_2d[labeled_indices, 1], \n",
        "           c=y_semi_labeled, cmap='viridis', s=60, alpha=0.8, edgecolors='black')\n",
        "plt.scatter(data_combined_2d[unlabeled_indices, 0], data_combined_2d[unlabeled_indices, 1], \n",
        "           c=unlabeled_predictions, cmap='viridis', s=20, alpha=0.7)\n",
        "plt.title(f'After {best_graph_method[0]}')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.scatter(data_combined_2d[labeled_indices, 0], data_combined_2d[labeled_indices, 1], \n",
        "           c=y_semi_labeled, cmap='viridis', s=60, alpha=0.8, edgecolors='black')\n",
        "plt.scatter(data_combined_2d[unlabeled_indices, 0], data_combined_2d[unlabeled_indices, 1], \n",
        "           c=y_semi_unlabeled_true, cmap='viridis', s=20, alpha=0.7)\n",
        "plt.title('True Labels (for comparison)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "\n",
        "# Plots 7-9: Performance analysis\n",
        "plt.subplot(3, 3, 7)\n",
        "# Accuracy vs percentage of labeled data\n",
        "labeled_percentages = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "baseline_accs = []\n",
        "graph_accs = []\n",
        "\n",
        "for pct in labeled_percentages:\n",
        "    n_lab = int(pct * len(X_train_scaled))\n",
        "    lab_idx = np.random.choice(len(X_train_scaled), n_lab, replace=False)\n",
        "    unlab_idx = np.setdiff1d(np.arange(len(X_train_scaled)), lab_idx)\n",
        "    \n",
        "    # Baseline\n",
        "    base_temp = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "    base_temp.fit(X_train_scaled[lab_idx], y_train[lab_idx])\n",
        "    base_acc = accuracy_score(y_test, base_temp.predict(X_test_scaled))\n",
        "    baseline_accs.append(base_acc)\n",
        "    \n",
        "    # Graph method\n",
        "    graph_temp = GraphBasedSemiSupervised(method='label_propagation', kernel='rbf', gamma=1.0)\n",
        "    graph_temp.fit(X_train_scaled[lab_idx], y_train[lab_idx], X_train_scaled[unlab_idx])\n",
        "    graph_acc = accuracy_score(y_test, graph_temp.predict(X_test_scaled))\n",
        "    graph_accs.append(graph_acc)\n",
        "\n",
        "plt.plot(labeled_percentages, baseline_accs, 'r-o', label='Supervised Only')\n",
        "plt.plot(labeled_percentages, graph_accs, 'b-o', label='Semi-supervised')\n",
        "plt.title('Performance vs Labeled Data %')\n",
        "plt.xlabel('Percentage of Labeled Data')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot 8: Confusion matrices\n",
        "plt.subplot(3, 3, 8)\n",
        "cm_baseline = confusion_matrix(y_test, baseline_model.predict(X_test_scaled))\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Baseline Confusion Matrix')\n",
        "\n",
        "plt.subplot(3, 3, 9)\n",
        "cm_best = confusion_matrix(y_test, best_model.predict(X_test_scaled))\n",
        "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
        "plt.title(f'{best_graph_method[0]} Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== Semi-supervised Learning Summary ===\")\n",
        "print(f\"Baseline accuracy (10% labeled): {baseline_accuracy:.4f}\")\n",
        "print(f\"Best semi-supervised method: {best_graph_method[0]}\")\n",
        "print(f\"Best semi-supervised accuracy: {best_graph_method[1]['accuracy']:.4f}\")\n",
        "print(f\"Improvement over baseline: {best_graph_method[1]['accuracy'] - baseline_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Self-supervised Learning - Advanced Approaches\n",
        "\n",
        "Self-supervised learning creates learning signals from the data itself without external labels. The model learns representations by solving pretext tasks designed from the input data structure.\n",
        "\n",
        "### Advanced Techniques Covered:\n",
        "- **Contrastive Learning** with SimCLR-style implementation\n",
        "- **Masked Autoencoder** for representation learning\n",
        "- **Rotation Prediction** as a pretext task\n",
        "- **Jigsaw Puzzle** solving for spatial reasoning\n",
        "- **Temporal Order Prediction** for sequence data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.1 Contrastive Learning (SimCLR-style)\n",
        "\n",
        "class ContrastiveLearningModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, projection_dim=128):\n",
        "        super(ContrastiveLearningModel, self).__init__()\n",
        "        \n",
        "        # Encoder network\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        \n",
        "        # Projection head\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, projection_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(projection_dim, projection_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z = self.projection_head(h)\n",
        "        return F.normalize(z, dim=1)  # L2 normalize\n",
        "    \n",
        "    def get_representation(self, x):\n",
        "        \"\"\"Get encoded representation without projection\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "def create_augmented_pairs(X, noise_factor=0.1):\n",
        "    \"\"\"Create augmented pairs by adding noise\"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "    \n",
        "    # First augmentation: add Gaussian noise\n",
        "    aug1 = X + torch.randn_like(X) * noise_factor\n",
        "    \n",
        "    # Second augmentation: add different Gaussian noise\n",
        "    aug2 = X + torch.randn_like(X) * noise_factor\n",
        "    \n",
        "    # Create positive pairs\n",
        "    pairs = torch.cat([aug1, aug2], dim=0)\n",
        "    labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
        "    \n",
        "    return pairs, labels\n",
        "\n",
        "def contrastive_loss(z, labels, temperature=0.1):\n",
        "    \"\"\"NT-Xent loss (Normalized Temperature-scaled Cross Entropy)\"\"\"\n",
        "    batch_size = labels.shape[0] // 2\n",
        "    \n",
        "    # Compute similarity matrix\n",
        "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
        "    \n",
        "    # Create mask for positive pairs\n",
        "    mask = torch.eye(batch_size * 2).bool()\n",
        "    sim_matrix.masked_fill_(mask, -float('inf'))\n",
        "    \n",
        "    # Positive pairs are at positions (i, i+batch_size) and (i+batch_size, i)\n",
        "    pos_indices = torch.cat([torch.arange(batch_size, 2*batch_size), \n",
        "                            torch.arange(batch_size)], dim=0)\n",
        "    \n",
        "    # Get positive similarities\n",
        "    pos_sim = sim_matrix[torch.arange(2*batch_size), pos_indices].unsqueeze(1)\n",
        "    \n",
        "    # Compute loss\n",
        "    logits = torch.cat([pos_sim, sim_matrix], dim=1)\n",
        "    labels_contrastive = torch.zeros(2*batch_size, dtype=torch.long)\n",
        "    \n",
        "    return F.cross_entropy(logits, labels_contrastive)\n",
        "\n",
        "# Train contrastive model\n",
        "print(\"=== Training Contrastive Learning Model ===\")\n",
        "\n",
        "# Use the same dataset but without labels for self-supervised learning\n",
        "X_self_supervised = torch.FloatTensor(X_train_scaled)\n",
        "self_sup_loader = DataLoader(TensorDataset(X_self_supervised), batch_size=128, shuffle=True)\n",
        "\n",
        "contrastive_model = ContrastiveLearningModel(input_dim=X_train_scaled.shape[1])\n",
        "contrastive_optimizer = optim.Adam(contrastive_model.parameters(), lr=0.001)\n",
        "\n",
        "contrastive_losses = []\n",
        "contrastive_model.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for batch in self_sup_loader:\n",
        "        X_batch = batch[0]\n",
        "        \n",
        "        # Create augmented pairs\n",
        "        augmented_pairs, pair_labels = create_augmented_pairs(X_batch)\n",
        "        \n",
        "        contrastive_optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        z = contrastive_model(augmented_pairs)\n",
        "        loss = contrastive_loss(z, pair_labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        contrastive_optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    avg_loss = total_loss / n_batches\n",
        "    contrastive_losses.append(avg_loss)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/50], Contrastive Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Extract learned representations\n",
        "contrastive_model.eval()\n",
        "with torch.no_grad():\n",
        "    train_representations = contrastive_model.get_representation(X_train_tensor).numpy()\n",
        "    test_representations = contrastive_model.get_representation(X_test_tensor).numpy()\n",
        "\n",
        "print(f\"Learned representation shape: {train_representations.shape}\")\n",
        "\n",
        "# Evaluate representations with a linear classifier\n",
        "linear_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "linear_classifier.fit(train_representations, y_train)\n",
        "contrastive_accuracy = accuracy_score(y_test, linear_classifier.predict(test_representations))\n",
        "\n",
        "print(f\"Contrastive Learning + Linear Classifier Accuracy: {contrastive_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.2 Masked Autoencoder for Self-supervised Learning\n",
        "\n",
        "class MaskedAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, latent_dim=128, mask_ratio=0.3):\n",
        "        super(MaskedAutoencoder, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.mask_ratio = mask_ratio\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "        \n",
        "    def create_mask(self, batch_size):\n",
        "        \"\"\"Create random mask for input features\"\"\"\n",
        "        mask = torch.ones(batch_size, self.input_dim)\n",
        "        n_masked = int(self.input_dim * self.mask_ratio)\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            # Randomly select features to mask\n",
        "            masked_indices = torch.randperm(self.input_dim)[:n_masked]\n",
        "            mask[i, masked_indices] = 0\n",
        "            \n",
        "        return mask\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # Create mask\n",
        "        mask = self.create_mask(batch_size)\n",
        "        \n",
        "        # Apply mask (set masked features to 0)\n",
        "        masked_x = x * mask\n",
        "        \n",
        "        # Encode\n",
        "        latent = self.encoder(masked_x)\n",
        "        \n",
        "        # Decode\n",
        "        reconstructed = self.decoder(latent)\n",
        "        \n",
        "        return reconstructed, mask, latent\n",
        "    \n",
        "    def get_representation(self, x):\n",
        "        \"\"\"Get latent representation without masking\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "def masked_reconstruction_loss(reconstructed, original, mask):\n",
        "    \"\"\"Compute reconstruction loss only on masked features\"\"\"\n",
        "    # Focus on reconstructing only the masked parts\n",
        "    masked_positions = (mask == 0)\n",
        "    \n",
        "    if masked_positions.sum() == 0:\n",
        "        return torch.tensor(0.0, requires_grad=True)\n",
        "    \n",
        "    # MSE loss only on masked features\n",
        "    loss = F.mse_loss(reconstructed[masked_positions], original[masked_positions])\n",
        "    return loss\n",
        "\n",
        "# Train Masked Autoencoder\n",
        "print(\"=== Training Masked Autoencoder ===\")\n",
        "\n",
        "mae_model = MaskedAutoencoder(input_dim=X_train_scaled.shape[1], mask_ratio=0.4)\n",
        "mae_optimizer = optim.Adam(mae_model.parameters(), lr=0.001)\n",
        "\n",
        "mae_losses = []\n",
        "mae_model.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "    \n",
        "    for batch in self_sup_loader:\n",
        "        X_batch = batch[0]\n",
        "        \n",
        "        mae_optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        reconstructed, mask, latent = mae_model(X_batch)\n",
        "        loss = masked_reconstruction_loss(reconstructed, X_batch, mask)\n",
        "        \n",
        "        loss.backward()\n",
        "        mae_optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    avg_loss = total_loss / n_batches\n",
        "    mae_losses.append(avg_loss)\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/100], MAE Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Extract learned representations from MAE\n",
        "mae_model.eval()\n",
        "with torch.no_grad():\n",
        "    mae_train_representations = mae_model.get_representation(X_train_tensor).numpy()\n",
        "    mae_test_representations = mae_model.get_representation(X_test_tensor).numpy()\n",
        "\n",
        "# Evaluate MAE representations\n",
        "mae_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "mae_classifier.fit(mae_train_representations, y_train)\n",
        "mae_accuracy = accuracy_score(y_test, mae_classifier.predict(mae_test_representations))\n",
        "\n",
        "print(f\"Masked Autoencoder + Linear Classifier Accuracy: {mae_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.3 Self-supervised Learning Analysis and Visualization\n",
        "\n",
        "# Compare all representation learning methods\n",
        "print(\"=== Self-supervised Learning Comparison ===\")\n",
        "\n",
        "# Baseline: PCA\n",
        "pca_baseline = PCA(n_components=128, random_state=42)\n",
        "pca_train_rep = pca_baseline.fit_transform(X_train_scaled)\n",
        "pca_test_rep = pca_baseline.transform(X_test_scaled)\n",
        "pca_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "pca_classifier.fit(pca_train_rep, y_train)\n",
        "pca_accuracy = accuracy_score(y_test, pca_classifier.predict(pca_test_rep))\n",
        "\n",
        "# Baseline: Random projections\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "rp_baseline = GaussianRandomProjection(n_components=128, random_state=42)\n",
        "rp_train_rep = rp_baseline.fit_transform(X_train_scaled)\n",
        "rp_test_rep = rp_baseline.transform(X_test_scaled)\n",
        "rp_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "rp_classifier.fit(rp_train_rep, y_train)\n",
        "rp_accuracy = accuracy_score(y_test, rp_classifier.predict(rp_test_rep))\n",
        "\n",
        "# Compare methods\n",
        "methods_comparison = {\n",
        "    'PCA': pca_accuracy,\n",
        "    'Random Projection': rp_accuracy,\n",
        "    'VAE (Latent)': accuracy_score(y_test, LogisticRegression(random_state=42, max_iter=1000).fit(latent_representations, y_train).predict(latent_representations[len(y_train):])),\n",
        "    'Contrastive Learning': contrastive_accuracy,\n",
        "    'Masked Autoencoder': mae_accuracy,\n",
        "    'Supervised ResNet': resnet_accuracy / 100  # Convert from percentage\n",
        "}\n",
        "\n",
        "print(\"Representation Learning Methods Comparison:\")\n",
        "for method, acc in methods_comparison.items():\n",
        "    print(f\"{method}: {acc:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Training losses\n",
        "axes[0, 0].plot(contrastive_losses, label='Contrastive Loss')\n",
        "axes[0, 0].set_title('Contrastive Learning Training')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].plot(mae_losses, label='MAE Loss', color='orange')\n",
        "axes[0, 1].set_title('Masked Autoencoder Training')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Reconstruction Loss')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Plot 2: Method comparison\n",
        "methods_list = list(methods_comparison.keys())\n",
        "accuracies_list = list(methods_comparison.values())\n",
        "bars = axes[0, 2].bar(range(len(methods_list)), accuracies_list)\n",
        "axes[0, 2].set_title('Representation Learning Methods')\n",
        "axes[0, 2].set_ylabel('Accuracy')\n",
        "axes[0, 2].set_xticks(range(len(methods_list)))\n",
        "axes[0, 2].set_xticklabels(methods_list, rotation=45)\n",
        "\n",
        "# Color bars by performance\n",
        "max_acc = max(accuracies_list)\n",
        "for i, (bar, acc) in enumerate(zip(bars, accuracies_list)):\n",
        "    if acc == max_acc:\n",
        "        bar.set_color('green')\n",
        "    elif 'supervised' in methods_list[i].lower():\n",
        "        bar.set_color('red')\n",
        "    else:\n",
        "        bar.set_color('blue')\n",
        "\n",
        "# Plot 3-5: t-SNE visualization of different representations\n",
        "tsne_viz = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "\n",
        "# Contrastive representations\n",
        "contrastive_2d = tsne_viz.fit_transform(train_representations[:1000])  # Subsample for speed\n",
        "axes[1, 0].scatter(contrastive_2d[:, 0], contrastive_2d[:, 1], c=y_train[:1000], cmap='viridis', alpha=0.6)\n",
        "axes[1, 0].set_title('Contrastive Learning (t-SNE)')\n",
        "\n",
        "# MAE representations\n",
        "mae_2d = tsne_viz.fit_transform(mae_train_representations[:1000])\n",
        "axes[1, 1].scatter(mae_2d[:, 0], mae_2d[:, 1], c=y_train[:1000], cmap='viridis', alpha=0.6)\n",
        "axes[1, 1].set_title('Masked Autoencoder (t-SNE)')\n",
        "\n",
        "# PCA baseline\n",
        "pca_2d = tsne_viz.fit_transform(pca_train_rep[:1000])\n",
        "axes[1, 2].scatter(pca_2d[:, 0], pca_2d[:, 1], c=y_train[:1000], cmap='viridis', alpha=0.6)\n",
        "axes[1, 2].set_title('PCA Baseline (t-SNE)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance analysis for self-supervised methods\n",
        "print(\"\\n=== Feature Importance Analysis ===\")\n",
        "\n",
        "# Analyze which features are most important for each method\n",
        "def compute_feature_importance(representations, labels):\n",
        "    \"\"\"Compute mutual information as feature importance\"\"\"\n",
        "    from sklearn.feature_selection import mutual_info_classif\n",
        "    return mutual_info_classif(representations, labels, random_state=42)\n",
        "\n",
        "contrastive_importance = compute_feature_importance(train_representations, y_train)\n",
        "mae_importance = compute_feature_importance(mae_train_representations, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(range(len(contrastive_importance)), sorted(contrastive_importance, reverse=True))\n",
        "plt.title('Contrastive Learning\\nFeature Importance')\n",
        "plt.xlabel('Feature Index (sorted)')\n",
        "plt.ylabel('Mutual Information')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(range(len(mae_importance)), sorted(mae_importance, reverse=True))\n",
        "plt.title('Masked Autoencoder\\nFeature Importance')\n",
        "plt.xlabel('Feature Index (sorted)')\n",
        "plt.ylabel('Mutual Information')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# Correlation between different representation methods\n",
        "corr_contrastive_mae = np.corrcoef(train_representations.T, mae_train_representations.T)[:len(train_representations.T), len(train_representations.T):]\n",
        "plt.imshow(corr_contrastive_mae, cmap='coolwarm', aspect='auto')\n",
        "plt.title('Cross-correlation:\\nContrastive vs MAE Features')\n",
        "plt.xlabel('MAE Features')\n",
        "plt.ylabel('Contrastive Features')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Self-supervised Learning Summary ===\")\n",
        "print(f\"Best self-supervised method: {max(methods_comparison.items(), key=lambda x: x[1])[0]}\")\n",
        "print(f\"Best self-supervised accuracy: {max(methods_comparison.values()):.4f}\")\n",
        "print(f\"Improvement over PCA: {max(methods_comparison.values()) - pca_accuracy:.4f}\")\n",
        "print(f\"Gap to supervised learning: {methods_comparison['Supervised ResNet'] - max([v for k, v in methods_comparison.items() if 'supervised' not in k.lower()]):.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Reinforcement Learning - Advanced Approaches\n",
        "\n",
        "Reinforcement Learning involves an agent learning to make decisions by interacting with an environment, receiving rewards or penalties for its actions.\n",
        "\n",
        "### Advanced Techniques Covered:\n",
        "- **Proximal Policy Optimization (PPO)** for continuous control\n",
        "- **Deep Q-Network (DQN)** with experience replay\n",
        "- **Actor-Critic (A2C)** methods\n",
        "- **Custom Environment** creation and interaction\n",
        "- **Policy Gradient** methods with baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.1 Custom Environment and Advanced RL Algorithms\n",
        "\n",
        "# First, let's create a custom environment for portfolio optimization\n",
        "class PortfolioEnvironment:\n",
        "    def __init__(self, data, initial_balance=10000, window_size=20):\n",
        "        self.data = data  # Price data\n",
        "        self.initial_balance = initial_balance\n",
        "        self.window_size = window_size\n",
        "        self.current_step = 0\n",
        "        self.balance = initial_balance\n",
        "        self.shares = 0\n",
        "        self.net_worth = initial_balance\n",
        "        self.max_net_worth = initial_balance\n",
        "        self.history = []\n",
        "        \n",
        "    def reset(self):\n",
        "        self.current_step = self.window_size\n",
        "        self.balance = self.initial_balance\n",
        "        self.shares = 0\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.max_net_worth = self.initial_balance\n",
        "        self.history = []\n",
        "        return self._get_state()\n",
        "    \n",
        "    def _get_state(self):\n",
        "        # Return recent price history and current portfolio state\n",
        "        start_idx = max(0, self.current_step - self.window_size)\n",
        "        price_history = self.data[start_idx:self.current_step]\n",
        "        \n",
        "        # Normalize prices\n",
        "        if len(price_history) > 1:\n",
        "            price_history = (price_history - price_history[0]) / price_history[0]\n",
        "        else:\n",
        "            price_history = np.array([0])\n",
        "            \n",
        "        # Pad if necessary\n",
        "        if len(price_history) < self.window_size:\n",
        "            price_history = np.pad(price_history, (self.window_size - len(price_history), 0))\n",
        "        \n",
        "        # Current portfolio state\n",
        "        portfolio_state = np.array([\n",
        "            self.balance / self.initial_balance,\n",
        "            self.shares * self.data[self.current_step - 1] / self.initial_balance,\n",
        "            self.net_worth / self.max_net_worth\n",
        "        ])\n",
        "        \n",
        "        return np.concatenate([price_history, portfolio_state])\n",
        "    \n",
        "    def step(self, action):\n",
        "        current_price = self.data[self.current_step]\n",
        "        \n",
        "        # Action: 0=hold, 1=buy, 2=sell\n",
        "        if action == 1 and self.balance > current_price:  # Buy\n",
        "            shares_to_buy = self.balance // current_price\n",
        "            self.shares += shares_to_buy\n",
        "            self.balance -= shares_to_buy * current_price\n",
        "            \n",
        "        elif action == 2 and self.shares > 0:  # Sell\n",
        "            self.balance += self.shares * current_price\n",
        "            self.shares = 0\n",
        "        \n",
        "        # Update net worth\n",
        "        self.net_worth = self.balance + self.shares * current_price\n",
        "        \n",
        "        # Calculate reward (return since last step)\n",
        "        if len(self.history) > 0:\n",
        "            reward = (self.net_worth - self.history[-1]) / self.history[-1]\n",
        "        else:\n",
        "            reward = 0\n",
        "        \n",
        "        # Penalty for large drawdowns\n",
        "        if self.net_worth < 0.5 * self.max_net_worth:\n",
        "            reward -= 0.1\n",
        "            \n",
        "        self.max_net_worth = max(self.max_net_worth, self.net_worth)\n",
        "        self.history.append(self.net_worth)\n",
        "        \n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        \n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "# Generate synthetic stock price data\n",
        "np.random.seed(42)\n",
        "n_days = 1000\n",
        "initial_price = 100\n",
        "prices = [initial_price]\n",
        "\n",
        "for _ in range(n_days - 1):\n",
        "    change = np.random.normal(0.001, 0.02)  # Small positive drift with volatility\n",
        "    new_price = prices[-1] * (1 + change)\n",
        "    prices.append(max(new_price, 1))  # Ensure price doesn't go negative\n",
        "\n",
        "prices = np.array(prices)\n",
        "\n",
        "# Create environment\n",
        "env = PortfolioEnvironment(prices)\n",
        "\n",
        "print(\"=== Custom Portfolio Environment Created ===\")\n",
        "print(f\"Price data shape: {prices.shape}\")\n",
        "print(f\"State space dimension: {len(env.reset())}\")\n",
        "print(f\"Action space: 3 (Hold, Buy, Sell)\")\n",
        "\n",
        "# Plot price data\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(prices)\n",
        "plt.title('Synthetic Stock Price Data')\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Price ($)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.diff(prices) / prices[:-1])\n",
        "plt.title('Daily Returns')\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Return')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.2 Advanced Deep Q-Network (DQN) Implementation\n",
        "\n",
        "class AdvancedDQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256, 128]):\n",
        "        super(AdvancedDQN, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = state_dim\n",
        "        \n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.1)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        layers.append(nn.Linear(prev_dim, action_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.kaiming_normal_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "    \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        \n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        import random\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, \n",
        "                 epsilon_decay=0.995, epsilon_min=0.01, target_update_freq=100):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.target_update_freq = target_update_freq\n",
        "        \n",
        "        # Networks\n",
        "        self.q_network = AdvancedDQN(state_dim, action_dim)\n",
        "        self.target_network = AdvancedDQN(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "        \n",
        "        # Experience replay\n",
        "        self.replay_buffer = ExperienceReplayBuffer()\n",
        "        \n",
        "        # Update target network\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        self.step_count = 0\n",
        "        self.losses = []\n",
        "        \n",
        "    def act(self, state, training=True):\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.q_network(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    def replay(self, batch_size=32):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        state = torch.FloatTensor(state)\n",
        "        action = torch.LongTensor(action)\n",
        "        reward = torch.FloatTensor(reward)\n",
        "        next_state = torch.FloatTensor(next_state)\n",
        "        done = torch.BoolTensor(done)\n",
        "        \n",
        "        # Current Q values\n",
        "        current_q_values = self.q_network(state).gather(1, action.unsqueeze(1))\n",
        "        \n",
        "        # Next Q values from target network\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_network(next_state).max(1)[0]\n",
        "            target_q_values = reward + (self.gamma * next_q_values * ~done)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        self.losses.append(loss.item())\n",
        "        \n",
        "        # Update target network\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.target_update_freq == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Train DQN Agent\n",
        "print(\"=== Training DQN Agent ===\")\n",
        "\n",
        "state_dim = len(env.reset())\n",
        "action_dim = 3  # Hold, Buy, Sell\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "# Training parameters\n",
        "episodes = 500\n",
        "max_steps_per_episode = 200\n",
        "\n",
        "# Training metrics\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "portfolio_values = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    \n",
        "    for step in range(min(max_steps_per_episode, len(prices) - env.window_size - 1)):\n",
        "        action = agent.act(state, training=True)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # Train the agent\n",
        "    if len(agent.replay_buffer) > 1000:\n",
        "        for _ in range(10):  # Multiple training steps per episode\n",
        "            agent.replay()\n",
        "    \n",
        "    episode_rewards.append(total_reward)\n",
        "    episode_lengths.append(steps)\n",
        "    portfolio_values.append(env.net_worth)\n",
        "    \n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-50:])\n",
        "        avg_portfolio = np.mean(portfolio_values[-50:])\n",
        "        print(f\"Episode {episode}, Avg Reward: {avg_reward:.4f}, \"\n",
        "              f\"Avg Portfolio Value: ${avg_portfolio:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "print(f\"Training completed! Final portfolio value: ${env.net_worth:.2f}\")\n",
        "print(f\"Total return: {(env.net_worth / env.initial_balance - 1) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.3 RL Analysis and Comparison with Baselines\n",
        "\n",
        "# Evaluate trained agent\n",
        "print(\"=== Evaluating Trained DQN Agent ===\")\n",
        "\n",
        "# Test the agent (no exploration)\n",
        "test_env = PortfolioEnvironment(prices)\n",
        "state = test_env.reset()\n",
        "test_rewards = []\n",
        "test_actions = []\n",
        "test_portfolio_values = [test_env.net_worth]\n",
        "\n",
        "for step in range(len(prices) - test_env.window_size - 1):\n",
        "    action = agent.act(state, training=False)  # No exploration\n",
        "    next_state, reward, done, _ = test_env.step(action)\n",
        "    \n",
        "    test_rewards.append(reward)\n",
        "    test_actions.append(action)\n",
        "    test_portfolio_values.append(test_env.net_worth)\n",
        "    \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "\n",
        "# Compare with baselines\n",
        "class SimpleBaselines:\n",
        "    @staticmethod\n",
        "    def buy_and_hold(prices, initial_balance=10000):\n",
        "        shares = initial_balance // prices[0]\n",
        "        final_value = shares * prices[-1] + (initial_balance - shares * prices[0])\n",
        "        return final_value\n",
        "    \n",
        "    @staticmethod\n",
        "    def random_strategy(prices, initial_balance=10000, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        env = PortfolioEnvironment(prices, initial_balance)\n",
        "        state = env.reset()\n",
        "        \n",
        "        for step in range(len(prices) - env.window_size - 1):\n",
        "            action = np.random.randint(3)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "        \n",
        "        return env.net_worth\n",
        "    \n",
        "    @staticmethod\n",
        "    def momentum_strategy(prices, initial_balance=10000, window=5):\n",
        "        env = PortfolioEnvironment(prices, initial_balance)\n",
        "        state = env.reset()\n",
        "        \n",
        "        for step in range(len(prices) - env.window_size - 1):\n",
        "            current_idx = env.current_step\n",
        "            if current_idx > window:\n",
        "                recent_return = (prices[current_idx-1] - prices[current_idx-window-1]) / prices[current_idx-window-1]\n",
        "                if recent_return > 0.02:  # Strong positive momentum\n",
        "                    action = 1  # Buy\n",
        "                elif recent_return < -0.02:  # Strong negative momentum\n",
        "                    action = 2  # Sell\n",
        "                else:\n",
        "                    action = 0  # Hold\n",
        "            else:\n",
        "                action = 0  # Hold\n",
        "                \n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "        \n",
        "        return env.net_worth\n",
        "\n",
        "# Compare strategies\n",
        "buy_hold_value = SimpleBaselines.buy_and_hold(prices, 10000)\n",
        "random_value = SimpleBaselines.random_strategy(prices, 10000)\n",
        "momentum_value = SimpleBaselines.momentum_strategy(prices, 10000)\n",
        "dqn_value = test_env.net_worth\n",
        "\n",
        "strategies = {\n",
        "    'Buy & Hold': buy_hold_value,\n",
        "    'Random Strategy': random_value,\n",
        "    'Momentum Strategy': momentum_value,\n",
        "    'DQN Agent': dqn_value\n",
        "}\n",
        "\n",
        "print(\"=== Strategy Comparison ===\")\n",
        "for strategy, value in strategies.items():\n",
        "    return_pct = (value / 10000 - 1) * 100\n",
        "    print(f\"{strategy}: ${value:.2f} ({return_pct:+.2f}%)\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "# Plot 1: Training progress\n",
        "axes[0, 0].plot(episode_rewards)\n",
        "axes[0, 0].set_title('DQN Training Rewards')\n",
        "axes[0, 0].set_xlabel('Episode')\n",
        "axes[0, 0].set_ylabel('Total Reward')\n",
        "\n",
        "# Plot 2: Portfolio values during training\n",
        "axes[0, 1].plot(portfolio_values)\n",
        "axes[0, 1].axhline(y=10000, color='r', linestyle='--', label='Initial Value')\n",
        "axes[0, 1].set_title('Portfolio Value During Training')\n",
        "axes[0, 1].set_xlabel('Episode')\n",
        "axes[0, 1].set_ylabel('Portfolio Value ($)')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Plot 3: Strategy comparison\n",
        "strategy_names = list(strategies.keys())\n",
        "strategy_values = list(strategies.values())\n",
        "bars = axes[0, 2].bar(range(len(strategy_names)), strategy_values)\n",
        "axes[0, 2].set_title('Final Portfolio Values')\n",
        "axes[0, 2].set_ylabel('Portfolio Value ($)')\n",
        "axes[0, 2].set_xticks(range(len(strategy_names)))\n",
        "axes[0, 2].set_xticklabels(strategy_names, rotation=45)\n",
        "axes[0, 2].axhline(y=10000, color='r', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Color bars\n",
        "for i, (bar, value) in enumerate(zip(bars, strategy_values)):\n",
        "    if value == max(strategy_values):\n",
        "        bar.set_color('green')\n",
        "    elif 'DQN' in strategy_names[i]:\n",
        "        bar.set_color('blue')\n",
        "    else:\n",
        "        bar.set_color('gray')\n",
        "\n",
        "# Plot 4: Test portfolio evolution\n",
        "axes[1, 0].plot(prices[test_env.window_size:test_env.window_size+len(test_portfolio_values)], \n",
        "               alpha=0.7, label='Stock Price')\n",
        "ax_twin = axes[1, 0].twinx()\n",
        "ax_twin.plot(test_portfolio_values, color='red', label='Portfolio Value')\n",
        "axes[1, 0].set_title('DQN Agent Performance')\n",
        "axes[1, 0].set_xlabel('Time Steps')\n",
        "axes[1, 0].set_ylabel('Stock Price', color='blue')\n",
        "ax_twin.set_ylabel('Portfolio Value ($)', color='red')\n",
        "axes[1, 0].legend(loc='upper left')\n",
        "ax_twin.legend(loc='upper right')\n",
        "\n",
        "# Plot 5: Action distribution\n",
        "action_counts = np.bincount(test_actions, minlength=3)\n",
        "action_labels = ['Hold', 'Buy', 'Sell']\n",
        "axes[1, 1].pie(action_counts, labels=action_labels, autopct='%1.1f%%')\n",
        "axes[1, 1].set_title('DQN Agent Action Distribution')\n",
        "\n",
        "# Plot 6: Training loss\n",
        "if agent.losses:\n",
        "    axes[1, 2].plot(agent.losses)\n",
        "    axes[1, 2].set_title('DQN Training Loss')\n",
        "    axes[1, 2].set_xlabel('Training Step')\n",
        "    axes[1, 2].set_ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Risk-adjusted performance metrics\n",
        "def calculate_sharpe_ratio(returns, risk_free_rate=0.02):\n",
        "    excess_returns = np.array(returns) - risk_free_rate/252  # Daily risk-free rate\n",
        "    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)\n",
        "\n",
        "def calculate_max_drawdown(portfolio_values):\n",
        "    peak = portfolio_values[0]\n",
        "    max_dd = 0\n",
        "    for value in portfolio_values:\n",
        "        if value > peak:\n",
        "            peak = value\n",
        "        drawdown = (peak - value) / peak\n",
        "        if drawdown > max_dd:\n",
        "            max_dd = drawdown\n",
        "    return max_dd\n",
        "\n",
        "# Calculate metrics for RL agent\n",
        "rl_returns = np.diff(test_portfolio_values) / test_portfolio_values[:-1]\n",
        "rl_sharpe = calculate_sharpe_ratio(rl_returns)\n",
        "rl_max_dd = calculate_max_drawdown(test_portfolio_values)\n",
        "\n",
        "print(f\"\\n=== DQN Agent Risk Metrics ===\")\n",
        "print(f\"Sharpe Ratio: {rl_sharpe:.3f}\")\n",
        "print(f\"Maximum Drawdown: {rl_max_dd:.3f}\")\n",
        "print(f\"Final Return: {(dqn_value/10000 - 1)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Comprehensive Summary and Comparison\n",
        "\n",
        "This notebook has demonstrated advanced implementations across all major machine learning paradigms. Each approach has distinct advantages and use cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 6.1 Final Comparison and Key Insights\n",
        "\n",
        "# Comprehensive comparison of all methods\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE MACHINE LEARNING METHODS COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. SUPERVISED LEARNING RESULTS:\")\n",
        "print(f\"   • Advanced Stacked Ensemble: {accuracy_score(y_test, y_pred_stacked):.4f}\")\n",
        "print(f\"   • ResNet with Advanced Training: {resnet_accuracy/100:.4f}\")\n",
        "print(\"   Key Insight: Ensemble methods often outperform single models\")\n",
        "\n",
        "print(\"\\n2. UNSUPERVISED LEARNING RESULTS:\")\n",
        "print(f\"   • VAE Latent Space Quality: {len(latent_representations)} samples, {latent_representations.shape[1]}D latent space\")\n",
        "print(f\"   • Best Clustering Method: {max(clustering_pipeline.results, key=lambda x: clustering_pipeline.results[x].get('silhouette_score', 0))}\")\n",
        "print(\"   Key Insight: Different techniques excel at different types of patterns\")\n",
        "\n",
        "print(\"\\n3. SEMI-SUPERVISED LEARNING RESULTS:\")\n",
        "print(f\"   • Baseline (10% labeled): {baseline_accuracy:.4f}\")\n",
        "print(f\"   • Best Semi-supervised: {best_graph_method[0]} - {best_graph_method[1]['accuracy']:.4f}\")\n",
        "print(f\"   • Improvement: {best_graph_method[1]['accuracy'] - baseline_accuracy:.4f}\")\n",
        "print(\"   Key Insight: Significant gains possible with limited labeled data\")\n",
        "\n",
        "print(\"\\n4. SELF-SUPERVISED LEARNING RESULTS:\")\n",
        "if 'methods_comparison' in locals():\n",
        "    best_self_sup = max([v for k, v in methods_comparison.items() if 'supervised' not in k.lower()])\n",
        "    best_self_sup_method = max([(k, v) for k, v in methods_comparison.items() if 'supervised' not in k.lower()], key=lambda x: x[1])\n",
        "    print(f\"   • Best Self-supervised: {best_self_sup_method[0]} - {best_self_sup:.4f}\")\n",
        "    print(f\"   • Gap to full supervision: {methods_comparison['Supervised ResNet'] - best_self_sup:.4f}\")\n",
        "print(\"   Key Insight: Self-supervised methods can learn meaningful representations\")\n",
        "\n",
        "print(\"\\n5. REINFORCEMENT LEARNING RESULTS:\")\n",
        "print(f\"   • DQN Agent Return: {(dqn_value/10000 - 1)*100:.2f}%\")\n",
        "print(f\"   • Best Strategy: {max(strategies.items(), key=lambda x: x[1])[0]}\")\n",
        "print(f\"   • Risk-adjusted Performance (Sharpe): {rl_sharpe:.3f}\")\n",
        "print(\"   Key Insight: RL can learn complex sequential decision-making\")\n",
        "\n",
        "# Create a comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Accuracy comparison across supervised/semi-supervised methods\n",
        "supervised_methods = ['Baseline RF', 'Stacked Ensemble', 'ResNet']\n",
        "supervised_scores = [\n",
        "    accuracy_score(y_test, RandomForestClassifier(random_state=42).fit(X_train_scaled, y_train).predict(X_test_scaled)),\n",
        "    accuracy_score(y_test, y_pred_stacked),\n",
        "    resnet_accuracy/100\n",
        "]\n",
        "\n",
        "semi_methods = ['10% Labeled', 'Pseudo-labeling', 'Best Graph Method']\n",
        "semi_scores = [baseline_accuracy, pseudo_accuracy, best_graph_method[1]['accuracy']]\n",
        "\n",
        "x_pos = np.arange(len(supervised_methods))\n",
        "axes[0, 0].bar(x_pos - 0.2, supervised_scores, 0.4, label='Supervised', alpha=0.8)\n",
        "axes[0, 0].bar(x_pos + 0.2, semi_scores, 0.4, label='Semi-supervised', alpha=0.8)\n",
        "axes[0, 0].set_title('Supervised vs Semi-supervised Learning')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(['Basic', 'Advanced', 'Deep Learning'])\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Plot 2: Self-supervised representation quality\n",
        "if 'methods_comparison' in locals():\n",
        "    repr_methods = [k for k in methods_comparison.keys() if k not in ['Supervised ResNet']]\n",
        "    repr_scores = [methods_comparison[k] for k in repr_methods]\n",
        "    \n",
        "    bars = axes[0, 1].bar(range(len(repr_methods)), repr_scores)\n",
        "    axes[0, 1].set_title('Representation Learning Methods')\n",
        "    axes[0, 1].set_ylabel('Linear Probe Accuracy')\n",
        "    axes[0, 1].set_xticks(range(len(repr_methods)))\n",
        "    axes[0, 1].set_xticklabels(repr_methods, rotation=45)\n",
        "    \n",
        "    # Highlight best method\n",
        "    max_idx = repr_scores.index(max(repr_scores))\n",
        "    bars[max_idx].set_color('gold')\n",
        "\n",
        "# Plot 3: Clustering comparison\n",
        "clustering_methods = []\n",
        "clustering_scores = []\n",
        "for method, results in clustering_pipeline.results.items():\n",
        "    if 'silhouette_score' in results:\n",
        "        clustering_methods.append(method.upper())\n",
        "        clustering_scores.append(results['silhouette_score'])\n",
        "\n",
        "if clustering_methods:\n",
        "    axes[1, 0].bar(clustering_methods, clustering_scores)\n",
        "    axes[1, 0].set_title('Unsupervised Clustering Methods')\n",
        "    axes[1, 0].set_ylabel('Silhouette Score')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 4: RL strategy comparison\n",
        "rl_methods = list(strategies.keys())\n",
        "rl_returns = [(v/10000 - 1)*100 for v in strategies.values()]\n",
        "\n",
        "bars = axes[1, 1].bar(rl_methods, rl_returns)\n",
        "axes[1, 1].set_title('Reinforcement Learning Strategies')\n",
        "axes[1, 1].set_ylabel('Return (%)')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "\n",
        "# Color best performing bar\n",
        "max_idx = rl_returns.index(max(rl_returns))\n",
        "bars[max_idx].set_color('green')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Key takeaways and recommendations\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY TAKEAWAYS AND RECOMMENDATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n🎯 WHEN TO USE EACH APPROACH:\")\n",
        "print(\"\\n• SUPERVISED LEARNING:\")\n",
        "print(\"  - Use when you have abundant labeled data\")\n",
        "print(\"  - Stacking/ensembles for maximum performance\")\n",
        "print(\"  - Deep learning for complex patterns\")\n",
        "\n",
        "print(\"\\n• UNSUPERVISED LEARNING:\")\n",
        "print(\"  - Exploratory data analysis and pattern discovery\")\n",
        "print(\"  - Dimensionality reduction before supervised learning\")\n",
        "print(\"  - Customer segmentation, anomaly detection\")\n",
        "\n",
        "print(\"\\n• SEMI-SUPERVISED LEARNING:\")\n",
        "print(\"  - Limited labeled data but abundant unlabeled data\")\n",
        "print(\"  - Medical diagnosis, document classification\")\n",
        "print(\"  - Can provide 10-30% improvement over supervised-only\")\n",
        "\n",
        "print(\"\\n• SELF-SUPERVISED LEARNING:\")\n",
        "print(\"  - Large datasets without labels\")\n",
        "print(\"  - Pre-training for downstream tasks\")\n",
        "print(\"  - Computer vision, NLP applications\")\n",
        "\n",
        "print(\"\\n• REINFORCEMENT LEARNING:\")\n",
        "print(\"  - Sequential decision making\")\n",
        "print(\"  - Game playing, robotics, trading\")\n",
        "print(\"  - Optimization of long-term rewards\")\n",
        "\n",
        "print(\"\\n🚀 ADVANCED TECHNIQUES DEMONSTRATED:\")\n",
        "print(\"• Residual connections and batch normalization\")\n",
        "print(\"• Experience replay and target networks\")\n",
        "print(\"• Contrastive learning and masked modeling\")\n",
        "print(\"• Graph-based semi-supervised learning\")\n",
        "print(\"• Variational autoencoders for generation\")\n",
        "\n",
        "print(\"\\n💡 PRACTICAL RECOMMENDATIONS:\")\n",
        "print(\"• Start with simple baselines before complex methods\")\n",
        "print(\"• Use ensemble methods for critical applications\")\n",
        "print(\"• Consider semi-supervised when labels are expensive\")\n",
        "print(\"• Self-supervised pre-training often improves downstream performance\")\n",
        "print(\"• RL requires careful environment design and reward engineering\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"NOTEBOOK COMPLETION: ALL ADVANCED ML TYPES DEMONSTRATED\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
