{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b597ac",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 1. Introduction to Machine Learning\n",
    "\n",
    "Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every scenario.\n",
    "\n",
    "## 1.1 Types of Machine Learning\n",
    "\n",
    "### 1.1.1 Supervised Learning\n",
    "- **Definition**: Learning with labeled training data (input-output pairs)\n",
    "- **Goal**: Predict outputs for new, unseen inputs\n",
    "- **Examples**: \n",
    "  - **Regression**: Predicting house prices, stock values, temperature\n",
    "  - **Classification**: Email spam detection, image recognition, medical diagnosis\n",
    "\n",
    "### 1.1.2 Unsupervised Learning\n",
    "- **Definition**: Finding patterns in data without labeled examples\n",
    "- **Goal**: Discover hidden structures in data\n",
    "- **Examples**: Customer segmentation, anomaly detection, dimensionality reduction\n",
    "\n",
    "### 1.1.3 Reinforcement Learning\n",
    "- **Definition**: Learning through interaction with an environment using rewards/penalties\n",
    "- **Goal**: Maximize cumulative reward over time\n",
    "- **Examples**: Game playing (Chess, Go), autonomous vehicles, robotics\n",
    "\n",
    "## 1.2 Key Concepts and Terminology\n",
    "\n",
    "### Core Components\n",
    "- **Features (X)**: Input variables or attributes used to make predictions\n",
    "- **Labels/Targets (y)**: Output variables we want to predict\n",
    "- **Model**: Mathematical function that maps features to predictions\n",
    "- **Training**: Process of learning model parameters from data\n",
    "- **Prediction**: Using trained model to estimate outputs for new inputs\n",
    "\n",
    "### Data Terminology\n",
    "- **Dataset**: Collection of data points used for training/testing\n",
    "- **Training Set**: Data used to train the model\n",
    "- **Test Set**: Data used to evaluate model performance\n",
    "- **Validation Set**: Data used for model selection and hyperparameter tuning\n",
    "\n",
    "### Model Performance\n",
    "- **Overfitting**: Model performs well on training data but poorly on new data\n",
    "- **Underfitting**: Model is too simple to capture underlying patterns\n",
    "- **Generalization**: Model's ability to perform well on unseen data\n",
    "\n",
    "## 1.3 Why Linear Regression?\n",
    "\n",
    "Linear regression is the foundation of machine learning because:\n",
    "\n",
    "1. **Simplicity**: Easy to understand and interpret\n",
    "2. **Mathematical Foundation**: Introduces key concepts like loss functions and optimization\n",
    "3. **Baseline Model**: Often used as a starting point for more complex models\n",
    "4. **Real-world Applications**: Widely used in economics, engineering, and data science\n",
    "5. **Gateway to Advanced Topics**: Concepts extend to neural networks and other algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21855fbb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üß† Self-Test Questions\n",
    "\n",
    "1. What type of machine learning would you use to predict the price of a used car based on its features?\n",
    "2. What's the difference between overfitting and underfitting?\n",
    "3. Why might you split your data into training and test sets?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answers</summary>\n",
    "\n",
    "1. **Supervised learning (regression)** - because we have labeled data (car features ‚Üí price) and want to predict a continuous value.\n",
    "\n",
    "2. **Overfitting**: Model memorizes training data but fails on new data (too complex). **Underfitting**: Model is too simple to capture the underlying pattern (too simple).\n",
    "\n",
    "3. **Data splitting** allows us to evaluate how well our model generalizes to unseen data, which is the true measure of model performance.\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7081f51",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 2. Python Libraries for Machine Learning\n",
    "\n",
    "Python's rich ecosystem of libraries makes it the preferred language for machine learning. Let's explore the essential libraries and their roles.\n",
    "\n",
    "## 2.1 NumPy - Numerical Computing Foundation\n",
    "\n",
    "**Purpose**: Efficient numerical operations and multi-dimensional arrays\n",
    "\n",
    "**Key Features**:\n",
    "- N-dimensional arrays (ndarray)\n",
    "- Vectorized operations (broadcasting)\n",
    "- Linear algebra functions\n",
    "- Random number generation\n",
    "\n",
    "**Why Important for ML**: All ML libraries are built on NumPy arrays for performance and memory efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d987e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy Demonstration\n",
    "import numpy as np\n",
    "\n",
    "# Creating arrays\n",
    "print(\"1. Array Creation:\")\n",
    "arr_1d = np.array([1, 2, 3, 4, 5])\n",
    "arr_2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"1D array: {arr_1d}\")\n",
    "print(f\"2D array:\\n{arr_2d}\")\n",
    "\n",
    "# Vectorized operations (much faster than Python loops)\n",
    "print(\"\\n2. Vectorized Operations:\")\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "print(f\"x + y = {x + y}\")\n",
    "print(f\"x * y = {x * y}\")\n",
    "print(f\"x^2 = {x**2}\")\n",
    "\n",
    "# Matrix operations (crucial for ML)\n",
    "print(\"\\n3. Matrix Operations:\")\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "print(f\"Matrix multiplication A @ B:\\n{A @ B}\")\n",
    "print(f\"Matrix transpose A.T:\\n{A.T}\")\n",
    "\n",
    "# Broadcasting (automatic array shape expansion)\n",
    "print(\"\\n4. Broadcasting:\")\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "vector = np.array([10, 20, 30])\n",
    "result = matrix + vector  # vector is broadcast to match matrix shape\n",
    "print(f\"Matrix + Vector (broadcasted):\\n{result}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\n5. Performance Comparison:\")\n",
    "import time\n",
    "\n",
    "# Python list approach (slow)\n",
    "python_list = list(range(1000000))\n",
    "start_time = time.time()\n",
    "result_python = [x**2 for x in python_list]\n",
    "python_time = time.time() - start_time\n",
    "\n",
    "# NumPy approach (fast)\n",
    "numpy_array = np.arange(1000000)\n",
    "start_time = time.time()\n",
    "result_numpy = numpy_array**2\n",
    "numpy_time = time.time() - start_time\n",
    "\n",
    "print(f\"Python list time: {python_time:.4f} seconds\")\n",
    "print(f\"NumPy array time: {numpy_time:.4f} seconds\")\n",
    "print(f\"NumPy is {python_time/numpy_time:.1f}x faster!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2.2 Pandas - Data Manipulation and Analysis\n",
    "\n",
    "**Purpose**: Data structures and tools for data manipulation and analysis\n",
    "\n",
    "**Key Features**:\n",
    "- DataFrame and Series data structures\n",
    "- Data loading/saving (CSV, Excel, JSON, databases)\n",
    "- Data cleaning and preprocessing\n",
    "- Grouping and aggregation operations\n",
    "\n",
    "**Why Important for ML**: Real-world data is messy; Pandas helps clean and prepare it for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Demonstration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating DataFrames\n",
    "print(\"1. Creating DataFrames:\")\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'Salary': [50000, 60000, 70000, 55000],\n",
    "    'Department': ['Engineering', 'Marketing', 'Engineering', 'Sales']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(\"\\n2. Dataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Data selection and filtering\n",
    "print(\"\\n3. Data Selection:\")\n",
    "print(\"Ages:\", df['Age'].tolist())\n",
    "print(\"Engineering employees:\")\n",
    "print(df[df['Department'] == 'Engineering'])\n",
    "\n",
    "# Statistical operations\n",
    "print(\"\\n4. Statistical Operations:\")\n",
    "print(f\"Mean salary: ${df['Salary'].mean():,.2f}\")\n",
    "print(f\"Age statistics:\\n{df['Age'].describe()}\")\n",
    "\n",
    "# Grouping operations\n",
    "print(\"\\n5. Grouping Operations:\")\n",
    "dept_stats = df.groupby('Department')['Salary'].agg(['mean', 'count'])\n",
    "print(\"Salary by Department:\")\n",
    "print(dept_stats)\n",
    "\n",
    "# Handling missing data\n",
    "print(\"\\n6. Handling Missing Data:\")\n",
    "df_with_missing = df.copy()\n",
    "df_with_missing.loc[1, 'Age'] = np.nan\n",
    "df_with_missing.loc[2, 'Salary'] = np.nan\n",
    "print(\"Data with missing values:\")\n",
    "print(df_with_missing)\n",
    "print(\"\\nAfter filling missing values:\")\n",
    "df_filled = df_with_missing.fillna(df_with_missing.mean())\n",
    "print(df_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2.3 Matplotlib - Data Visualization\n",
    "\n",
    "**Purpose**: Creating static, animated, and interactive visualizations\n",
    "\n",
    "**Key Features**:\n",
    "- Line plots, scatter plots, histograms, bar charts\n",
    "- Customizable styling and formatting\n",
    "- Subplots and multi-figure layouts\n",
    "- Mathematical notation support (LaTeX)\n",
    "\n",
    "**Why Important for ML**: Visualization helps understand data patterns, model performance, and communicate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f6f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib Demonstration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create sample data\n",
    "x = np.linspace(0, 10, 100)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "y3 = x**2 / 50\n",
    "\n",
    "# Create subplots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 1. Line plot\n",
    "ax1.plot(x, y1, 'b-', label='sin(x)', linewidth=2)\n",
    "ax1.plot(x, y2, 'r--', label='cos(x)', linewidth=2)\n",
    "ax1.set_title('Line Plot Example')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Scatter plot with noise\n",
    "np.random.seed(42)\n",
    "x_scatter = np.random.randn(100)\n",
    "y_scatter = 2 * x_scatter + np.random.randn(100) * 0.5\n",
    "ax2.scatter(x_scatter, y_scatter, alpha=0.6, c='green')\n",
    "ax2.set_title('Scatter Plot (Linear Relationship)')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "\n",
    "# 3. Histogram\n",
    "data = np.random.normal(0, 1, 1000)\n",
    "ax3.hist(data, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "ax3.set_title('Histogram (Normal Distribution)')\n",
    "ax3.set_xlabel('Value')\n",
    "ax3.set_ylabel('Frequency')\n",
    "\n",
    "# 4. Bar chart\n",
    "categories = ['A', 'B', 'C', 'D', 'E']\n",
    "values = [23, 45, 56, 78, 32]\n",
    "ax4.bar(categories, values, color=['red', 'blue', 'green', 'orange', 'purple'])\n",
    "ax4.set_title('Bar Chart Example')\n",
    "ax4.set_xlabel('Categories')\n",
    "ax4.set_ylabel('Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Advanced example: Mathematical functions with LaTeX\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y1 = x**2\n",
    "y2 = x**3\n",
    "y3 = np.exp(x)\n",
    "\n",
    "ax.plot(x, y1, label=r'$f(x) = x^2$', linewidth=2)\n",
    "ax.plot(x, y2, label=r'$f(x) = x^3$', linewidth=2)\n",
    "ax.plot(x, y3, label=r'$f(x) = e^x$', linewidth=2)\n",
    "\n",
    "ax.set_title(r'Mathematical Functions with $\\LaTeX$ Notation', fontsize=14)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-5, 10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2.4 Scikit-learn - Machine Learning Library\n",
    "\n",
    "**Purpose**: Simple and efficient tools for predictive data analysis\n",
    "\n",
    "**Key Features**:\n",
    "- Wide range of algorithms (regression, classification, clustering)\n",
    "- Consistent API across all algorithms\n",
    "- Built-in cross-validation and model selection\n",
    "- Data preprocessing utilities\n",
    "\n",
    "**Why Important for ML**: Industry-standard library with production-ready implementations of ML algorithms.\n",
    "\n",
    "## 2.5 Optional: TensorFlow/PyTorch\n",
    "\n",
    "**Purpose**: Deep learning and neural networks\n",
    "\n",
    "**When to Use**: For complex problems requiring neural networks (image recognition, NLP, etc.)\n",
    "\n",
    "**Note**: We'll focus on Scikit-learn for this tutorial, but these libraries extend the same concepts to deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Self-Test Questions\n",
    "\n",
    "1. Why is NumPy faster than Python lists for numerical operations?\n",
    "2. What's the main advantage of Pandas over working with raw Python data structures?\n",
    "3. When would you use Matplotlib in a machine learning workflow?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answers</summary>\n",
    "\n",
    "1. **NumPy speed**: Uses optimized C code, vectorized operations, and efficient memory layout (contiguous arrays vs. Python's scattered objects).\n",
    "\n",
    "2. **Pandas advantages**: Built-in data cleaning tools, powerful grouping/aggregation, handles missing data gracefully, and integrates well with other ML libraries.\n",
    "\n",
    "3. **Matplotlib in ML**: Exploratory data analysis, visualizing model performance, plotting learning curves, and communicating results to stakeholders.\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d568a4f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 3. Mathematical Foundation of Linear Regression\n",
    "\n",
    "Linear regression is fundamentally about finding the best line through data points. Let's build the mathematical foundation step by step.\n",
    "\n",
    "## 3.1 The Linear Model\n",
    "\n",
    "### Simple Linear Regression (One Feature)\n",
    "\n",
    "The simplest form relates one input feature to one output:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $y$: target variable (what we predict)\n",
    "- $x$: input feature  \n",
    "- $\\beta_0$: intercept (y-axis crossing point)\n",
    "- $\\beta_1$: slope (change in y per unit change in x)\n",
    "- $\\epsilon$: error term (noise)\n",
    "\n",
    "### Multiple Linear Regression (Multiple Features)\n",
    "\n",
    "For multiple features, we extend to:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$$\n",
    "\n",
    "### Vector Form (Matrix Notation)\n",
    "\n",
    "More compactly, using vectors and matrices:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{y}$: vector of target values $(n \\times 1)$\n",
    "- $\\mathbf{X}$: design matrix of features $(n \\times p)$\n",
    "- $\\boldsymbol{\\beta}$: parameter vector $(p \\times 1)$\n",
    "- $\\boldsymbol{\\epsilon}$: error vector $(n \\times 1)$\n",
    "\n",
    "## 3.2 The Prediction Function\n",
    "\n",
    "Our model makes predictions using:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{X}\\boldsymbol{\\beta}$$\n",
    "\n",
    "The \"hat\" notation $\\hat{y}$ indicates predicted values (as opposed to actual values $y$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c61664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Linear Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "x = np.linspace(0, 10, n_samples)\n",
    "true_slope = 2.5\n",
    "true_intercept = 1.0\n",
    "noise = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# True relationship with noise\n",
    "y_true = true_intercept + true_slope * x\n",
    "y_observed = y_true + noise\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Perfect linear relationship\n",
    "ax1.plot(x, y_true, 'r-', linewidth=3, label=f'True: y = {true_intercept} + {true_slope}x')\n",
    "ax1.set_title('Perfect Linear Relationship\\n(No Noise)', fontsize=14)\n",
    "ax1.set_xlabel('x (Feature)', fontsize=12)\n",
    "ax1.set_ylabel('y (Target)', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Real-world data with noise\n",
    "ax2.scatter(x, y_observed, alpha=0.6, color='blue', s=50, label='Observed Data')\n",
    "ax2.plot(x, y_true, 'r-', linewidth=2, label='True Relationship')\n",
    "ax2.set_title('Real-World Data\\n(With Noise)', fontsize=14)\n",
    "ax2.set_xlabel('x (Feature)', fontsize=12)\n",
    "ax2.set_ylabel('y (Target)', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Vector representation example\n",
    "print(\"Vector Representation Example:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Small example with 3 data points and 2 features\n",
    "X = np.array([[1, 2, 3],      # Feature 1\n",
    "              [1, 1, 1]])     # Bias term (intercept)\n",
    "X = X.T  # Transpose to get proper shape (3x2)\n",
    "y = np.array([5, 7, 9])       # Target values\n",
    "beta = np.array([2, 1])       # Parameters [slope, intercept]\n",
    "\n",
    "print(\"Design matrix X:\")\n",
    "print(X)\n",
    "print(\"\\nTarget vector y:\")\n",
    "print(y)\n",
    "print(\"\\nParameter vector Œ≤:\")\n",
    "print(beta)\n",
    "print(\"\\nPredictions ≈∑ = X¬∑Œ≤:\")\n",
    "y_pred = X @ beta\n",
    "print(y_pred)\n",
    "print(\"\\nResiduals (errors) = y - ≈∑:\")\n",
    "residuals = y - y_pred\n",
    "print(residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3.3 Loss Functions - Measuring Model Performance\n",
    "\n",
    "To find the best parameters $\\boldsymbol{\\beta}$, we need to measure how well our model fits the data. This is done using **loss functions**.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "The most common loss function for regression:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Or in vector form:\n",
    "$$\\text{MSE} = \\frac{1}{n}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2$$\n",
    "\n",
    "**Why MSE?**\n",
    "- Penalizes large errors more than small ones (quadratic penalty)\n",
    "- Mathematically convenient (differentiable)\n",
    "- Corresponds to maximum likelihood estimation under Gaussian noise\n",
    "\n",
    "### Cost Function (Objective Function)\n",
    "\n",
    "Often we use the **sum** of squared errors as our cost function:\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{2}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2$$\n",
    "\n",
    "The factor of $\\frac{1}{2}$ simplifies derivatives during optimization.\n",
    "\n",
    "## 3.4 Gradient Descent - Finding Optimal Parameters\n",
    "\n",
    "**Gradient descent** is an iterative optimization algorithm to minimize the cost function.\n",
    "\n",
    "### The Gradient\n",
    "\n",
    "The gradient of the cost function with respect to parameters:\n",
    "\n",
    "$$\\nabla J(\\boldsymbol{\\beta}) = \\frac{\\partial J}{\\partial \\boldsymbol{\\beta}} = -\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$$\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "At each iteration, we update parameters in the direction of steepest descent:\n",
    "\n",
    "$$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\alpha \\nabla J(\\boldsymbol{\\beta}^{(t)})$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$: learning rate (step size)\n",
    "- $t$: iteration number\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **Initialize** parameters $\\boldsymbol{\\beta}^{(0)}$ (usually zeros or small random values)\n",
    "2. **Repeat** until convergence:\n",
    "   - Compute predictions: $\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}^{(t)}$\n",
    "   - Compute gradient: $\\nabla J = -\\mathbf{X}^T(\\mathbf{y} - \\hat{\\mathbf{y}})$\n",
    "   - Update parameters: $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\alpha \\nabla J$\n",
    "   - Check convergence criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Cost Function and Gradient Descent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Generate simple 1D dataset\n",
    "np.random.seed(42)\n",
    "n = 20\n",
    "x = np.linspace(0, 5, n)\n",
    "y_true = 2 * x + 1  # True relationship: y = 1 + 2x\n",
    "y = y_true + np.random.normal(0, 0.5, n)  # Add noise\n",
    "\n",
    "# Prepare data for matrix operations\n",
    "X = np.column_stack([np.ones(n), x])  # Add bias column [1, x]\n",
    "\n",
    "print(\"Dataset shape:\", X.shape, y.shape)\n",
    "print(\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"X[{i}] = [{X[i,0]:.1f}, {X[i,1]:.1f}], y[{i}] = {y[i]:.2f}\")\n",
    "\n",
    "# Define cost function\n",
    "def cost_function(beta, X, y):\n",
    "    \"\"\"Compute MSE cost function\"\"\"\n",
    "    predictions = X @ beta\n",
    "    errors = y - predictions\n",
    "    return 0.5 * np.mean(errors**2)\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    \"\"\"Compute gradient of cost function\"\"\"\n",
    "    predictions = X @ beta\n",
    "    errors = y - predictions\n",
    "    return -X.T @ errors / len(y)\n",
    "\n",
    "# Create cost function surface for visualization\n",
    "beta0_range = np.linspace(-2, 4, 50)\n",
    "beta1_range = np.linspace(0, 4, 50)\n",
    "B0, B1 = np.meshgrid(beta0_range, beta1_range)\n",
    "\n",
    "# Compute cost for each parameter combination\n",
    "costs = np.zeros_like(B0)\n",
    "for i in range(B0.shape[0]):\n",
    "    for j in range(B0.shape[1]):\n",
    "        beta_test = np.array([B0[i,j], B1[i,j]])\n",
    "        costs[i,j] = cost_function(beta_test, X, y)\n",
    "\n",
    "# Create 3D surface plot\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(B0, B1, costs, cmap='viridis', alpha=0.7)\n",
    "ax1.set_xlabel('Œ≤‚ÇÄ (intercept)')\n",
    "ax1.set_ylabel('Œ≤‚ÇÅ (slope)')\n",
    "ax1.set_zlabel('Cost J(Œ≤)')\n",
    "ax1.set_title('3D Cost Function Surface')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(B0, B1, costs, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('Œ≤‚ÇÄ (intercept)')\n",
    "ax2.set_ylabel('Œ≤‚ÇÅ (slope)')\n",
    "ax2.set_title('Cost Function Contours')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark the true parameters and optimal point\n",
    "true_params = np.array([1, 2])  # True intercept and slope\n",
    "ax2.plot(true_params[0], true_params[1], 'r*', markersize=15, label='True Parameters')\n",
    "\n",
    "# Find optimal parameters analytically for comparison\n",
    "beta_optimal = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "ax2.plot(beta_optimal[0], beta_optimal[1], 'g*', markersize=15, label='Optimal Parameters')\n",
    "ax2.legend()\n",
    "\n",
    "# Demonstrate gradient at a point\n",
    "test_point = np.array([0.5, 1.5])\n",
    "grad_at_point = gradient(test_point, X, y)\n",
    "ax2.arrow(test_point[0], test_point[1], -grad_at_point[0]*2, -grad_at_point[1]*2, \n",
    "          head_width=0.1, head_length=0.1, fc='red', ec='red')\n",
    "ax2.plot(test_point[0], test_point[1], 'ro', markersize=8, label='Test Point')\n",
    "ax2.text(test_point[0], test_point[1]+0.2, 'Gradient Direction', ha='center')\n",
    "\n",
    "# Show data and best fit line\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(x, y, alpha=0.7, color='blue', s=50, label='Data')\n",
    "ax3.plot(x, y_true, 'g--', linewidth=2, label='True Line')\n",
    "ax3.plot(x, X @ beta_optimal, 'r-', linewidth=2, label='Fitted Line')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title('Data and Fitted Line')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue parameters: Œ≤‚ÇÄ = {true_params[0]}, Œ≤‚ÇÅ = {true_params[1]}\")\n",
    "print(f\"Optimal parameters: Œ≤‚ÇÄ = {beta_optimal[0]:.3f}, Œ≤‚ÇÅ = {beta_optimal[1]:.3f}\")\n",
    "print(f\"Cost at optimal point: {cost_function(beta_optimal, X, y):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üß† Self-Test Questions\n",
    "\n",
    "1. Why do we square the errors in the MSE instead of just taking absolute values?\n",
    "2. What happens if the learning rate Œ± is too large? Too small?\n",
    "3. How does the gradient tell us which direction to move the parameters?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answers</summary>\n",
    "\n",
    "1. **Why square errors**: \n",
    "   - Penalizes large errors more heavily (quadratic vs. linear)\n",
    "   - Makes the cost function differentiable everywhere\n",
    "   - Corresponds to maximum likelihood under Gaussian noise assumption\n",
    "   - Computationally efficient\n",
    "\n",
    "2. **Learning rate effects**:\n",
    "   - **Too large**: Algorithm may overshoot the minimum and diverge\n",
    "   - **Too small**: Very slow convergence, may get stuck in local minima\n",
    "   - **Just right**: Smooth, efficient convergence to global minimum\n",
    "\n",
    "3. **Gradient direction**: The gradient points in the direction of steepest **increase** of the cost function. Since we want to minimize cost, we move in the **opposite direction** (negative gradient).\n",
    "</details>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b47cb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4. Linear Regression with Scikit-learn\n",
    "\n",
    "Now that we understand the mathematical foundation, let's see how to implement linear regression using Scikit-learn, the industry-standard machine learning library.\n",
    "\n",
    "## 4.1 Scikit-learn's Consistent API\n",
    "\n",
    "Scikit-learn follows a consistent API pattern across all algorithms:\n",
    "\n",
    "1. **Import** the algorithm class\n",
    "2. **Initialize** the model with hyperparameters\n",
    "3. **Fit** the model to training data\n",
    "4. **Predict** on new data\n",
    "5. **Evaluate** model performance\n",
    "\n",
    "## 4.2 Basic Linear Regression Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Linear Regression Example with Scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic dataset\n",
    "print(\"1. Creating Dataset\")\n",
    "print(\"=\"*50)\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X = X.flatten()  # Convert to 1D for easier plotting\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"X range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"y range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.reshape(-1, 1), y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Create and train the model\n",
    "print(\"\\n2. Training the Model\")\n",
    "print(\"=\"*50)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Intercept (Œ≤‚ÇÄ): {model.intercept_:.3f}\")\n",
    "print(f\"Coefficient (Œ≤‚ÇÅ): {model.coef_[0]:.3f}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n3. Making Predictions\")\n",
    "print(\"=\"*50)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"\\n4. Model Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "print(\"Training Set Performance:\")\n",
    "print(f\"  Mean Squared Error: {train_mse:.3f}\")\n",
    "print(f\"  R¬≤ Score: {train_r2:.3f}\")\n",
    "print(f\"  Mean Absolute Error: {train_mae:.3f}\")\n",
    "\n",
    "# Test metrics\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  Mean Squared Error: {test_mse:.3f}\")\n",
    "print(f\"  R¬≤ Score: {test_r2:.3f}\")\n",
    "print(f\"  Mean Absolute Error: {test_mae:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "print(\"\\n5. Visualization\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Training data and predictions\n",
    "ax1.scatter(X_train, y_train, alpha=0.6, color='blue', label='Training Data')\n",
    "ax1.plot(X_train, y_train_pred, color='red', linewidth=2, label='Predictions')\n",
    "ax1.set_title('Training Set: Data vs Predictions')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test data and predictions\n",
    "ax2.scatter(X_test, y_test, alpha=0.6, color='green', label='Test Data')\n",
    "ax2.plot(X_test, y_test_pred, color='red', linewidth=2, label='Predictions')\n",
    "ax2.set_title('Test Set: Data vs Predictions')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals (errors) for training set\n",
    "residuals_train = y_train - y_train_pred\n",
    "ax3.scatter(y_train_pred, residuals_train, alpha=0.6, color='blue')\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_title('Training Set: Residual Plot')\n",
    "ax3.set_xlabel('Predicted Values')\n",
    "ax3.set_ylabel('Residuals (y - ≈∑)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Predicted vs Actual values\n",
    "ax4.scatter(y_test, y_test_pred, alpha=0.6, color='green')\n",
    "# Perfect prediction line (y = x)\n",
    "min_val = min(y_test.min(), y_test_pred.min())\n",
    "max_val = max(y_test.max(), y_test_pred.max())\n",
    "ax4.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax4.set_title('Test Set: Predicted vs Actual')\n",
    "ax4.set_xlabel('Actual Values')\n",
    "ax4.set_ylabel('Predicted Values')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on new data\n",
    "print(\"\\n6. Predicting on New Data\")\n",
    "print(\"=\"*50)\n",
    "new_X = np.array([-2, -1, 0, 1, 2]).reshape(-1, 1)\n",
    "new_predictions = model.predict(new_X)\n",
    "\n",
    "print(\"New predictions:\")\n",
    "for i, (x_val, pred) in enumerate(zip(new_X.flatten(), new_predictions)):\n",
    "    print(f\"  X = {x_val:4.1f} ‚Üí ≈∑ = {pred:7.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4.3 Understanding Model Metrics\n",
    "\n",
    "### R¬≤ Score (Coefficient of Determination)\n",
    "- **Range**: 0 to 1 (can be negative for very poor models)\n",
    "- **Interpretation**: Proportion of variance in the target explained by the model\n",
    "- **Formula**: $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$ where $SS_{res} = \\sum(y_i - \\hat{y}_i)^2$ and $SS_{tot} = \\sum(y_i - \\bar{y})^2$\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "- **Range**: 0 to ‚àû (lower is better)\n",
    "- **Units**: Same as target variable squared\n",
    "- **Use**: Penalizes large errors more heavily\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "- **Range**: 0 to ‚àû (lower is better)\n",
    "- **Units**: Same as target variable\n",
    "- **Use**: More robust to outliers than MSE\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f0af5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 5. Linear Regression from Scratch\n",
    "\n",
    "Understanding how algorithms work internally is crucial for debugging, optimization, and extending to new problems. Let's implement linear regression using only NumPy.\n",
    "\n",
    "## 5.1 Gradient Descent Variants\n",
    "\n",
    "### Batch Gradient Descent\n",
    "- Uses **all** training samples to compute gradient\n",
    "- **Pros**: Stable convergence, exact gradient\n",
    "- **Cons**: Slow for large datasets, requires all data in memory\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "- Uses **one** sample at a time to compute gradient\n",
    "- **Pros**: Fast updates, can escape local minima, works with large datasets\n",
    "- **Cons**: Noisy convergence, may not reach exact minimum\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "- Uses **small batches** of samples (e.g., 32, 64, 128)\n",
    "- **Pros**: Balance between stability and speed, vectorized operations\n",
    "- **Cons**: Hyperparameter tuning (batch size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression from Scratch - Complete Implementation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class LinearRegressionScratch:\n",
    "    \"\"\"Linear Regression implemented from scratch with multiple gradient descent variants\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6, method='batch'):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: Step size for gradient descent\n",
    "        - max_iterations: Maximum number of training iterations\n",
    "        - tolerance: Convergence threshold\n",
    "        - method: 'batch', 'stochastic', or 'mini_batch'\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.method = method\n",
    "        self.cost_history = []\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def _add_bias_term(self, X):\n",
    "        \"\"\"Add bias term (column of ones) to feature matrix\"\"\"\n",
    "        return np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    def _compute_cost(self, X, y, weights):\n",
    "        \"\"\"Compute mean squared error cost\"\"\"\n",
    "        predictions = X @ weights\n",
    "        errors = y - predictions\n",
    "        return 0.5 * np.mean(errors**2)\n",
    "    \n",
    "    def _compute_gradient(self, X, y, weights):\n",
    "        \"\"\"Compute gradient of cost function\"\"\"\n",
    "        predictions = X @ weights\n",
    "        errors = y - predictions\n",
    "        return -X.T @ errors / len(y)\n",
    "    \n",
    "    def fit(self, X, y, batch_size=32, verbose=False):\n",
    "        \"\"\"\n",
    "        Train the linear regression model\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Feature matrix (n_samples, n_features)\n",
    "        - y: Target vector (n_samples,)\n",
    "        - batch_size: Size of mini-batches (only used for mini_batch method)\n",
    "        - verbose: Print training progress\n",
    "        \"\"\"\n",
    "        # Add bias term\n",
    "        X_with_bias = self._add_bias_term(X)\n",
    "        n_samples, n_features = X_with_bias.shape\n",
    "        \n",
    "        # Initialize weights randomly\n",
    "        np.random.seed(42)\n",
    "        weights = np.random.normal(0, 0.01, n_features)\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Store current cost\n",
    "            current_cost = self._compute_cost(X_with_bias, y, weights)\n",
    "            self.cost_history.append(current_cost)\n",
    "            \n",
    "            if self.method == 'batch':\n",
    "                # Batch Gradient Descent - use all samples\n",
    "                gradient = self._compute_gradient(X_with_bias, y, weights)\n",
    "                weights -= self.learning_rate * gradient\n",
    "                \n",
    "            elif self.method == 'stochastic':\n",
    "                # Stochastic Gradient Descent - use one sample at a time\n",
    "                for i in range(n_samples):\n",
    "                    X_single = X_with_bias[i:i+1]  # Keep 2D shape\n",
    "                    y_single = y[i:i+1]\n",
    "                    gradient = self._compute_gradient(X_single, y_single, weights)\n",
    "                    weights -= self.learning_rate * gradient\n",
    "                    \n",
    "            elif self.method == 'mini_batch':\n",
    "                # Mini-batch Gradient Descent\n",
    "                indices = np.random.permutation(n_samples)\n",
    "                for start_idx in range(0, n_samples, batch_size):\n",
    "                    end_idx = min(start_idx + batch_size, n_samples)\n",
    "                    batch_indices = indices[start_idx:end_idx]\n",
    "                    X_batch = X_with_bias[batch_indices]\n",
    "                    y_batch = y[batch_indices]\n",
    "                    gradient = self._compute_gradient(X_batch, y_batch, weights)\n",
    "                    weights -= self.learning_rate * gradient\n",
    "            \n",
    "            # Check convergence\n",
    "            if len(self.cost_history) > 1:\n",
    "                cost_change = abs(self.cost_history[-2] - self.cost_history[-1])\n",
    "                if cost_change < self.tolerance:\n",
    "                    if verbose:\n",
    "                        print(f\"Converged at iteration {iteration}\")\n",
    "                    break\n",
    "            \n",
    "            if verbose and iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}, Cost: {current_cost:.6f}\")\n",
    "        \n",
    "        # Store final parameters\n",
    "        self.bias = weights[0]\n",
    "        self.weights = weights[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R¬≤ score\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        ss_res = np.sum((y - predictions)**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Creating Dataset for Comparison\")\n",
    "print(\"=\"*50)\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=15, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features for better convergence\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Compare different gradient descent methods\n",
    "methods = ['batch', 'stochastic', 'mini_batch']\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "print(\"\\nTraining Models with Different Methods\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nTraining with {method} gradient descent...\")\n",
    "    \n",
    "    # Adjust learning rate for different methods\n",
    "    if method == 'stochastic':\n",
    "        lr = 0.001  # Lower learning rate for SGD\n",
    "    else:\n",
    "        lr = 0.01\n",
    "    \n",
    "    model = LinearRegressionScratch(\n",
    "        learning_rate=lr, \n",
    "        max_iterations=500, \n",
    "        method=method\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train, batch_size=16, verbose=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    models[method] = model\n",
    "    results[method] = {\n",
    "        'train_r2': train_score,\n",
    "        'test_r2': test_score,\n",
    "        'final_cost': model.cost_history[-1],\n",
    "        'iterations': len(model.cost_history)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train R¬≤: {train_score:.4f}\")\n",
    "    print(f\"  Test R¬≤: {test_score:.4f}\")\n",
    "    print(f\"  Final Cost: {model.cost_history[-1]:.6f}\")\n",
    "    print(f\"  Iterations: {len(model.cost_history)}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Cost function convergence\n",
    "for method in methods:\n",
    "    ax1.plot(models[method].cost_history, label=f'{method.title()} GD', linewidth=2)\n",
    "ax1.set_title('Cost Function Convergence')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Predictions comparison\n",
    "method = 'batch'  # Use batch method for cleaner visualization\n",
    "model = models[method]\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "ax2.scatter(X_test, y_test, alpha=0.6, color='blue', label='Actual')\n",
    "ax2.scatter(X_test, y_pred, alpha=0.6, color='red', label='Predicted')\n",
    "ax2.set_title(f'Predictions ({method.title()} GD)')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning curves comparison\n",
    "ax3.bar(methods, [results[m]['test_r2'] for m in methods], \n",
    "        color=['blue', 'red', 'green'], alpha=0.7)\n",
    "ax3.set_title('Test R¬≤ Score Comparison')\n",
    "ax3.set_ylabel('R¬≤ Score')\n",
    "ax3.set_ylim(0, 1)\n",
    "for i, method in enumerate(methods):\n",
    "    ax3.text(i, results[method]['test_r2'] + 0.01, f\"{results[method]['test_r2']:.3f}\", \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Residuals\n",
    "residuals = y_test - y_pred\n",
    "ax4.scatter(y_pred, residuals, alpha=0.6, color='purple')\n",
    "ax4.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax4.set_title('Residual Plot')\n",
    "ax4.set_xlabel('Predicted Values')\n",
    "ax4.set_ylabel('Residuals')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"=\"*50)\n",
    "for method in methods:\n",
    "    r = results[method]\n",
    "    print(f\"{method.title():12} | Train R¬≤: {r['train_r2']:.4f} | Test R¬≤: {r['test_r2']:.4f} | Iterations: {r['iterations']:3d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 6. Normal Equation (Closed-form Solution)\n",
    "\n",
    "While gradient descent is an iterative approach, the Normal Equation provides a **direct analytical solution** for linear regression parameters.\n",
    "\n",
    "## 6.1 Mathematical Derivation\n",
    "\n",
    "Starting from the cost function:\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2$$\n",
    "\n",
    "To minimize, we take the derivative with respect to $\\boldsymbol{\\beta}$ and set it to zero:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\boldsymbol{\\beta}} = -\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = 0$$\n",
    "\n",
    "Solving for $\\boldsymbol{\\beta}$:\n",
    "$$\\mathbf{X}^T\\mathbf{y} = \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}$$\n",
    "\n",
    "$$\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "This is the **Normal Equation**!\n",
    "\n",
    "## 6.2 Advantages and Disadvantages\n",
    "\n",
    "### Advantages ‚úÖ\n",
    "- **Exact solution**: No approximation, finds global minimum\n",
    "- **No hyperparameters**: No learning rate or iterations to tune\n",
    "- **Fast for small datasets**: Direct computation\n",
    "\n",
    "### Disadvantages ‚ùå\n",
    "- **Computational complexity**: $O(n^3)$ due to matrix inversion\n",
    "- **Memory requirements**: Must store $\\mathbf{X}^T\\mathbf{X}$ matrix\n",
    "- **Numerical instability**: When $\\mathbf{X}^T\\mathbf{X}$ is singular or near-singular\n",
    "- **Not scalable**: Impractical for large datasets (n > 10,000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Equation Implementation and Comparison\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NormalEquationRegression:\n",
    "    \"\"\"Linear Regression using Normal Equation (closed-form solution)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using Normal Equation\n",
    "        Œ≤ = (X^T X)^(-1) X^T y\n",
    "        \"\"\"\n",
    "        # Add bias term (column of ones)\n",
    "        X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        # Normal equation: Œ≤ = (X^T X)^(-1) X^T y\n",
    "        XtX = X_with_bias.T @ X_with_bias\n",
    "        Xty = X_with_bias.T @ y\n",
    "        \n",
    "        # Check if matrix is invertible\n",
    "        try:\n",
    "            # Use pseudo-inverse for numerical stability\n",
    "            beta = np.linalg.solve(XtX, Xty)\n",
    "            self.bias = beta[0]\n",
    "            self.weights = beta[1:]\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Matrix is singular! Using pseudo-inverse...\")\n",
    "            beta = np.linalg.pinv(XtX) @ Xty\n",
    "            self.bias = beta[0]\n",
    "            self.weights = beta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model must be trained first!\")\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R¬≤ score\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        ss_res = np.sum((y - predictions)**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Performance Comparison: Normal Equation vs Gradient Descent\n",
    "print(\"Performance Comparison: Normal Equation vs Gradient Descent\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test with different dataset sizes\n",
    "dataset_sizes = [50, 100, 500, 1000, 2000]\n",
    "comparison_results = {\n",
    "    'sizes': [],\n",
    "    'normal_times': [],\n",
    "    'gd_times': [],\n",
    "    'normal_r2': [],\n",
    "    'gd_r2': []\n",
    "}\n",
    "\n",
    "for n_samples in dataset_sizes:\n",
    "    print(f\"\\nDataset size: {n_samples}\")\n",
    "    \n",
    "    # Generate dataset\n",
    "    X, y = make_regression(n_samples=n_samples, n_features=1, noise=10, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normal Equation\n",
    "    start_time = time.time()\n",
    "    normal_model = NormalEquationRegression()\n",
    "    normal_model.fit(X_train, y_train)\n",
    "    normal_time = time.time() - start_time\n",
    "    normal_r2 = normal_model.score(X_test, y_test)\n",
    "    \n",
    "    # Gradient Descent (from our previous implementation)\n",
    "    start_time = time.time()\n",
    "    gd_model = LinearRegressionScratch(learning_rate=0.01, max_iterations=1000, method='batch')\n",
    "    gd_model.fit(X_train, y_train, verbose=False)\n",
    "    gd_time = time.time() - start_time\n",
    "    gd_r2 = gd_model.score(X_test, y_test)\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results['sizes'].append(n_samples)\n",
    "    comparison_results['normal_times'].append(normal_time)\n",
    "    comparison_results['gd_times'].append(gd_time)\n",
    "    comparison_results['normal_r2'].append(normal_r2)\n",
    "    comparison_results['gd_r2'].append(gd_r2)\n",
    "    \n",
    "    print(f\"  Normal Equation: {normal_time:.4f}s, R¬≤ = {normal_r2:.4f}\")\n",
    "    print(f\"  Gradient Descent: {gd_time:.4f}s, R¬≤ = {gd_r2:.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training time comparison\n",
    "ax1.plot(comparison_results['sizes'], comparison_results['normal_times'], \n",
    "         'bo-', label='Normal Equation', linewidth=2, markersize=8)\n",
    "ax1.plot(comparison_results['sizes'], comparison_results['gd_times'], \n",
    "         'ro-', label='Gradient Descent', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Dataset Size')\n",
    "ax1.set_ylabel('Training Time (seconds)')\n",
    "ax1.set_title('Training Time Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: R¬≤ score comparison\n",
    "ax2.plot(comparison_results['sizes'], comparison_results['normal_r2'], \n",
    "         'bo-', label='Normal Equation', linewidth=2, markersize=8)\n",
    "ax2.plot(comparison_results['sizes'], comparison_results['gd_r2'], \n",
    "         'ro-', label='Gradient Descent', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Dataset Size')\n",
    "ax2.set_ylabel('R¬≤ Score')\n",
    "ax2.set_title('Model Performance Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.8, 1.0)\n",
    "\n",
    "# Plot 3: Demonstrate exact solution with small dataset\n",
    "np.random.seed(42)\n",
    "X_demo = np.random.randn(20, 1)\n",
    "y_demo = 3 * X_demo.flatten() + 2 + np.random.randn(20) * 0.5\n",
    "\n",
    "# Fit both models\n",
    "normal_demo = NormalEquationRegression()\n",
    "normal_demo.fit(X_demo, y_demo)\n",
    "\n",
    "gd_demo = LinearRegressionScratch(learning_rate=0.01, max_iterations=1000, method='batch')\n",
    "gd_demo.fit(X_demo, y_demo, verbose=False)\n",
    "\n",
    "# Plot predictions\n",
    "x_range = np.linspace(X_demo.min(), X_demo.max(), 100).reshape(-1, 1)\n",
    "y_normal = normal_demo.predict(x_range)\n",
    "y_gd = gd_demo.predict(x_range)\n",
    "\n",
    "ax3.scatter(X_demo, y_demo, alpha=0.7, color='blue', s=50, label='Data')\n",
    "ax3.plot(x_range, y_normal, 'g-', linewidth=3, label='Normal Equation')\n",
    "ax3.plot(x_range, y_gd, 'r--', linewidth=2, label='Gradient Descent')\n",
    "ax3.set_title('Solution Comparison (Small Dataset)')\n",
    "ax3.set_xlabel('X')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Computational complexity illustration\n",
    "theoretical_sizes = np.array([100, 200, 500, 1000, 2000, 5000])\n",
    "# O(n¬≥) for normal equation (matrix inversion)\n",
    "normal_complexity = (theoretical_sizes ** 3) / (100 ** 3)  # Normalized\n",
    "# O(n) per iteration for gradient descent (assuming fixed iterations)\n",
    "gd_complexity = theoretical_sizes / 100  # Normalized\n",
    "\n",
    "ax4.plot(theoretical_sizes, normal_complexity, 'b-', linewidth=3, \n",
    "         label='Normal Equation O(n¬≥)')\n",
    "ax4.plot(theoretical_sizes, gd_complexity, 'r-', linewidth=3, \n",
    "         label='Gradient Descent O(n)')\n",
    "ax4.set_xlabel('Dataset Size (n)')\n",
    "ax4.set_ylabel('Relative Computational Cost')\n",
    "ax4.set_title('Theoretical Computational Complexity')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSummary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Normal Equation:\")\n",
    "print(f\"  - Exact solution, no iterations needed\")\n",
    "print(f\"  - Parameters: Œ≤‚ÇÄ = {normal_demo.bias:.4f}, Œ≤‚ÇÅ = {normal_demo.weights[0]:.4f}\")\n",
    "print(f\"  - Best for: Small datasets (n < 10,000)\")\n",
    "\n",
    "print(f\"\\nGradient Descent:\")\n",
    "print(f\"  - Iterative solution, converged in {len(gd_demo.cost_history)} iterations\")\n",
    "print(f\"  - Parameters: Œ≤‚ÇÄ = {gd_demo.bias:.4f}, Œ≤‚ÇÅ = {gd_demo.weights[0]:.4f}\")\n",
    "print(f\"  - Best for: Large datasets, online learning\")\n",
    "\n",
    "print(f\"\\nParameter difference:\")\n",
    "print(f\"  - Bias difference: {abs(normal_demo.bias - gd_demo.bias):.6f}\")\n",
    "print(f\"  - Weight difference: {abs(normal_demo.weights[0] - gd_demo.weights[0]):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 7. Advanced Concepts\n",
    "\n",
    "## 7.1 Regularization - Preventing Overfitting\n",
    "\n",
    "**Problem**: Complex models can memorize training data but fail on new data (overfitting).\n",
    "\n",
    "**Solution**: Add penalty terms to the cost function to discourage complex models.\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2 + \\lambda||\\boldsymbol{\\beta}||^2_2$$\n",
    "\n",
    "- **Effect**: Shrinks coefficients toward zero\n",
    "- **Use case**: When all features are somewhat relevant\n",
    "- **Hyperparameter**: $\\lambda$ (regularization strength)\n",
    "\n",
    "### Lasso Regression (L1 Regularization)\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2 + \\lambda||\\boldsymbol{\\beta}||_1$$\n",
    "\n",
    "- **Effect**: Can set coefficients exactly to zero (feature selection)\n",
    "- **Use case**: When only some features are relevant\n",
    "- **Hyperparameter**: $\\lambda$ (regularization strength)\n",
    "\n",
    "### Elastic Net (L1 + L2)\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2 + \\lambda_1||\\boldsymbol{\\beta}||_1 + \\lambda_2||\\boldsymbol{\\beta}||^2_2$$\n",
    "\n",
    "- **Effect**: Combines benefits of Ridge and Lasso\n",
    "- **Use case**: High-dimensional data with grouped features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization Comparison\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a dataset with noise and many features\n",
    "print(\"Regularization Demonstration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate dataset with more features than samples (overfitting scenario)\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 50, 20\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                      noise=10, random_state=42)\n",
    "\n",
    "# Add some irrelevant features\n",
    "X_irrelevant = np.random.randn(n_samples, 10)\n",
    "X_extended = np.column_stack([X, X_irrelevant])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_extended, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "\n",
    "# Define models with different regularization\n",
    "models = {\n",
    "    'Linear Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge (Œ±=1.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=1.0))\n",
    "    ]),\n",
    "    'Ridge (Œ±=10.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=10.0))\n",
    "    ]),\n",
    "    'Lasso (Œ±=1.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=1.0, max_iter=2000))\n",
    "    ]),\n",
    "    'Lasso (Œ±=5.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=5.0, max_iter=2000))\n",
    "    ]),\n",
    "    'Elastic Net': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=2000))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    # Get coefficients\n",
    "    regressor = model.named_steps['regressor']\n",
    "    if hasattr(regressor, 'coef_'):\n",
    "        coefficients = regressor.coef_\n",
    "        n_zero_coefs = np.sum(np.abs(coefficients) < 1e-5)\n",
    "    else:\n",
    "        coefficients = None\n",
    "        n_zero_coefs = 0\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_score,\n",
    "        'test_r2': test_score,\n",
    "        'coefficients': coefficients,\n",
    "        'n_zero_coefs': n_zero_coefs\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Model':<15} {'Train R¬≤':<10} {'Test R¬≤':<10} {'Zero Coefs':<10} {'Overfitting':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, result in results.items():\n",
    "    overfitting = result['train_r2'] - result['test_r2']\n",
    "    print(f\"{name:<15} {result['train_r2']:<10.3f} {result['test_r2']:<10.3f} \"\n",
    "          f\"{result['n_zero_coefs']:<10d} {overfitting:<12.3f}\")\n",
    "\n",
    "# Visualize coefficients\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Model performance comparison\n",
    "model_names = list(results.keys())\n",
    "train_scores = [results[name]['train_r2'] for name in model_names]\n",
    "test_scores = [results[name]['test_r2'] for name in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x_pos - width/2, train_scores, width, label='Training R¬≤', alpha=0.8)\n",
    "ax1.bar(x_pos + width/2, test_scores, width, label='Test R¬≤', alpha=0.8)\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('R¬≤ Score')\n",
    "ax1.set_title('Model Performance: Training vs Test')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Coefficient magnitudes for different regularization\n",
    "selected_models = ['Linear Regression', 'Ridge (Œ±=1.0)', 'Lasso (Œ±=1.0)']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for i, (model_name, color) in enumerate(zip(selected_models, colors)):\n",
    "    coefs = results[model_name]['coefficients']\n",
    "    if coefs is not None:\n",
    "        ax2.plot(range(len(coefs)), np.abs(coefs), 'o-', color=color, \n",
    "                label=model_name, alpha=0.7, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Feature Index')\n",
    "ax2.set_ylabel('|Coefficient|')\n",
    "ax2.set_title('Coefficient Magnitudes')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: Regularization path for Ridge\n",
    "alphas = np.logspace(-3, 2, 50)\n",
    "ridge_coefs = []\n",
    "ridge_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=alpha))\n",
    "    ])\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    ridge_coefs.append(ridge_model.named_steps['regressor'].coef_)\n",
    "    ridge_scores.append(ridge_model.score(X_test, y_test))\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "# Plot first 10 features for clarity\n",
    "for i in range(min(10, ridge_coefs.shape[1])):\n",
    "    ax3.plot(alphas, ridge_coefs[:, i], alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Regularization Parameter (Œ±)')\n",
    "ax3.set_ylabel('Coefficient Value')\n",
    "ax3.set_title('Ridge Regularization Path')\n",
    "ax3.set_xscale('log')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Lasso regularization path\n",
    "lasso_alphas = np.logspace(-3, 1, 30)\n",
    "lasso_coefs = []\n",
    "lasso_scores = []\n",
    "\n",
    "for alpha in lasso_alphas:\n",
    "    lasso_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=alpha, max_iter=2000))\n",
    "    ])\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    lasso_coefs.append(lasso_model.named_steps['regressor'].coef_)\n",
    "    lasso_scores.append(lasso_model.score(X_test, y_test))\n",
    "\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "# Plot first 10 features for clarity\n",
    "for i in range(min(10, lasso_coefs.shape[1])):\n",
    "    ax4.plot(lasso_alphas, lasso_coefs[:, i], alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Regularization Parameter (Œ±)')\n",
    "ax4.set_ylabel('Coefficient Value')\n",
    "ax4.set_title('Lasso Regularization Path')\n",
    "ax4.set_xscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature selection demonstration\n",
    "print(f\"\\nFeature Selection with Lasso:\")\n",
    "print(\"=\"*40)\n",
    "lasso_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Lasso(alpha=5.0, max_iter=2000))\n",
    "])\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "lasso_coefs = lasso_model.named_steps['regressor'].coef_\n",
    "selected_features = np.where(np.abs(lasso_coefs) > 1e-5)[0]\n",
    "print(f\"Original features: {len(lasso_coefs)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "print(f\"Feature selection ratio: {len(selected_features)/len(lasso_coefs):.2%}\")\n",
    "print(f\"Selected feature indices: {selected_features.tolist()}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(\"=\"*40)\n",
    "print(\"1. Linear regression overfits when n_features ‚âà n_samples\")\n",
    "print(\"2. Ridge regression shrinks coefficients but keeps all features\")\n",
    "print(\"3. Lasso regression performs automatic feature selection\")\n",
    "print(\"4. Higher Œ± values ‚Üí more regularization ‚Üí simpler models\")\n",
    "print(\"5. Regularization helps generalization (better test performance)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 8. Real-World Case Study: Boston Housing Dataset\n",
    "\n",
    "Let's apply everything we've learned to a real-world dataset. We'll predict house prices using various features like number of rooms, crime rate, etc.\n",
    "\n",
    "**Dataset Features:**\n",
    "- `CRIM`: Crime rate per capita\n",
    "- `ZN`: Proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- `INDUS`: Proportion of non-retail business acres\n",
    "- `CHAS`: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- `NOX`: Nitric oxides concentration (parts per 10 million)\n",
    "- `RM`: Average number of rooms per dwelling\n",
    "- `AGE`: Proportion of owner-occupied units built prior to 1940\n",
    "- `DIS`: Weighted distances to employment centers\n",
    "- `RAD`: Index of accessibility to radial highways\n",
    "- `TAX`: Property tax rate per $10,000\n",
    "- `PTRATIO`: Pupil-teacher ratio by town\n",
    "- `B`: Proportion of blacks by town\n",
    "- `LSTAT`: % lower status of the population\n",
    "- `MEDV`: Median value of owner-occupied homes (target variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-World Case Study: Boston Housing Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing  # Using California housing as Boston is deprecated\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Real-World Case Study: California Housing Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "feature_names = housing.feature_names\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"  Samples: {X.shape[0]:,}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Target: House value (in hundreds of thousands of dollars)\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df.describe().round(2))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot distributions of all features\n",
    "for i, feature in enumerate(feature_names):\n",
    "    axes[i].hist(df[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify features most correlated with target\n",
    "target_corr = correlation_matrix['target'].abs().sort_values(ascending=False)\n",
    "print(\"Features most correlated with house prices:\")\n",
    "for feature, corr in target_corr.items():\n",
    "    if feature != 'target':\n",
    "        print(f\"  {feature:15}: {corr:.3f}\")\n",
    "\n",
    "# Feature engineering: Create some polynomial features for demonstration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add some engineered features\n",
    "df['MedInc_squared'] = df['MedInc'] ** 2  # Median income squared\n",
    "df['Rooms_per_household'] = df['AveRooms'] / df['AveOccup']  # Rooms per person\n",
    "df['Bedrooms_ratio'] = df['AveBedrms'] / df['AveRooms']  # Bedroom ratio\n",
    "\n",
    "# Prepare data for modeling\n",
    "feature_columns = list(feature_names) + ['MedInc_squared', 'Rooms_per_household', 'Bedrooms_ratio']\n",
    "X_engineered = df[feature_columns].values\n",
    "y = df['target'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge (Œ±=1.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=1.0))\n",
    "    ]),\n",
    "    'Ridge (Œ±=10.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=10.0))\n",
    "    ]),\n",
    "    'Lasso (Œ±=0.1)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=0.1, max_iter=2000))\n",
    "    ]),\n",
    "    'Our Implementation': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegressionScratch(learning_rate=0.01, max_iterations=2000, method='batch'))\n",
    "    ])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "print(f\"{'Model':<20} {'Train R¬≤':<10} {'Test R¬≤':<10} {'RMSE':<10} {'MAE':<10} {'CV Score':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation (skip for our custom implementation to avoid complexity)\n",
    "    if name != 'Our Implementation':\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "        cv_mean = cv_scores.mean()\n",
    "    else:\n",
    "        cv_mean = np.nan\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'cv_score': cv_mean,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<20} {train_r2:<10.3f} {test_r2:<10.3f} {rmse:<10.3f} {mae:<10.3f} {cv_mean:<10.3f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['test_r2'] if not np.isnan(results[x]['test_r2']) else -1)\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "\n",
    "# Detailed analysis of best model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED MODEL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "if hasattr(best_model.named_steps['regressor'], 'coef_'):\n",
    "    coefficients = best_model.named_steps['regressor'].coef_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'coefficient': coefficients,\n",
    "        'abs_coefficient': np.abs(coefficients)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(\"Feature Importance (by absolute coefficient value):\")\n",
    "    for _, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']:<20}: {row['coefficient']:8.3f}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "ax1.scatter(y_test, y_pred_best, alpha=0.5)\n",
    "min_val = min(y_test.min(), y_pred_best.min())\n",
    "max_val = max(y_test.max(), y_pred_best.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "ax1.set_xlabel('Actual Price')\n",
    "ax1.set_ylabel('Predicted Price')\n",
    "ax1.set_title(f'Predicted vs Actual ({best_model_name})')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals = y_test - y_pred_best\n",
    "ax2.scatter(y_pred_best, residuals, alpha=0.5)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Predicted Price')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Residual Plot')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Feature importance\n",
    "if hasattr(best_model.named_steps['regressor'], 'coef_'):\n",
    "    top_features = feature_importance.head(8)\n",
    "    colors = ['red' if coef < 0 else 'blue' for coef in top_features['coefficient']]\n",
    "    ax3.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7)\n",
    "    ax3.set_yticks(range(len(top_features)))\n",
    "    ax3.set_yticklabels(top_features['feature'])\n",
    "    ax3.set_xlabel('Coefficient Value')\n",
    "    ax3.set_title('Top Feature Coefficients')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Model comparison\n",
    "model_names = list(results.keys())[:-1]  # Exclude our implementation for cleaner plot\n",
    "test_scores = [results[name]['test_r2'] for name in model_names]\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "ax4.bar(model_names, test_scores, color=colors, alpha=0.7)\n",
    "ax4.set_ylabel('Test R¬≤ Score')\n",
    "ax4.set_title('Model Performance Comparison')\n",
    "ax4.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Business insights\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUSINESS INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Key Findings:\")\n",
    "print(f\"1. Best model achieves R¬≤ = {results[best_model_name]['test_r2']:.3f} on test data\")\n",
    "print(f\"2. Average prediction error: ${results[best_model_name]['mae']:.1f}k\")\n",
    "print(f\"3. RMSE: ${results[best_model_name]['rmse']:.1f}k\")\n",
    "\n",
    "if hasattr(best_model.named_steps['regressor'], 'coef_'):\n",
    "    top_positive = feature_importance[feature_importance['coefficient'] > 0].head(3)\n",
    "    top_negative = feature_importance[feature_importance['coefficient'] < 0].head(3)\n",
    "    \n",
    "    print(\"\\nFactors that INCREASE house prices:\")\n",
    "    for _, row in top_positive.iterrows():\n",
    "        print(f\"  - {row['feature']}: +${row['coefficient']:.1f}k per unit increase\")\n",
    "    \n",
    "    print(\"\\nFactors that DECREASE house prices:\")\n",
    "    for _, row in top_negative.iterrows():\n",
    "        print(f\"  - {row['feature']}: ${row['coefficient']:.1f}k per unit increase\")\n",
    "\n",
    "# Make sample predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select a few test samples for demonstration\n",
    "sample_indices = [0, 100, 500, 1000]\n",
    "for i in sample_indices:\n",
    "    if i < len(X_test):\n",
    "        sample_X = X_test[i:i+1]\n",
    "        actual_price = y_test[i]\n",
    "        predicted_price = best_model.predict(sample_X)[0]\n",
    "        error = abs(actual_price - predicted_price)\n",
    "        \n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Actual price: ${actual_price:.1f}k\")\n",
    "        print(f\"  Predicted price: ${predicted_price:.1f}k\")\n",
    "        print(f\"  Error: ${error:.1f}k ({error/actual_price*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "print(\"Case Study Complete! üéâ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 9. Summary and Further Study\n",
    "\n",
    "## üéØ Key Concepts Mastered\n",
    "\n",
    "### Mathematical Foundation\n",
    "- **Linear Model**: $y = \\mathbf{X}\\boldsymbol{\\beta} + \\epsilon$\n",
    "- **Cost Function**: Mean Squared Error (MSE)\n",
    "- **Optimization**: Gradient descent and Normal Equation\n",
    "- **Regularization**: Ridge (L2) and Lasso (L1) for preventing overfitting\n",
    "\n",
    "### Implementation Skills\n",
    "- ‚úÖ **Scikit-learn**: Industry-standard implementations\n",
    "- ‚úÖ **From Scratch**: Deep understanding through NumPy implementations\n",
    "- ‚úÖ **Gradient Descent Variants**: Batch, Stochastic, Mini-batch\n",
    "- ‚úÖ **Performance Evaluation**: R¬≤, MSE, MAE, cross-validation\n",
    "\n",
    "### Advanced Topics\n",
    "- ‚úÖ **Feature Engineering**: Creating meaningful features\n",
    "- ‚úÖ **Regularization**: Controlling model complexity\n",
    "- ‚úÖ **Model Selection**: Comparing different approaches\n",
    "- ‚úÖ **Real-world Application**: End-to-end ML pipeline\n",
    "\n",
    "## üöÄ Next Steps in Your ML Journey\n",
    "\n",
    "### Immediate Next Topics\n",
    "1. **Classification Algorithms**\n",
    "   - Logistic Regression\n",
    "   - Decision Trees\n",
    "   - Support Vector Machines (SVM)\n",
    "\n",
    "2. **Model Evaluation & Selection**\n",
    "   - Cross-validation strategies\n",
    "   - Hyperparameter tuning\n",
    "   - Bias-variance tradeoff\n",
    "\n",
    "3. **Feature Engineering**\n",
    "   - Polynomial features\n",
    "   - Feature selection techniques\n",
    "   - Dimensionality reduction (PCA)\n",
    "\n",
    "### Intermediate Topics\n",
    "4. **Ensemble Methods**\n",
    "   - Random Forest\n",
    "   - Gradient Boosting (XGBoost, LightGBM)\n",
    "   - Voting and Stacking\n",
    "\n",
    "5. **Unsupervised Learning**\n",
    "   - K-Means Clustering\n",
    "   - Hierarchical Clustering\n",
    "   - DBSCAN\n",
    "\n",
    "6. **Time Series Analysis**\n",
    "   - ARIMA models\n",
    "   - Seasonal decomposition\n",
    "   - Forecasting techniques\n",
    "\n",
    "### Advanced Topics\n",
    "7. **Neural Networks & Deep Learning**\n",
    "   - Multi-layer Perceptrons\n",
    "   - Convolutional Neural Networks (CNNs)\n",
    "   - Recurrent Neural Networks (RNNs)\n",
    "\n",
    "8. **Specialized Applications**\n",
    "   - Natural Language Processing (NLP)\n",
    "   - Computer Vision\n",
    "   - Recommendation Systems\n",
    "\n",
    "## üìö Recommended Resources\n",
    "\n",
    "### Books\n",
    "- **\"Hands-On Machine Learning\"** by Aur√©lien G√©ron\n",
    "- **\"The Elements of Statistical Learning\"** by Hastie, Tibshirani & Friedman\n",
    "- **\"Pattern Recognition and Machine Learning\"** by Christopher Bishop\n",
    "\n",
    "### Online Courses\n",
    "- **Coursera**: Andrew Ng's Machine Learning Course\n",
    "- **edX**: MIT Introduction to Machine Learning\n",
    "- **Udacity**: Machine Learning Engineer Nanodegree\n",
    "\n",
    "### Practice Platforms\n",
    "- **Kaggle**: Competitions and datasets\n",
    "- **Google Colab**: Free GPU/TPU for experiments\n",
    "- **Papers With Code**: Latest research implementations\n",
    "\n",
    "### Mathematics Review\n",
    "- **Linear Algebra**: Khan Academy, 3Blue1Brown\n",
    "- **Calculus**: Paul's Online Math Notes\n",
    "- **Statistics**: Think Stats, Introduction to Statistical Learning\n",
    "\n",
    "## üõ†Ô∏è Building Your Portfolio\n",
    "\n",
    "### Project Ideas\n",
    "1. **Regression Projects**\n",
    "   - Stock price prediction\n",
    "   - Sales forecasting\n",
    "   - Energy consumption modeling\n",
    "\n",
    "2. **Classification Projects**\n",
    "   - Image recognition\n",
    "   - Sentiment analysis\n",
    "   - Fraud detection\n",
    "\n",
    "3. **End-to-End Projects**\n",
    "   - Build and deploy a web app\n",
    "   - Create an API for model serving\n",
    "   - Implement real-time predictions\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed a comprehensive introduction to machine learning through linear regression. You now have:\n",
    "\n",
    "- **Solid mathematical foundation** for understanding ML algorithms\n",
    "- **Practical implementation skills** in Python and scikit-learn\n",
    "- **Experience with real-world data** and complete ML pipelines\n",
    "- **Understanding of advanced concepts** like regularization and model evaluation\n",
    "\n",
    "The journey in machine learning is continuous. Keep practicing, stay curious, and remember that every expert was once a beginner!\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Final Self-Assessment\n",
    "\n",
    "Test your understanding with these comprehensive questions:\n",
    "\n",
    "1. **Mathematical**: Derive the gradient of the MSE cost function with respect to the parameters.\n",
    "\n",
    "2. **Practical**: When would you choose Ridge over Lasso regression, and why?\n",
    "\n",
    "3. **Conceptual**: Explain the bias-variance tradeoff and how it relates to overfitting.\n",
    "\n",
    "4. **Implementation**: How would you modify the gradient descent algorithm to handle very large datasets?\n",
    "\n",
    "5. **Business**: Given a new regression problem, outline the complete process from data collection to model deployment.\n",
    "\n",
    "<details>\n",
    "<summary>Hints for deeper thinking</summary>\n",
    "\n",
    "1. Start with $J(\\boldsymbol{\\beta}) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2$ and use chain rule.\n",
    "\n",
    "2. Consider feature selection vs. feature shrinkage, and when you have correlated features.\n",
    "\n",
    "3. Think about model complexity, training data size, and generalization performance.\n",
    "\n",
    "4. Consider mini-batch gradient descent, online learning, and distributed computing.\n",
    "\n",
    "5. Include data exploration, feature engineering, model selection, validation, and monitoring.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! üöÄüìäü§ñ**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
